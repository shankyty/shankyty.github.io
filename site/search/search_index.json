{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Notes","text":"<p>This is a collection of my personal notes on various topics.</p>"},{"location":"#interactive-graph","title":"Interactive Graph","text":""},{"location":"#markdown-extensions","title":"Markdown Extensions","text":"<p>We've added support for some helpful Markdown extensions.</p>"},{"location":"#admonitions","title":"Admonitions","text":"<p>This is a note</p> <p>You can use admonitions to call out important information.</p> <p>Danger Zone</p> <p>This is a warning. Be careful!</p>"},{"location":"#code-highlighting","title":"Code Highlighting","text":"<pre><code>def hello_world():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"system-design-interview/chapter02/","title":"Scale From Zero to Millions of Users","text":"<p>Here, we're building a system that supports a few users &amp; gradually scale it to support millions.</p>"},{"location":"system-design-interview/chapter02/#single-server-setup","title":"Single server setup","text":"<p>To start off, we're going to put everything on a single server - web app, database, cache, etc. </p> <p>What's the request flow in there?  * User asks DNS server for the IP of my site (ie <code>api.mysite.com -&gt; 15.125.23.214</code>). Usually, DNS is provided by third-parties instead of hosting it yourself.  * HTTP requests are sent directly to server (via its IP) from your device  * Server returns HTML pages or JSON payloads, used for rendering.</p> <p>Traffic to web server comes from either a web application or a mobile application:  * Web applications use a combo of server-side languages (ie Java, Python) to handle business logic &amp; storage. Client-side languages (ie HTML, JS) are used for presentation.  * Mobile apps use the HTTP protocol for communication between mobile &amp; the web server. JSON is used for formatting transmitted data. Example payload: <pre><code>{\n  \"id\":12,\n  \"firstName\":\"John\",\n  \"lastName\":\"Smith\",\n  \"address\":{\n     \"streetAddress\":\"21 2nd Street\",\n     \"city\":\"New York\",\n     \"state\":\"NY\",\n     \"postalCode\":10021\n  },\n  \"phoneNumbers\":[\n     \"212 555-1234\",\n     \"646 555-4567\"\n  ]\n}\n</code></pre></p>"},{"location":"system-design-interview/chapter02/#database","title":"Database","text":"<p>As the user base grows, storing everything on a single server is insufficient.  We can separate our database on another server so that it can be scaled independently from the web tier: </p>"},{"location":"system-design-interview/chapter02/#which-databases-to-use","title":"Which databases to use?","text":"<p>You can choose either a traditional relational database or a non-relational (NoSQL) one.  * Most popular relational DBs - MySQL, Oracle, PostgreSQL.  * Most popular NoSQL DBs - CouchDB, Neo4J, Cassandra, HBase, DynamoDB</p> <p>Relational databases represent &amp; store data in tables &amp; rows. You can join different tables to represent aggregate objects. NoSQL databases are grouped into four categories - key-value stores, graph stores, column stores &amp; document stores. Join operations are generally not supported.</p> <p>For most use-cases, relational databases are the best option as they've been around the most &amp; have worked quite well historically.</p> <p>If not suitable though, it might be worth exploring NoSQL databases. They might be a better option if:  * Application requires super-low latency.  * Data is unstructured or you don't need any relational data.  * You only need to serialize/deserialize data (JSON, XML, YAML, etc).  * You need to store a massive amount of data.</p>"},{"location":"system-design-interview/chapter02/#vertical-scaling-vs-horizontal-scaling","title":"Vertical scaling vs. horizontal scaling","text":"<p>Vertical scaling == scale up. This means adding more power to your servers - CPU, RAM, etc.</p> <p>Horizontal scaling == scale out. Add more servers to your pool of resources.</p> <p>Vertical scaling is great when traffic is low. Simplicity is its main advantage, but it has limitations:  * It has a hard limit. Impossible to add unlimited CPU/RAM to a single server.  * Lack of fail over and redundancy. If server goes down, whole app/website goes down with it.</p> <p>Horizontal scaling is more appropriate for larger applications due to vertical scaling's limitations. Its main disadvantage is that it's harder to get right.</p> <p>In design so far, the server going down (ie due to failure or overload) means the whole application goes down with it.  A good solution for this problem is to use a load balancer.</p>"},{"location":"system-design-interview/chapter02/#load-balancer","title":"Load balancer","text":"<p>A load balancer evenly distributes incoming traffic among web servers in a load-balanced set: </p> <p>Clients connect to the public IP of the load balancer. Web servers are unreachable by clients directly. Instead, they have private IPs, which the load balancer has access to.</p> <p>By adding a load balancer, we successfully made our web tier more available and we also added possibility for fail over.</p> <p>How it works?  * If server 1 goes down, all traffic will be routed to server 2. This prevents website from going offline. We'll also add a fresh new server to balance the load.  * If website traffic spikes and two servers are not sufficient to handle traffic, load balancer can handle this gracefully by adding more servers to the pool.</p> <p>Web tier looks lit now. But what about the data tier?</p>"},{"location":"system-design-interview/chapter02/#database-replication","title":"Database replication","text":"<p>Database replication can usually be achieved via master/slave replication (side note - nowadays, it's usually referred to as primary/secondary replication).</p> <p>A master database generally only supports writes. Slave databases store copies of the data from the master &amp; only support read operations. This setup works well for most applications as there's usually a higher read to write ratio. Reads can easily be scaled by adding more slave instances. </p> <p>Advantages:  * Better performance - enables more read queries to be processed in parallel.  * Reliability - If one database gets destroyed, data is still preserved.  * High availability - Data is accessible as long as one instance is not offline.</p> <p>So what if one database goes offline?  * If slave database goes offline, read operations are routed to the master/other slaves temporarily.   * If master goes down, a slave instance will be promoted to the new master. A new slave instance will replace the old master. </p> <p>Here's the refined request lifecycle:  * user gets IP address of load balancer from DNS  * user connects to load balancer via IP  * HTTP request is routed to server 1 or server 2  * web server reads user data from a slave database instance or routes data modifications to the master instance.</p> <p>Sweet, let's now improve the load/response time by adding a cache &amp; shifting static content to a CDN.</p>"},{"location":"system-design-interview/chapter02/#cache","title":"Cache","text":"<p>Cache is a temporary storage which stores frequently accessed data or results of expensive computations.</p> <p>In our web application, every time a web page is loaded, expensive queries are sent to the database.  We can mitigate this using a cache.</p>"},{"location":"system-design-interview/chapter02/#cache-tier","title":"Cache tier","text":"<p>The cache tier is a temporary storage layer, from which results are fetched much more rapidly than from within a database. It can also be scaled independently from the database. </p> <p>The example above is a read-through cache - server checks if data is available in the cache. If not, data is fetched from the database.</p>"},{"location":"system-design-interview/chapter02/#considerations-for-using-cache","title":"Considerations for using cache","text":"<ul> <li>When to use it - usually useful when data is read frequently but modified infrequently. Caches usually don't preserve data upon restart so it's not a good persistence layer.</li> <li>Expiration policy - controls whether (and when) cached data expires and is removed from it. Make it too short - DB will be queried frequently. Make it too long - data will become stale.</li> <li>Consistency - How in sync should the data store &amp; cache be? Inconsistency happens if data is changed in DB, but cache is not updated.</li> <li>Mitigating failures - A single cache server could be a single point of failure (SPOF). Consider over-provisioning it with a lot of memory and/or provisioning servers in multiple locations.</li> <li>Eviction policy - What happens when you want to add items to a cache, but it's full? Cache eviction policy controls that. Common policies - LRU, LFU, FIFO.</li> </ul>"},{"location":"system-design-interview/chapter02/#content-delivery-network-cdn","title":"Content Delivery Network (CDN)","text":"<p>CDN == network of geographically dispersed servers, used for delivering static content - eg images, HTML, CSS, JS files.</p> <p>Whenever a user requests some static content, the CDN server closest to the user serves it: </p> <p>Here's the request flow:   * User tries fetching an image via URL. URLs are provided by the CDN, eg <code>https://mysite.cloudfront.net/logo.jpg</code>  * If the image is not in the cache, the CDN requests the file from the origin - eg web server, S3 bucket, etc.  * Origin returns the image to the CDN with an optional TTL (time to live) parameter, which controls how long that static resource is to be cached.  * Subsequent users fetch the image from the CDN without any requests reaching the origin as long as it's within the TTL.</p>"},{"location":"system-design-interview/chapter02/#considerations-of-using-cdn","title":"Considerations of using CDN","text":"<ul> <li>Cost - CDNs are managed by third-parties for which you pay a fee. Be careful not to store infrequently accessed data in there.</li> <li>Cache expiry - consider appropriate cache expiry. Too short - frequent requests to origin. Too long - data becomes stale.</li> <li>CDN fallback - clients should be able to workaround the CDN provider if there is a temporary outage on their end.</li> <li>Invalidation - can be done via an API call or by passing object versions.</li> </ul> <p>Refined design of our web application: </p>"},{"location":"system-design-interview/chapter02/#stateless-web-tier","title":"Stateless web tier","text":"<p>In order to scale our web tier, we need to make it stateless.</p> <p>In order to do that, we can store user session data in persistent data storage such as our relational database or a NoSQL database.</p>"},{"location":"system-design-interview/chapter02/#stateful-architecture","title":"Stateful architecture","text":"<p>Stateful servers remember client data across different requests. Stateless servers don't. </p> <p>In the above case, users are coupled to the server which stores their session data. If they make a request to another server, it won't have access to the user's session.</p> <p>This can be solved via sticky sessions, which most load balancers support, but it adds overhead. Adding/removing servers is much more challenging, which limits our options in case of server failures.</p>"},{"location":"system-design-interview/chapter02/#stateless-architecture","title":"Stateless architecture","text":"<p>In this scenario, servers don't store any user data themselves.  Instead, they store it in a shared data store, which all servers have access to.</p> <p>This way, HTTP requests from users can be served by any web server.</p> <p>Updated web application architecture: </p> <p>The user session data store could either be a relational database or a NoSQL data store, which is easier to scale for this kind of data. The next step in the app's evolution is supporting multiple data centers.</p>"},{"location":"system-design-interview/chapter02/#data-centers","title":"Data centers","text":"<p>In the above example, clients are geo-routed to the nearest data center based on the IP address.</p> <p>In the event of an outage, we route all traffic to the healthy data center: </p> <p>To achieve this multi-datacenter setup, there are several issues we need to address:  * traffic redirection - tooling for correctly directing traffic to the right data center. GeoDNS can be used in this case.  * data synchronization - in case of failover, users from DC1 go to DC2. A challenge is whether their user data is there.  * test and deployment - automated deployment &amp; testing is crucial to keep deployments consistent across DCs.</p> <p>To further scale the system, we need to decouple different system components so they can scale independently.</p>"},{"location":"system-design-interview/chapter02/#message-queues","title":"Message queues","text":"<p>Message queues are durable components, which enable asynchronous communication. </p> <p>Basic architecture:  * Producers create messages.  * Consumers/Subscribers subscribe to new messages and consume them.</p> <p>Message queues enable producers to be decoupled from consumers.  If a consumer is down, a producer can still publish a message and the consumer will receive it at a later point.</p> <p>Example use-case in our application - photo processing:  * Web servers publish \"photo processing tasks\" to a message queue  * A variable number of workers (can be scaled up or down) subscribe to the queue and process those tasks. </p>"},{"location":"system-design-interview/chapter02/#logging-metrics-automation","title":"Logging, metrics, automation","text":"<p>Once your web application grows beyond a given point, investing in monitoring tooling is critical.  * Logging - error logs can be emitted to a data store, which can later be read by service operators.  * Metrics - collecting various types of metrics helps us collect business insight &amp; monitor the health of the system.  * Automation - investing in continuous integration such as automated build, test, deployment can detect various problems early and also increases developer productivity.</p> <p>Updated system design: </p>"},{"location":"system-design-interview/chapter02/#database-scaling","title":"Database scaling","text":"<p>There are two approaches to database scaling - vertical and horizontal.</p>"},{"location":"system-design-interview/chapter02/#vertical-scaling","title":"Vertical scaling","text":"<p>Also known as scaling up, it means adding more physical resources to your database nodes - CPU, RAM, HDD, etc. In Amazon RDS, for example, you can get a database node with 24 TB of RAM.</p> <p>This kind of database can handle lots of data - eg stackoverflow in 2013 had 10mil monthly unique visitors \\w a single database node.</p> <p>Vertical scaling has some drawbacks, though:  * There are hardware limits to the amount of resources you can add to a node.  * You still have a single point of failure.  * Overall cost is high - the price of powerful servers is high.</p>"},{"location":"system-design-interview/chapter02/#horizontal-scaling","title":"Horizontal scaling","text":"<p>Instead of adding bigger servers, you can add more of them: </p> <p>Sharding is a type of database horizontal scaling which separates large data sets into smaller ones. Each shard shares the same schema, but the actual data is different.</p> <p>One way to shard the database is based on some key, which is equally distributed on all shards using the modulo operator: </p> <p>Here's how the user data looks like in this example: </p> <p>The sharding key (aka partition key) is the most important factor to consider when using sharding. In particular, the key should be chosen in a way that distributes the data as evenly as possible.</p> <p>Although a useful technique, it introduces a lot of complexities in the system:  * Resharding data - you need to do it if a single shard grows too big. This can happen rather quickly if data is distributed unevenly. Consistent hashing helps to avoid moving too much data around.  * Celebrity problem (aka hotspot) - one shard could be accessed much more frequently than others and can lead to server overload. We may have to resort to using separate shards for certain celebrities.  * Join and de-normalization - It is hard to perform join operations across shards. A common workaround is to de-normalize your tables to avoid making joins.</p> <p>Here's how our application architecture looks like after introducing sharding and a NoSQL database for some of the non-relational data: </p>"},{"location":"system-design-interview/chapter02/#millions-of-users-and-beyond","title":"Millions of users and beyond","text":"<p>Scaling a system is iterative.</p> <p>What we've learned so far can get us far, but we might need to apply even more sophisticated techniques to scale the application beyond millions of users.</p> <p>The techniques we saw so far can offer a good foundation to start from.</p> <p>Here's a summary:  * Keep web tier stateless  * Build redundancy at every layer  * Cache frequently accessed data  * Support multiple data centers  * Host static assets in CDNs  * Scale your data tier via sharding  * Split your big application into multiple services  * Monitor your system &amp; use automation</p>"},{"location":"system-design-interview/chapter03/","title":"Back-of-the-envelope Estimation","text":"<p>You are sometimes asked in a system design interview to estimate performance requirements or system capacity.</p> <p>These are usually done with thought experiments and common performance numbers, according to Jeff Dean (Google Senior Fellow).</p> <p>To do this estimation effectively, there are several mechanisms one should be aware of.</p>"},{"location":"system-design-interview/chapter03/#power-of-two","title":"Power of two","text":"<p>Data volumes can become enormous, but calculation boils down to basics.</p> <p>For precise calculations, you need to be aware of the power of two, which corresponds to given data units:  * 2^10 == ~1000 == 1kb  * 2^20 == ~1mil == 1mb  * 2^30 == ~1bil == 1gb  * 2^40 == ~1tril == 1tb  * 2^50 == ~1quad == 1pb</p>"},{"location":"system-design-interview/chapter03/#latency-numbers-every-programmer-should-know","title":"Latency numbers every programmer should know","text":"<p>There's a well-known table of the duration of typical computer operations, created by Jeff Dean.</p> <p>These might be a bit outdated due to hardware improvements, but they still give a good relative measure among the operations:  * L1 cache reference == 0.5ns  * Branch mispredict == 5ns  * L2 cache reference == 7ns  * Mutex lock/unlock == 100ns  * Main memory reference == 100ns  * Compress 1kb == 10,000ns == 10us  * Send 2kb over 1gbps network == 20,000ns == 20us  * Read 1mb sequentially from memory == 250us  * Round trip within same DC == 500us  * Disk seek == 10ms  * Read 1mb sequentially from network == 10ms  * Read 1mb sequentially from disk == 30ms  * Send packet CA-&gt;Netherlands-&gt;CA == 150ms</p> <p>A good visualization of the above: </p> <p>Some conclusions from the above numbers:  * Memory is fast, disk is slow  * Avoid disk seeks if possible  * Compression is usually fast  * Compress data before sending over the network if possible  * Data centers round trips are expensive</p>"},{"location":"system-design-interview/chapter03/#availability-numbers","title":"Availability numbers","text":"<p>High availability == ability of a system to be continuously operational. In other words, minimizing downtime.</p> <p>Typically, services aim for availability in the range of 99% to 100%.</p> <p>An SLA is a formal agreement between a service provider and a customer.  This formally defines the level of uptime your service needs to support.</p> <p>Cloud providers typically set their uptime at 99.9% or more. Eg AWS EC2 has an SLA of 99.99%</p> <p>Here's a summary of the allowed downtime based on different SLAs: </p>"},{"location":"system-design-interview/chapter03/#example-estimate-twitter-qps-and-storage-requirements","title":"Example - estimate Twitter QPS and storage requirements","text":"<p>Assumptions:  * 300mil MAU  * 50% of users use twitter daily  * Users post 2 tweets per day on average  * 10% of tweets contain media  * Data is stored for 5y</p> <p>Estimations:  * Write RPS estimation == 150mil * 2 / 24h / 60m / 60s = 3400-3600 tweets per second, peak=7000 TPS  * Media storage per day == 300mil * 10% == 30mil media storage per day      * If we assume 1mb per media -&gt; 30mil * 1mb = 30tb per day      * in 5y -&gt; 30tb * 365 * 5 == 55pb  * tweet storage estimation:      * 1 tweet = 64byte id + 140 bytes text + 1000 bytes metadata      * 3500 * 60 * 60 * 24 = 302mb per day      * In 5y -&gt; 302 * 365 * 5 == 551gb in 5y</p>"},{"location":"system-design-interview/chapter03/#tips","title":"Tips","text":"<p>Back-of-the-envelope Estimations are about the process, not the results. Interviewers might test your problem-solving skills.</p> <p>Some tips to take into consideration:  * Rounding and approximation - don't try to calculate 99987/9.1, round it to 100000/10 instead, which is easier to calculate.  * Write down your assumptions before going forward with estimations  * Label your units explicitly. Write 5mb instead of 5.  * Commonly asked estimations to make - QPS (queries per second), peak QPS, storage, cache, number of servers.</p>"},{"location":"system-design-interview/chapter04/","title":"A Framework for System Design Interviews","text":"<p>System design interviews are intimidating.</p> <p>How could you possible design a popular system in an hour when it has taken other people decades?  * It is about the problem solving aspect, not the final solution you came up with.  * Goal is to demonstrate your design skills, defend your choices, respond to feedback constructively.</p> <p>What's the goal of a system design interview?  * It is much more than evaluating a person's technical design skills.  * It is also about evaluating collaboration skills, working under pressure, resolving ambiguity, asking good questions.  * Catching red flags - over-engineering, narrow mindedness, stubborness, etc.</p>"},{"location":"system-design-interview/chapter04/#a-4-step-process-for-effective-system-design-interview","title":"A 4-step process for effective system design interview","text":"<p>Although every interview is different, there are some steps to cover in any interview.</p>"},{"location":"system-design-interview/chapter04/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - understand the problem and establish design scope","text":"<p>Don't rush to give answers before understanding the problem well enough first. In fact, this might be a red flag - answering without a thorough understanding of requirements.</p> <p>Don't jump to solutions. Ask questions to clarify requirements and assumptions.</p> <p>We have a tendency as engineers to jump ahead and solve a hard problem. But that often leads to designing the wrong system.</p> <p>When you get your answers (or you're asked to make assumptions yourself), write them down on the whiteboard to not forget.</p> <p>What kind of questions to ask? - to understand the exact requirements. Some examples:  * What specific features do we need to design?  * How many users does the product have?  * How fast is the company anticipated to scale up? - in 3, 6, 12 months?  * What is the company's tech stack? What existing services can you leverage to simplify the design?</p> <p>For example, say you have to design a news feed. Here's an example conversation:  * Candidate: Is it mobile, web, both?  * Interviewer: both.  * C: What's the most important features?  * I: Ability to make posts &amp; see friends' news feed.  * C: How is the feed sorted? Just chronologically or based on eg some weight to posts from close friends.  * I: To keep things simple, assume posts are sorted chronologically.  * C: Max friends on a user?  * I: 5000  * C: What's the traffic volume?  * I: 10mil daily active users (DAU)  * C: Is there any media in the feed? - images, video?  * I: It can contain media files, including video &amp; images.</p>"},{"location":"system-design-interview/chapter04/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>The goal of this step is to develop a high-level design, while collaborating with the interviewer.  * Come up with a blueprint, ask for feedback. Many good interviewers involve the interviewer.  * Draw boxes on the whiteboard with key components - clients, APIs, web servers, data stores, cache, cdn, message queue, etc.  * Do back-of-the-envelope calculations to evaluate if the blueprint fits the scale constraints. Communicate with interviewer if this type of estimation is required beforehand.</p> <p>You could go through some concrete use-cases, which can help you refine the design. It might help you uncover edge cases.</p> <p>Should we include API schema and database design? Depends on the problem. For larger scale systems, this might be too low level. Communicate with the interviewer to figure this out.</p> <p>Example - designing a news feed.</p> <p>High-level features:  * feed publishing - user creates a post and that post is written in cache/database, after which it gets populated in other news feeds.  * news feed building - news feed is built by aggregating friends' posts in chronological order.</p> <p>Example diagram - feed publishing: </p> <p>Example diagram - news feed building: </p>"},{"location":"system-design-interview/chapter04/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>Objectives you should have achieved so far:  * Agreed on overall goals &amp; features  * Sketched out a high-level blueprint of the design  * Obtained feedback about the high-level design  * Received initial ideas on areas you need to focus on in the deep dive</p> <p>At this stage, you should work with interviewer to prioritize which components you should focus on.</p> <p>Example things you might have to focus on:  * High-level design.  * System performance characteristics.  * In most cases, dig into the details of some system component.</p> <p>What details could you dig into? Some examples:  * For URL shortener - the hash function which converts long URLs into small ones  * For Chat system - reducing latency and supporting online/offline status</p> <p>Time management is essential - don't get carried away with details which don't show off your skills. For example, don't get carried away about how facebook's EdgeRank algorithm works as it doesn't demonstrate your design skills.</p> <p>Example design deep dive for feed publishing: </p> <p>Example design deep dive for news feed building: </p>"},{"location":"system-design-interview/chapter04/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>At this stage, the interviewer might ask you some follow-up questions or give you the freedom to discuss anything you want.</p> <p>A few directions to follow:  * Identify system bottlenecks and discuss improvements. No design is perfect, there are always things to improve.  * Give a recap of your design.  * Failure modes - server failure, network loss, etc.  * Operational issues - monitoring, alerting, rolling out the system.  * What needs to change to support the next scale curve? Eg 1mil -&gt; 10mil users  * Propose other refinements.</p> <p>Do's:  * Ask for clarification, don't assume your assumption is correct.  * Understand problem requirements.  * There is no right or perfect answer. A solution for a small company is different from one for a larger one.  * Let interviewer know what you're thinking.  * Suggest multiple approaches.  * Agree on blueprint and then design the most critical components first.  * Share ideas with interviewer. They should work with you.</p> <p>Dont's:  * Come unprepared for typical interview questions.  * Jump into a solution without understanding the requirements first.  * Don't go into too much details on a single component at first. Design at a high-level initially.  * Feel free to ask for hints if you get stuck.  * Communicate, don't think silently.</p>"},{"location":"system-design-interview/chapter04/#time-allocation-on-each-step","title":"Time allocation on each step","text":"<p>45 minutes are not enough to cover any design into sufficient detail.</p> <p>Here's a rough guide on how much time you should spend on each step:  * Understand problem &amp; establish design scope - 3-10m  * Propose high-level design &amp; get buy-in - 10-15m  * Design deep dive - 10-25m  * Wrap-up - 3-5m</p>"},{"location":"system-design-interview/chapter05/","title":"Design a Rate Limiter","text":"<p>The rate limiter's purpose in a distributed system is to control the rate of traffic sent from clients to a given server.</p> <p>It controls the maximum number of requests allowed in a given time period.  If the number of requests exceeds the threshold, the extra requests are dropped by the rate limiter.</p> <p>Examples:</p> <ul> <li>User can write no more than 2 posts per second.</li> <li>You can create 10 accounts max per day from the same IP.</li> <li>You can claim rewards max 10 times per week.</li> </ul> <p>Almost all APIs have some sort of rate limiting - eg Twitter allows 300 tweets per 3h max.</p> <p>What are the benefits of using a rate limiter?</p> <ul> <li>Prevents DoS attacks.</li> <li>Reduces cost - fewer servers are allocated to lower priority APIs.     Also, you might have a downstream dependency which charges you on a per-call basis, eg making a payment, retrieving health records, etc.</li> <li>Prevents servers from getting overloaded.</li> </ul>"},{"location":"system-design-interview/chapter05/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>There are multiple techniques which you can use to implement a rate limiter, each with its pros and cons.</p> <p>Example Candidate-Interviewer conversation:</p> <ul> <li>C: What kind of rate limiter are we designing? Client-side or server-side?</li> <li>I: Server-side</li> <li>C: Does the rate limiter throttle API requests based on IP, user ID or anything else?</li> <li>I: The system should be flexible enough to support different throttling rules.</li> <li>C: What's the scale of the system? Startup or big company?</li> <li>I: It should handle a large number of requests.</li> <li>C: Will the system work in a distributed environment?</li> <li>I: Yes.</li> <li>C: Should it be a separate service or a library?</li> <li>I: Up to you.</li> <li>C: Do we need to inform throttled users?</li> <li>I: Yes.</li> </ul> <p>Warning</p> <p>Summary of requirements:   - Accurately limit excess requests   - Low latency &amp; as little memory as possible   - Distributed rate limiting.   - Exception handling   - High fault tolerance - if cache server goes down, rate limiter should continue functioning.</p>"},{"location":"system-design-interview/chapter05/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>We'll stick with a simple client-server model for simplicity.</p>"},{"location":"system-design-interview/chapter05/#where-to-put-the-rate-limiter","title":"Where to put the rate limiter?","text":"<p>It can be implemented either client-side, server-side or as a middleware.</p> <p>Client-side - Unreliable, because client requests can easily be forged by malicious actors. We also might not have control over client implementation.</p> <p>Server-side: </p> <p>As a middleware between client and server: </p> <p>How it works, assuming 2 requests per second are allowed: </p> <p>In cloud microservices, rate limiting is usually implemented in the API Gateway. This service supports rate limiting, ssl termination, authentication, IP whitelisting, serving static content, etc.</p> <p>So where should the rate limiter be implemented? On the server-side or in the API gateway?</p> <p>It depends on several things:  -  Current tech stack - if you're implementing it server-side, then your language should be sufficient enough to support it.  -  If implemented on the server-side, you have control over the rate limiting algorithm.  -  If you already have an API gateway, you might as well add the rate limiter in there.  -  Building your own rate limiter takes time. If you don't have sufficient resources, consider using an off-the-shelf third-party solution instead.</p>"},{"location":"system-design-interview/chapter05/#algorithms-for-rate-limiting","title":"Algorithms for rate limiting","text":"<p>There are multiple algorithms for rate limiting, each with its pros and cons.</p> <p>Some of the popular algorithms - token bucket, leaking bucket, fixed window counter, sliding window log, sliding window counter.</p>"},{"location":"system-design-interview/chapter05/#token-bucket-algorithm","title":"Token bucket algorithm","text":"<p>Simple, well understood and commonly used by popular companies. Amazon and Stripe use it for throttling their APIs.</p> <p></p> <p>It works as follows:  -  There's a container with predefined capacity  -  Tokens are periodically put in the bucket  -  Once full, no more tokens are added  -  Each request consumes a single token  -  If no tokens left, request is dropped</p> <p></p> <p>There are two parameters for this algorithm:</p> <ul> <li>Bucket size - maximum number of tokens allowed in the bucket</li> <li>Refill rate - number of tokens put into the bucket every second</li> </ul> <p>How many buckets do we need? - depends on the requirements:</p> <ul> <li>We might need different buckets per API endpoints if we need to support 3 tweets per second, 5 posts per second, etc.</li> <li>Different buckets per IP if we want to make IP-based throttling.</li> <li>A single global bucket if we want to globally setup 10k requests per second max.</li> </ul> <p>Warning</p> <p>Pros:  -  Easy to implement  -  Memory efficient  -  Throttling gets activated in the event of sustained high traffic only. If bucket size is large, this algorithm supports short bursts in traffic as long as they're not prolonged.  Cons:  -  Parameters might be challenging to tune properly</p>"},{"location":"system-design-interview/chapter05/#leaking-bucket-algorithm","title":"Leaking bucket algorithm","text":"<p>Similar to token bucket algorithm, but requests are processed at a fixed rate.</p> <p>How it works:  -  When request arrives, system checks if queue is full. If not, request is added to the queue, otherwise, it is dropped.  -  Requests are pulled from the queue and processed at regular intervals. </p> <p>Parameters:  -  Bucket size - aka the queue size. It specifies how many requests will be held to be processed at fixed intervals.  -  Outflow rate - how many requests to be processed at fixed intervals.</p> <p>Shopify uses leaking bucket for rate-limiting.</p> <p>Warning</p> <p>Pros:   -  Memory efficient   -  Requests processed at fixed interval. Useful for use-cases where a stable outflow rate is required.</p> <p>Cons:   -  A burst of traffic fills up the queue with old requests. Recent requests will be rate limited.   -  Parameters might not be easy to tune.</p>"},{"location":"system-design-interview/chapter05/#fixed-window-counter-algorithm","title":"Fixed window counter algorithm","text":"<p>How it works:  -  Time is divided in fix windows with a counter for each one  -  Each request increments the counter  -  Once the counter reaches the threshold, subsequent requests in that window are dropped </p> <p>One major problem with this approach is that a burst of traffic in the edges can allow more requests than allowed to pass through: </p> <p>Warning</p> <p>Pros:   -  Memory efficient   -  Easy to understand   -  Resetting available quota at the end of a unit of time fits certain use cases  Cons:   -  Spike in traffic could cause more requests than allowed to go through a given time window</p>"},{"location":"system-design-interview/chapter05/#sliding-window-log-algorithm","title":"Sliding window log algorithm","text":"<p>To resolve the previous algorithm's issue, we could use a sliding time window instead of a fixed one.</p> <p>How it works:  -  Algorithm keeps track of request timestamps. Timestamp data is usually kept in a cache, such as Redis sorted set.  -  When a request comes in, remove outdated timestamps.  -  Add timestamp of the new request in the log.  -  If the log size is same or lower than threshold, request is allowed, otherwise, it is rejected.</p> <p>Note that the 3rd request in this example is rejected, but timestamp is still recorded in the log: </p> <p>Warning</p> <p>Pros:   -  Rate limiting accuracy is very high  Cons:   -  Memory footprint is very high</p>"},{"location":"system-design-interview/chapter05/#sliding-window-counter-algorithm","title":"Sliding window counter algorithm","text":"<p>A hybrid approach which combines the fixed window + sliding window log algorithms. </p> <p>How it works:</p> <ul> <li>Maintain a counter for each time window. Increment for given time window on each request.</li> <li>Derive sliding window counter = <code>prev_window * prev_window_overlap + curr_window * curr_window_overlap</code> (see screenshot above)</li> <li>If counter exceeds threshold, request is rejected, otherwise it is accepted.</li> </ul> <p>Warning</p> <p>Pros:   -  Smooths out spikes in traffic as rate is based on average rate of previous window   -  Memory efficient</p> <p>Cons:   -  Not very accurate rate limiting, as it's based on overlaps. But experiments show that only ~0.003% of requests are inaccurately accepted.</p>"},{"location":"system-design-interview/chapter05/#high-level-architecture","title":"High-level architecture","text":"<p>We'll use an in-memory cache as it's more efficient than a database for storing the rate limiting buckets - eg Redis. </p> <p>How it works:</p> <ul> <li>Client sends request to rate limiting middleware</li> <li>Rate limiter fetches counter from corresponding bucket &amp; checks if request is to be let through</li> <li>If request is let through, it reaches the API servers</li> </ul>"},{"location":"system-design-interview/chapter05/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>What wasn't answered in the high-level design:</p> <ul> <li>How are rate limiting rules created?</li> <li>How to handle rate-limited requests?</li> </ul> <p>Let's check those topics out, along with some other topics.</p>"},{"location":"system-design-interview/chapter05/#rate-limiting-rules","title":"Rate limiting rules","text":"<p>Example rate limiting rules, used by Lyft for 5 marketing messages per day: </p> <p>Another example \\w max login attempts in a minute: </p> <p>Rules like these are generally written in config files and saved on disk.</p>"},{"location":"system-design-interview/chapter05/#exceeding-the-rate-limit","title":"Exceeding the rate limit","text":"<p>When a request is rate limited, a 429 (too many requests) error code is returned.</p> <p>In some cases, the rate-limited requests can be enqueued for future processing.</p> <p>We could also include some additional HTTP headers to provide additional metadata info to clients: <pre><code>X-Ratelimit-Remaining: The remaining number of allowed requests within the window.\nX-Ratelimit-Limit: It indicates how many calls the client can make per time window.\nX-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again without being throttled.\n</code></pre></p>"},{"location":"system-design-interview/chapter05/#detailed-design","title":"Detailed design","text":"<ul> <li>Rules are stored on disk, workers populate them periodically in an in-memory cache.</li> <li>Rate limiting middleware intercepts client requests.</li> <li>Middleware loads the rules from the cache. It also fetches counters from the redis cache.</li> <li>If request is allowed, it proceeds to API servers. If not, a 429 HTTP status code is returned. Then, request is either dropped or enqueued.</li> </ul>"},{"location":"system-design-interview/chapter05/#rate-limiter-in-a-distributed-environment","title":"Rate limiter in a distributed environment","text":"<p>How will we scale the rate limited beyond a single server?</p> <p>There are several challenges to consider:</p> <ul> <li>Race condition</li> <li>Synchronization</li> </ul> <p>In case of race conditions, the counter might not be updated correctly when mutated by multiple instances: </p> <p>Locks are a typical way to solve this issue, but they are costly. Alternatively, one could use Lua scripts or Redis sorted sets, which solve the race conditions.</p> <p>If we maintain user information within the application memory, the rate limiter is stateful and we'll need to use sticky sessions to make sure requests from the same user is handled by the same rate limiter instance. </p> <p>To solve this issue, we can use a centralized data store (eg Redis) so that the rate limiter instances are stateless. </p>"},{"location":"system-design-interview/chapter05/#performance-optimization","title":"Performance optimization","text":"<p>There are two things we can do as a performance optimization for our rate limiters:</p> <ul> <li>Multi-data center setup - so that users interact with instances geographically close to them.</li> <li>Use eventual consistency as a synchronization model to avoid excessive locking.</li> </ul>"},{"location":"system-design-interview/chapter05/#monitoring","title":"Monitoring","text":"<p>After the rate limiter is deployed, we'd want to monitor if it's effective.</p> <p>To do so, we need to track:</p> <ul> <li>If the rate limiting algorithm is effective</li> <li>If the rate limiting rules are effective</li> </ul> <p>If too many requests are dropped, we might have to tune some of the rules or the algorithm parameters.</p>"},{"location":"system-design-interview/chapter05/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We discussed a bunch of rate-limiting algorithms:</p> <ul> <li>Token bucket - good for supporting traffic bursts.</li> <li>Leaking bucket - good for ensuring consistent inbound request flow to downstream services</li> <li>Fixed window - good for specific use-cases where you want time divided in explicit windows</li> <li>Sliding window log - good when you want high rate-limiting accuracy at the expense of memory footprint.</li> <li>Sliding window counter - good when you don't want 100% accuracy with a very low memory footprint.</li> </ul> <p>Additional talking points if time permits:</p> <ul> <li>Hard vs. soft rate limiting</li> <li>Hard - requests cannot exceed the specified threshold </li> <li> <p>Soft - requests can exceed threshold for some limited time</p> Feature Hard Rate Limiting Soft Rate Limiting Behavior at Limit Immediately rejects new requests Allows a temporary burst over the limit Strictness Very strict, no exceptions Flexible, has a grace period or burst allowance Use Case Protecting critical resources, enforcing strict API usage tiers Ensuring good user experience, handling temporary traffic spikes Analogy A locked gate once full A waiting area for temporary overflow </li> <li> <p>Rate limiting at different layers - L7 (application) vs L3 (network)</p> </li> <li> <p>At its core, Layer 7 rate limiting operates at the application level, affording it a deep understanding of the traffic it's managing. This allows for the creation of sophisticated and context-aware rules. Instead of just looking at the source of the traffic, Layer 7 rate limiting can inspect the content of the requests themselves.</p> </li> <li> <p>Layer 3 rate limiting functions at the network layer, primarily focusing on the IP protocol. Its main concern is the volume of traffic originating from specific IP addresses. </p> Feature Layer 7 (Application) Layer 3 (Network) Focus Application-specific requests and content IP addresses and traffic volume Granularity High (User ID, API key, URL path, etc.) Low (Primarily IP address) Typical Use Cases API protection, brute-force prevention, content scraping mitigation Volumetric DDoS mitigation Resource Intensity High Low Complexity High Low Accuracy High, less prone to false positives Lower, can block legitimate users </li> <li> <p>Client-side measures to avoid being rate limited:</p> <ul> <li>Client-side cache to avoid excessive calls</li> <li>Understand limit and avoid sending too many requests in a small time frame</li> <li>Gracefully handle exceptions due to being rate limited</li> <li>Add sufficient back-off and retry logic</li> </ul> </li> </ul>"},{"location":"system-design-interview/chapter06/","title":"Design Consistent Hashing","text":"<p>For horizontal scaling, it is important to distribute requests across servers efficiently.</p> <p>Consistent hashing is a common approach to achieve this.</p>"},{"location":"system-design-interview/chapter06/#the-rehashing-problem","title":"The rehashing problem","text":"<p>One way to determine which server a request gets routed to is by applying a simple hash+module formula: <pre><code>serverIndex = hash(key) % N, where N is the number of servers\n</code></pre></p> <p>This makes it so requests are distributed uniformly across all servers. However, whenever new servers are added or removed, the result of the above equation is very different, meaning that a lot of requests will get rerouted across servers.</p> <p>Note</p> <p>This causes a lot of cache misses as clients will be connected to new instances which will have to fetch the user data from cache all over again.</p>"},{"location":"system-design-interview/chapter06/#consistent-hashing","title":"Consistent hashing","text":"<p>Consistent hashing is a technique which allows only a K/N servers to be remapped whenever N changes, where K is the number of keys.</p> <p>For example, K=100, N=10 -&gt; 10 re-mappings, compared to close to 100 in the normal scenario.</p>"},{"location":"system-design-interview/chapter06/#hash-space-and-hash-ring","title":"Hash space and hash ring","text":"<p>A hash ring is a visualization of the possible key space of a given hash algorithm, which is combined into a ring-like structure: </p> <p>Note</p> <p>Murmurhash3 is popular because of its speed on x86 platforms and good distribution for low collision</p>"},{"location":"system-design-interview/chapter06/#hash-servers","title":"Hash servers","text":"<p>Using the same hash function for the requests, we map the servers based on server IP or name onto the hash ring: </p>"},{"location":"system-design-interview/chapter06/#hash-keys","title":"Hash keys","text":"<p>The hashes of the requests also get resolved somewhere along the hash ring. </p> <p>Warning</p> <p>We're not using the modulo operator since hash ring contains all possible value of output of hash function in ring.</p> <p></p>"},{"location":"system-design-interview/chapter06/#server-lookup","title":"Server lookup","text":"<p>Now, to determine which server is going to serve each request, we go clockwise from the request's hash until we reach the first server hash: </p>"},{"location":"system-design-interview/chapter06/#add-a-server","title":"Add a server","text":"<p>Via this approach, adding a new server causes only one of the requests to get remapped to a new server: </p>"},{"location":"system-design-interview/chapter06/#remove-a-server","title":"Remove a server","text":"<p>Likewise, removing a server causes only a single request to get remapped: </p>"},{"location":"system-design-interview/chapter06/#two-issues-in-the-basic-approach","title":"Two issues in the basic approach","text":"<p>The first problem with this approach is that hash partitions can be uneven across servers: </p> <p>The second problem derives from the first - it is possible that requests are unevenly distributed across servers: </p>"},{"location":"system-design-interview/chapter06/#virtual-nodes","title":"Virtual nodes","text":"<p>To solve this issue, we can map a servers on the hash ring multiple times, creating virtual nodes and assigning multiple partitions to the same server: </p> <p>Now, a request is mapped to the closest virtual node on the hash ring: </p> <p>The more virtual nodes we have, the more evenly distributed the requests will be. This will enable to add server with heterogenous resources to be part of ring.</p> <p>Note</p> <p>An experiment showed that between 100-200 virtual nodes leads to a standard deviation between 5-10% for key distrubution among virtual node.</p>"},{"location":"system-design-interview/chapter06/#wrap-up","title":"Wrap up","text":"<p>Benefits of consistent hashing:  * Very low number of keys are distributed in a re-balancing event  * Easy to scale horizontally as data is uniformly distributed  * Hotspot issue is mitigated by uniformly distributing data, related to eg a celebrity, which is often accessed</p> <p>Examples of real-world applications of consistent hashing:  * Amazon's DynamoDB partitioning component  * Data partitioning in Cassandra  * Discord chat application  * Akamai CDN  * Maglev network load balancer</p>"},{"location":"system-design-interview/chapter07/","title":"Design a Key-Value Store","text":"<p>Key-value stores are a type of non-relational databases.  * Each unique identifier is stored as a key with a value associated to it.  * Keys must be unique and can be plain text or hashes.  * Performance-wise, short keys work better.</p> <p>Example keys:  * plain-text - \"last_logged_in_at\"  * hashed key - <code>253DDEC4</code></p> <p>We're now about to design a key-value store which supports:  * <code>put(key, value)</code> - insert <code>value</code> associated to <code>key</code>  * <code>get(key)</code> - get <code>value</code> associated to <code>key</code></p>"},{"location":"system-design-interview/chapter07/#understand-the-problem-and-establish-design-scope","title":"Understand the problem and establish design scope","text":"<p>Warning</p> <ul> <li>There's always a trade-off to be made between read/write and memory usage.</li> <li>Another trade-off is between consistency and availability.</li> </ul> <p>Here are the characteristics we're striving to achieve:  * Key-value pair size is small - 10kb(what if this is 10MB,100MB ,what would change?)  * We need to be able to store a lot of data.  * High availability - system responds quickly even during failures.  * High scalability - system can be scaled to support large data sets.  * Automatic scaling - addition/deletion of servers should happen automatically based on traffic.  * Tunable consistency.  * Low latency.</p>"},{"location":"system-design-interview/chapter07/#single-server-key-value-store","title":"Single server key-value store","text":"<p>Single server key-value stores are easy to develop.</p> <p>We can just maintain an in-memory hash map which stores the key-value pairs.</p> <p>Memory however, can be a bottleneck, as we can't fit everything in-memory. Here are our options to scale:  * Data compression  * Store only frequently used data in-memory. The rest store on disk.</p> <p>Even with these optimizations, a single server can quickly reach its capacity.</p>"},{"location":"system-design-interview/chapter07/#distributed-key-value-store","title":"Distributed key-value store","text":"<p>A distributed key-value store consists of a distributed hash table, which distributes keys across many nodes.</p> <p>When developing a distributed data store, we need to factor in the CAP theorem</p>"},{"location":"system-design-interview/chapter07/#cap-theorem","title":"CAP Theorem","text":"<p>This theorem states that a data store can't provide more than two of the following guarantees - consistency, availability, partition tolerance;  * Consistency - all clients see the same data at the same time, no matter which node they're connected to.  * Availability - all clients get a response, regardless of which node they connect to.  * Partition tolerance - A network partition means that not all nodes within the cluster can communicate. Partition tolerance means that the system is operational even in such circumstances. </p> <p>A distributed system which supports consistency and availability cannot exist in the real world as network failures are inevitable.</p> <p>Example distributed data store in an ideal situation: </p> <p>In the real world, a network partition can occur which hinders communication with eg node 3: </p> <p>If we favor consistency over availability, all write operations need to be blocked when the above scenario occurs.</p> <p>If we favor availability on the other hand, the system continues accepting reads and writes, risking some clients receiving stale data. When node 3 is back online, it will be re-synced with the latest data.</p> <p>What you choose is something you need to clarify with the interviewer. There are different trade-offs with each option.</p>"},{"location":"system-design-interview/chapter07/#system-components","title":"System components","text":"<p>This section goes through the key components needed to build a distributed key-value store.</p>"},{"location":"system-design-interview/chapter07/#data-partition","title":"Data partition","text":"<p>For a large enough data set, it is infeasible to maintain it on a single server. Hence, we can split the data into smaller partitions and distribute them across multiple nodes.</p> <p>The challenge then, is to distribute data evenly and minimize data movement when the cluster is resized.</p> <p>Both these problems can be addressed using consistent hashing (discussed in previous chapter):  * Servers are put on a hash ring  * Keys are hashed and put on the closest server in clockwise direction </p> <p>This has the following advantages:  * Automatic scaling - servers can be added/removed at will with minimal impact on key location  * Heterogeneity - servers with higher capacity can be allocated with more virtual nodes</p>"},{"location":"system-design-interview/chapter07/#data-replication","title":"Data replication","text":"<p>To achieve high availability &amp; reliability, data needs to be replicated on multiple nodes.</p> <p>We can achieve that by allocating a key to multiple nodes on the hash ring: </p> <p>[!Note]  One caveat to keep in mind that your key might get allocated to virtual nodes mapped to the same physical node.  To avoid this, we can only choose unique physical nodes when replicating data.</p> <p>An additional reliability measure is to replicate data across multiple data centers as nodes in the same data centers can fail at the same time.</p>"},{"location":"system-design-interview/chapter07/#consistency","title":"Consistency","text":"<p>Since data is replicated, it must be synchronized.</p> <p>Quorum consensus can guarantee consistency for both reads and writes:  * N - number of replicas  * W - write quorum. Write operations must be acknowledged by W nodes.  * R - read quorum. Read operations must be acknowledged by R nodes. </p> <p>The configuration of W and R is a trade-off between latency and consistency.  * W = 1, R = 1 -&gt; low latency, eventual consistency  * W + R &gt; N -&gt; strong consistency, high latency</p> <p>Other configurations:  * R = 1, W = N -&gt; strong consistency, fast reads, slow writes  * R = N, W = 1 -&gt; strong consistency, fast writes, slow reads</p>"},{"location":"system-design-interview/chapter07/#consistency-models","title":"Consistency models","text":"<p>There are multiple flavors of consistency we can tune our key-value store for:  * Strong consistency - read operations return a value corresponding to most up-to-date data. Clients never see stale data.  * Weak consistency - read operations might not see the most up-to-date data.  * Eventual consistency - read operations might not see most up-to-date data, but with time, all keys will converge to the latest state.</p> <p>Strong consistency usually means a node not accepting reads/writes until all nodes have acknowledged a write. This is not ideal for highly available systems as it can block new operations.</p> <p>Eventual consistency (supported by Cassandra and DynamoDB) is preferable for our key-value store. This allows concurrent writes to enter the system and clients need to reconcile the data mismatches.</p>"},{"location":"system-design-interview/chapter07/#inconsistency-resolution-versioning","title":"Inconsistency resolution: versioning","text":"<p>Replication provides high availability, but it leads to data inconsistencies across replicas.</p> <p>Example inconsistency: </p> <p>This kind of inconsistency can be resolved using a versioning system using a vector clock. A vector clock is a [server, version] pair, associated with a data item. Each time a data item is changed in a server, it's associated vector clock changes to [server_id, curr_version+1].</p> <p>Example inconsistency resolution:   * Client writes D1, handled by Sx, which writes version [Sx, 1]  * Another client reads D1, updates it and Sx increments version to [Sx, 2]  * Client writes D3 based on D2 in Sy -&gt; D3([Sx, 2][Sy, 1]).  * Simultaneously, another one writes D4 in Sz -&gt; D4([Sx, 2][Sz, 1])  * A client reads D3 and D4 and detects a conflict. It makes a resolution and adds the updated version in Sx -&gt; D5([Sx, 3][Sy, 1][Sz, 1])</p> <p>A conflict is detected by checking whether a version is an ancestor of another one. That can be done by verifying that all version stamps are less than or equal to the other one.  * [s0, 1][s1, 1] is an ancestor of [s0, 1][s1, 2] -&gt; no conflict.  * [s0, 1] is not an ancestor of [s1, 1] -&gt; conflict needs to be resolved.</p> <p>This conflict resolution technique comes with trade-offs:  * Client complexity is increased as clients need to resolve conflicts.  * Vector clocks can grow rapidly, increasing the memory footprint of each key-value pair.</p>"},{"location":"system-design-interview/chapter07/#handling-failures","title":"Handling failures","text":"<p>At a large enough scale, failures are inevitable. It is important to determine your error detection &amp; resolution strategies.</p>"},{"location":"system-design-interview/chapter07/#failure-detection","title":"Failure detection","text":"<p>In a distributed system, it is insufficient to conclude that a server is down just because you can't reach it. You need at least another source of information.</p> <p>One approach to do that is to use all-to-all multi-casting. This, however, is inefficient when there are many servers in the system. </p> <p>A better solution is to use a decentralized failure detection mechanism, such as a gossip protocol:  * Each node maintains a node membership list with member IDs and heartbeat counters.  * Each node periodically increments its heartbeat counter  * Each node periodically sends heartbeats to a random set of other nodes, which propagate it onwards.  * Once a node receives a heartbeat, its membership list is updated.  * If a heartbeat is not received after a given threshold, the member is marked offline.</p> <p></p> <p>In the above scenario, s0 detects that s2 is down as no heartbeat is received for a long time.  It propagates that information to other nodes which also verify that the heartbeat hasn't been updated. Hence, s2 is marked offline.</p>"},{"location":"system-design-interview/chapter07/#handling-temporary-failures","title":"Handling temporary failures","text":"<p>A nice little trick to improve availability in the event of failures is hinted handoff.</p> <p>What it means is that if a server if temporarily offline, you can promote another healthy service in its place to process data temporarily. After the server is back online, the data &amp; control is handed back to it.</p>"},{"location":"system-design-interview/chapter07/#handling-permanent-failures","title":"Handling permanent failures","text":"<p>Hinted handoff is used when the failure is intermittent.</p> <p>If a replica is permanently unavailable, we implement an anti-entropy protocol to keep the replicas in-sync.</p> <p>This is achieved by leveraging merkle trees in order to reduce the amount of data transmitted and compared to a minimum.</p> <p>The merkle tree works by building a tree of hashes where leaf nodes are buckets of key-value pairs.</p> <p>If any of the buckets across two replicas is different, then the merkle tree's hashes will be different all the way to the root: </p> <p>Using merkle trees, two replicas will compare only as much data as is different between them, instead of comparing the entire data set.</p>"},{"location":"system-design-interview/chapter07/#handling-data-center-outage","title":"Handling data center outage","text":"<p>A data center outage could happen due to a natural disaster or serious hardware failure.</p> <p>To ensure resiliency, make sure your data is replicated across multiple data centers.</p>"},{"location":"system-design-interview/chapter07/#system-architecture-diagram","title":"System architecture diagram","text":"<p>Main features:  * Clients communicate with the key-value store through a simple API  * A coordinator is a proxy between the clients and the key-value store  * Nodes are distributed on the ring using consistent hashing  * System is decentralized, hence adding and removing nodes is supported and can be automated  * Data is replicated at multiple nodes  * There is no single point of failure</p> <p>Some of the tasks each node is responsible for: </p>"},{"location":"system-design-interview/chapter07/#write-path","title":"Write path","text":"<p>  * Write requests are persisted in a commit log  * Data is saved in the memory cache  * When memory cache is full or reaches a given threshold, data is flushed to an SSTable on disk</p> <p>SSTable == Sorted String Table. Holds a sorted list of key-value pairs.</p>"},{"location":"system-design-interview/chapter07/#read-path","title":"Read path","text":"<p>Read path when data is in memory: </p> <p>Read path when data is not in memory:   * If data is in memory, fetch it from there. Otherwise, find it in the SSTable.  * A bloom filter is used for efficient lookup in the SSTable.  * The SSTables returns the resulting data, which is returned to the client</p>"},{"location":"system-design-interview/chapter07/#summary","title":"Summary","text":"<p>We covered a lot of concepts and techniques, here's a summary: | Goal/Problems               | Technique                                             | |-----------------------------|-------------------------------------------------------| | Ability to store big data   | Use consistent hashing to spread load across servers  | | High availability reads     | Data replication Multi-datacenter setup               | | Highly available writes     | Versioning and conflict resolution with vector clocks | | Dataset partition           | Consistent Hashing                                    | | Incremental scalability     | Consistent Hashing                                    | | Heterogeneity               | Consistent Hashing                                    | | Tunable consistency         | Quorum consensus                                      | | Handling temporary failures | Sloppy quorum and hinted handoff                      | | Handling permanent failures | Merkle tree                                           | | Handling data center outage | Cross-datacenter replication                          |</p>"},{"location":"system-design-interview/chapter08/","title":"Design a Unique ID Generator in Distributed Systems","text":"<p>We need to design a unique ID generator, compatible with distributed systems.</p> <p>A primary key with auto_increment won't work here, because generating IDs across multiple database servers has high latency.</p>"},{"location":"system-design-interview/chapter08/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What characteristics should the unique IDs have?</li> <li>I: They should be unique and sortable.</li> <li>C: For each record, does the ID increment by 1?</li> <li>I: IDs increment by time, but not necessarily by 1.</li> <li>C: Do IDs contain only numerical values?</li> <li>I: Yes</li> <li>C: What is the ID length requirement?</li> <li>I: 64 bits</li> <li>C: What's the system scale?</li> <li>I: We should be able to generate 10,000 IDs per second</li> </ul>"},{"location":"system-design-interview/chapter08/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>Here's the options we'll consider:  * Multi-master replication  * Universally-unique IDs (UUIDs)  * Ticket server  * Twitter snowflake approach</p>"},{"location":"system-design-interview/chapter08/#multi-master-replication","title":"Multi-master replication","text":"<p>This uses the database's auto_increment feature, but instead of increasing by 1, we increase by K where K = number of servers.</p> <p>This solves the scalability issues as id generation is confined within a single server, but it introduces other challenges:  * Hard to scale \\w multiple data centers  * IDs do not go up in time across servers  * Adding/removing servers breaks this mechanism</p>"},{"location":"system-design-interview/chapter08/#uuid","title":"UUID","text":"<p>A UUID is a 128-byte unique ID.</p> <p>The probability of UUID collision across the whole world is very little.</p> <p>Example UUID - <code>09c93e62-50b4-468d-bf8a-c07e1040bfb2</code>.</p> <p>Pros:  * UUIDs can be generated independently across servers without any synchronization or coordination.  * Easy to scale.</p> <p>Cons:  * IDs are 128 bytes, which doesn't fit our requirement  * IDs do not increase with time  * IDs can be non-numeric</p>"},{"location":"system-design-interview/chapter08/#ticket-server","title":"Ticket server","text":"<p>A ticket server is a centralized server for generating unique primary keys across multiple services: </p> <p>Pros:  * Numeric IDs  * Easy to implement &amp; works for small &amp; medium applications</p> <p>Cons:  * Single point of failure.  * Additional latency due to network call.</p>"},{"location":"system-design-interview/chapter08/#twitter-snowflake-approach","title":"Twitter snowflake approach","text":"<p>Twitter's snowflake meets our design requirements because it is sortable by time, 64-bits and can be generated independently in each server. </p> <p>Breakdown of the different sections:  * Sign bit - always 0. Reserved for future use.  * Timestamp - 41 bits. Milliseconds since epoch (or since custom epoch). Allows 69 years max.  * Datacenter ID - 5 bits, which enables 32 data centers max.  * Machine ID - 5 bits, which enables 32 machines per data center.  * Sequence number - For every generated ID, the sequence number is incremented. Reset to 0 on every millisecond.</p>"},{"location":"system-design-interview/chapter08/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>We'll use twitter's snowflake algorithm as it fits our needs best.</p> <p>Datacenter ID and machine ID are chosen at startup time. The rest is determined at runtime.</p> <pre><code>func (n *singleWorker) NextID() (id SingleWorkerID, err error) {\n\n    n.mu.Lock()\n\n    now := time.Now().UnixNano() / 1000000\n\n    if now &lt; n.lastTimeStamp {\n        return SingleWorkerID(0), fmt.Errorf(\"Clock moved backwards. Refusing to generate id for %d milliseconds\", n.lastTimeStamp-now)\n    }\n\n    if n.lastTimeStamp == now {\n        n.sequence = (n.sequence + 1) &amp; sequenceMask\n\n        if n.sequence == 0 {\n            now = snowflake.TilNextMillis(n.lastTimeStamp)\n        }\n    } else {\n        n.sequence = 0\n    }\n\n    n.lastTimeStamp = now\n\n    id = SingleWorkerID((now-snowflake.TW_EPOCH)&lt;&lt;timestampLeftShift |\n        (n.nodeID &lt;&lt; nodeIdShift) |\n        n.sequence)\n\n    n.mu.Unlock()\n    return\n}\n</code></pre>"},{"location":"system-design-interview/chapter08/#step-4-wrap-up","title":"Step 4 - wrap up","text":"<p>We explored multiple ways to generate unique IDs and settled on snowflake eventually as it serves our purpose best.</p>"},{"location":"system-design-interview/chapter08/#additional-talking-points","title":"Additional talking points:","text":""},{"location":"system-design-interview/chapter08/#clock-synchronization-network-time-protocol-can-be-used-to-resolve-clock-inconsistencies-across-different-machinescpu-cores","title":"Clock synchronization - network time protocol can be used to resolve clock inconsistencies across different machines/CPU cores.","text":"<ul> <li>NTP is absolutely critical for the timestamp portion. By using NTP, every machine in the distributed system that generates Snowflake IDs agrees on the current time. This synchronization ensures:</li> <li>Global Monotonicity: IDs generated later will always have a greater timestamp value than IDs generated earlier, regardless of which machine created them. This allows you to sort records by their Snowflake ID and have them be chronologically ordered.</li> <li>Collision Avoidance: Without synchronized time, if a machine's clock were to go backward (e.g., after a reboot and incorrect sync), it could start generating timestamps it has already used, leading to duplicate IDs. NTP prevents this clock regression.</li> <li>In short, NTP acts as the conductor of an orchestra, ensuring every machine's clock \"plays\" in perfect time. This allows the timestamp component of the Snowflake ID to be a reliable and globally consistent measure of time, which is the foundation of the entire system.</li> </ul>"},{"location":"system-design-interview/chapter08/#section-length-tuning-we-could-sacrifice-some-sequence-number-bits-for-more-timestamp-bits-in-case-of-low-concurrency-and-long-term-applications","title":"Section length tuning - we could sacrifice some sequence number bits for more timestamp bits in case of low concurrency and long-term applications.","text":"<ul> <li>The 41 bits in a standard Snowflake ID can represent 2^41 milliseconds. This is a massive number, but it still has a limit: about 69 years.</li> <li>For many applications, like a social media post ID, a 69-year lifespan is perfectly fine. But for certain types of applications, it's a critical flaw.</li> <li>Applications That Outlive the Standard Timestamp</li> <li>Applications that deal with long-term, foundational data need a much longer horizon.</li> <li>Government and Civic Records: Think of land ownership deeds, birth certificates, or national archive records. These must remain valid and unique for hundreds of years.</li> <li>Financial Ledgers: Systems tracking long-term assets, mortgages, or foundational company shares need to operate far beyond a 70-year window.</li> <li>Core Scientific Data: Datasets from long-running experiments or astronomical observations need identifiers that won't clash for generations.</li> <li>For these systems, an ID generator that will stop working correctly after 69 years is not a viable option. When the timestamp \"rolls over,\" the system can no longer generate new IDs that are guaranteed to be greater than old ones, breaking the chronological sorting guarantee.</li> <li>How Adding Bits Solves the Problem<ul> <li>Adding bits to the timestamp exponentially increases its lifespan. 41 bits: ~2.2\u00d710^12ms = 69 years</li> <li>42 bits: ~4.4\u00d710^12 ms = 138 years (doubled the lifespan)</li> <li>43 bits: ~8.8\u00d710^12 ms = 276 years (quadrupled the lifespan)</li> <li>By adding just a couple of bits (often at the expense of sequence bits, as discussed), a designer can easily future-proof the ID generator, ensuring the application remains stable and reliable for centuries to come. It's a strategic trade-off that prioritizes the system's longevity over its ability to handle momentary, extreme traffic spikes.  ### High availability - ID generators are a critical component and must be highly available.</li> </ul> </li> <li>Decentralization: The Core Principle</li> <li>Instead of having one central service that hands out IDs (which would be a major bottleneck and a single point of failure), a Snowflake-style approach makes every machine its own ID generator.</li> <li>Each application server, or \"worker,\" that needs to create an ID can generate it locally without talking to any other service. This is the foundation of its high availability.</li> <li>How it works: Each machine is assigned a unique Machine ID (or Worker ID) during startup. This ID is embedded into every unique ID it generates. Because each machine has its own distinct ID, there's no risk of two different machines generating the same ID, even at the exact same millisecond.</li> <li>Fault Isolation and Resilience</li> <li>This decentralized model provides excellent fault tolerance.</li> <li>If one machine fails: The other machines are completely unaffected. They continue to generate their own unique IDs without interruption. The system as a whole remains available and can still create new records. The only impact is that one machine is temporarily out of the pool.</li> <li>No network dependency for generation: Once a machine has its Machine ID, it doesn't need to communicate with a central coordinator to create an ID. This makes it resilient to network partitions and latency issues that would cripple a centralized ID generator.</li> <li>The Role of a Coordinator (and Its Limits)</li> <li>While the ID generation is decentralized, the assignment of the unique Machine ID often requires a lightweight coordination service, like Apache ZooKeeper.</li> <li>On Startup: A worker machine will register with ZooKeeper to claim a unique Machine ID from a predefined pool (e.g., from 0 to 1023).</li> <li>During Operation: The machine operates independently. It does not need to talk to ZooKeeper again unless it reboots.</li> <li>This means ZooKeeper is only a dependency during the brief startup phase, not during the critical path of ID generation. Even if ZooKeeper goes down, all currently running machines will continue to function perfectly, ensuring high availability for the core service.</li> </ul>"},{"location":"system-design-interview/chapter09/","title":"Design a URL Shortener","text":"<p>We're tackling a classical system design problem - designing a URL shortening service like tinyurl.</p>"},{"location":"system-design-interview/chapter09/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: Can you give an example of how a URL shortening service works?</li> <li>I: Given URL <code>https://www.systeminterview.com/q=chatsystem&amp;c=loggedin&amp;v=v3&amp;l=long</code> and alias <code>https://tinyurl.com/y7keocwj</code>. You open the alias and get to the original URL.</li> <li>C: What is the traffic volume?</li> <li>I: 100 million URLs are generated per day.</li> <li>C: How long is the shortened URL?</li> <li>I: As short as possible</li> <li>C: What characters are allowed?</li> <li>I: numbers and letters</li> <li>C: Can shortened URLs be updated or deleted?</li> <li>I: For simplicity, let's assume they can't.</li> </ul> <p>Other functional requirements - high availability, scalability, fault tolerance.</p>"},{"location":"system-design-interview/chapter09/#back-of-the-envelope-calculation","title":"Back of the envelope calculation","text":"<ul> <li>100 mil URLs per day -&gt; ~1200 URLs per second.</li> <li>Assuming read-to-write ratio of 10:1 -&gt; 12000 reads per second.</li> <li>Assuming URL shortener will run for 10 years, we need to support 365bil records.</li> <li>Average URL length is 100 characters</li> <li>Storage requirements for 10y - 36.5 TB</li> </ul>"},{"location":"system-design-interview/chapter09/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":""},{"location":"system-design-interview/chapter09/#api-endpoints","title":"API Endpoints","text":"<p>We'll make a REST API.</p> <p>A URL shortening service needs two endpoints:  * <code>POST api/v1/data/shorten</code> - accepts long url and returns a short one.  * <code>GET api/v1/shortURL</code> - return long URL for HTTP redirection.</p>"},{"location":"system-design-interview/chapter09/#url-redirecting","title":"URL Redirecting","text":"<p>How it works: </p> <p>What's the difference between 301 and 302 statuses?  * 301 (Permanently moved) - indicates that the URL permanently points to the new URL. This instructs the browser to bypass the tinyurl service on subsequent calls.  * 302 (Temporarily moved) - indicates that the URL is temporarily moved to the new URL. Browser will not bypass the tinyurl service on future calls.</p> <p>Choose 301 if you want to avoid extra server load. Choose 302 if tracking analytics is important.</p> <p>Easiest way to implement the URL redirection is to store the <code>&lt;shortURL, longURL&gt;</code> pair in an in-memory hash-table.</p>"},{"location":"system-design-interview/chapter09/#url-shortening","title":"URL Shortening","text":"<p>To support the URL shortening, we need to find a suitable hash function.</p> <p>It needs to support hashing long URL to shortURL and mapping them back.</p> <p>Details are discussed in the detailed design.</p>"},{"location":"system-design-interview/chapter09/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>We'll explore the data model, hash function, URL shortening and redirection.</p>"},{"location":"system-design-interview/chapter09/#data-model","title":"Data model","text":"<p>In the simplified version, we're storing the URLs in a hash table. That is problematic as we'll run out of memory and also, in-memory doesn't persist across server reboot.</p> <p>That's why we can use a simple relational table instead: </p>"},{"location":"system-design-interview/chapter09/#hash-function","title":"Hash function","text":"<p>The hash value consists of characters <code>[0-9a-zA-Z]</code>, which gives a max of 62 characters.</p> <p>To figure out the smallest hash value we can use, we need to calculate n in <code>62^n &gt;= 365bil</code> -&gt; this results in <code>n=7</code>, which can support ~3.5 trillion URLs.</p> <p>For the hash function itself, we can either use <code>base62 conversion</code> or <code>hash + collision detection</code>.</p> <p>In the latter case, we can use something like MD-5 or SHA256, but only taking the first 7 characters. To resolve collisions, we can reiterate \\w an some padding to input string until there is no collision: </p> <p>The problem with this method is that we have to query the database to detect collision. Bloom filters could help in this case.</p> <p>Alternatively, we can use base62 conversion, which can convert an arbitrary ID into a string consisting of the 62 characters we need to support.</p> <p>Comparison between the two approaches: | Hash + collision resolution                                                                   | Base 62 conversion                                                                                                                   | |-----------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------| | Fixed short URL length.                                                                       | Short URL length is not fixed. It goes up with the ID.                                                                               | | Does not need a unique ID generator.                                                          | This option depends on a unique ID generator.                                                                                        | | Collision is possible and needs to be resolved.                                               | Collision is not possible because ID is unique.                                                                                      | | It\u2019s not possible to figure out the next available short URL because it doesn\u2019t depend on ID. | It is easy to figure out what is the next available short URL if ID increments by 1 for a new entry. This can be a security concern. |</p>"},{"location":"system-design-interview/chapter09/#url-shortening-deep-dive","title":"URL shortening deep dive","text":"<p>To keep our service simple, we'll use base62 encoding for the URL shortening.</p> <p>Here's the whole workflow: </p> <p>To ensure our ID generator works in a distributed environment, we can use Twitter's snowflake algorithm.</p>"},{"location":"system-design-interview/chapter09/#url-redirection-deep-dive","title":"URL redirection deep dive","text":"<p>We've introduced a cache as there are more reads than writes, in order to improve read performance:   * User clicks short URL  * Load balancer forwards the request to one of the service instances  * If shortURL is in cache, return the longURL directly  * Otherwise, fetch the longURL from the database and store in cache. If not found, then the short URL doesn't exist</p>"},{"location":"system-design-interview/chapter09/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We discussed:  * API design  * data model  * hash function  * URL shortening  * URL redirecting</p> <p>Additional talking points:  * Rate limiter - We can introduce a rate limiter to protect us against malicious actors, trying to make too many URL shortening requests.  * Web server scaling - We can easily scale the web tier by introducing more service instances as it's stateless.  * Database scaling - Replication and sharding are a common approach to scale the data layer.  * Analytics - Integrating analytics tracking in our URL shortener service can reap some business insights for clients such as \"how many users clicked the link\".  * Availability, consistency, reliability - At the core of every distributed systems. We'd leverage concepts already discussed in Chapter 02.</p>"},{"location":"system-design-interview/chapter10/","title":"Design a Web Crawler","text":"<p>We'll focus next on designing a web crawler - a classical system design problem.</p> <p>Web crawlers (aka robots) are used to discover new or updated content on the web, such as articles, videos, PDFs, etc. </p>"},{"location":"system-design-interview/chapter10/#use-cases","title":"Use-cases:","text":"<ul> <li>Search engine indexing - for creating a local index of a search engine, eg Google's Googlebot.</li> <li>Web archiving - collect data from the web and preserve it for future uses. </li> <li>Web mining - it can also be used for data mining. Eg finding important insights such as shareholder meetings for trading firms.</li> <li>Web monitoring - monitor the internet for copyright infringements or eg company internal information leaks.</li> </ul> <p>The complexities of building a web crawler depend on our target scale. It can be very simple (eg a student project) or a multi-year project, maintained by a dedicated team.</p>"},{"location":"system-design-interview/chapter10/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>How it works at a high-level:  - Given a set of URLs, download all the pages these URLs point to  - Extract URLs from the web pages  - Add the new URLs to the list of URLs to be traversed</p> <p>A real web crawler is much more complicated, but this is what it does in a nutshell.</p> <p>You'll need to clarify what kind of features your interviewer would like you to support exactly:  - C: What's the main purpose of the web crawler? Search engine indexing, data mining, something else?  - I: Search engine indexing  - C: How many web pages does it collect per month  - I: 1 billion  - C: What content types are included? HTML, PDF, images?  - I: HTML only  - C: Should we consider newly added/edited content?  - I: Yes  - C: Do we need to persist the crawled web pages?  - I: Yes, for 5 years  - C: What do we do with pages with duplicate content  - I: Ignore them</p> <p>This is an example conversation. It is important to go through this even if the project is simple. Your assumptions and the ones of your interviewer could differ.</p> <p>Other characteristics of a good web crawler:  - Scalable - it should be extremely efficient  - Robust - handle edge-cases such as bad HTML, infinite loops, server crashes, etc  - Polite - not make too many requests to a server within a short time interval  - Extensibility - it should be easy to add support for new types of content, eg images in the future</p>"},{"location":"system-design-interview/chapter10/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<p>Given 1 billion pages per month -&gt; ~400 pages per second Peak QPS = 800 pages per second</p> <p>Given average web page size is 500kb -&gt; 500 TB per month -&gt; 30 PB for 5y.</p>"},{"location":"system-design-interview/chapter10/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>What's going on in there?  - Seed URLs - These URLs are the starting point for crawlers. It's important to pick the seed URLs well in order to traverse the web appropriately.  - URL Frontier - This component stores the URLs to be downloaded in a FIFO queue.  - HTML Downloader - component downloads HTML pages from URLs in the frontier.  - DNS Resolver - Resolve the IP for a given URL's domain.  - Content Parser - validate that the web page is ok. You don't want to store &amp; process malformed web pages.  - Content Seen? - component eliminates pages which have already been processed. This compares the content, not the URLs. Efficient way to do it is by comparing web page hashes.  - Content Storage - Storage system for HTML documents. Most content is stored on disk \\w most popular ones in memory for fast retrieval.  - URL Extractor - component which extracts links from an HTML document.  - URL Filter - exclude URLs which are invalid, unsupported content types, blacklisted, etc.  - URL Seen? - component which keeps track of visited URLs to avoid traversing them again. Bloom filters are an efficient way to implement this component.  - URL Storage - Store already visited URLs.</p> <p>Those are all the component, but what about the workflow?   1. Add Seed URLs to URL Frontier  2. HTML Downloader fetches a list of URLs from frontier  3. Match URLs to IP Addresses via the DNS resolver  4. Parse HTML pages and discard if malformed  5. Once validated, content is passed to \"Content Seen?\"  6. Check if HTML page is already in storage. If yes - discard. If no - process.  7. Extract links from HTML page  8. Pass extracted links to URL Filter  9. Pass filtered links to \"URL Seen?\" component  10. If URL is in storage - discard. Otherwise - process.  11. If URL is not processed before, it is added to URL Frontier</p>"},{"location":"system-design-interview/chapter10/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's now explore some of the most important mechanisms in the web crawler:  - DFS vs. BFS  - URL frontier  - HTML Downloader  - Robustness  - Extensibility  - Detect and avoid problematic content</p>"},{"location":"system-design-interview/chapter10/#dfs-vs-bfs","title":"DFS vs. BFS","text":"<p>The web is a directed graph, where the links in a web page are the edges to other pages (the nodes).</p> <p>Two common approaches for traversing this data structure are DFS and BFS.  DFS is usually not a good choice as the traversal depth can get very big</p> <p>BFS is typically preferable. It uses a FIFO queue which traverses URLs in order of encountering them.</p> <p>There are two problems with traditional BFS though:  - Most links in a page are backlinks to the same domain, eg wikipedia.com/page -&gt; wikipedia.com.     When a crawler attempts to visit those links, the server is flooded with requests which is \"impolite\".  - Standard BFS doesn't take URL priority into account</p>"},{"location":"system-design-interview/chapter10/#url-frontier","title":"URL Frontier","text":"<p>The URL Frontier helps address these problems. It prioritizes URLs and ensures politeness.</p>"},{"location":"system-design-interview/chapter10/#politeness","title":"Politeness","text":"<p>A web crawler should avoid sending too many requests to the same host in a short time frame as it can cause excessive traffic to traversed website.</p> <p>Politeness is implemented by maintaining a download queue per hostname \\w a delay between element processing:   - The queue router ensures that each queue contains URLs from the same host.  - Mapping table - maps each host to a queue.   - FIFO queues maintain URLs belonging to the same host.  - Queue selector - Each worker thread is mapped to a FIFO queue and only downloads URLs from that queue. Queue selector chooses which worker processes which queue.  - Worker thread 1 to N - Worker threads download web pages one by one from the same host. Delay can be added between two download tasks.</p>"},{"location":"system-design-interview/chapter10/#priority","title":"Priority","text":"<p>We prioritize URLs by usefulness, which can be determined based on PageRank web traffic, update frequency, etc.</p> <p>Prioritizer manages the priority for each URL: </p> <ul> <li>Takes URLs as input and calculates priority</li> <li>Queues have different priorities. URLs are put into respective queue based on its priority.</li> <li>Queue selector - randomly choose queue to select from with bias towards high-priority ones.</li> </ul>"},{"location":"system-design-interview/chapter10/#freshness","title":"Freshness","text":"<p>Web pages are constantly updated. We need to periodically recrawl updated content.</p> <p>We can choose to recrawl based on the web page's update history. We can also prioritize recrawling important pages which are updated first.</p>"},{"location":"system-design-interview/chapter10/#storage-for-url-frontier","title":"Storage for URL Frontier","text":"<p>In the real world, the URLs in the frontier can be millions. Putting everything in-memory is infeasible. But putting it on disk is also slow and can cause a bottleneck for our crawling logic.</p> <p>We've adopted a hybrid approach where most URLs are on disk, but we maintain a buffer in-memory with URLs which are currently processed. We periodically flush that to disk.</p>"},{"location":"system-design-interview/chapter10/#html-downloader","title":"HTML Downloader","text":"<p>This component downloads HTML pages from the web using the HTTP protocol.</p> <p>One protocol we need to also bear in mind is the Robots Exclusion Protocol.</p> <p>It is a <code>robots.txt</code> file, available on websites, which website owners use to communicate with web crawlers. It is used to communicate which web pages are ok to be traversed and which ones should be skipped.</p> <p>It looks like this: <pre><code>User-agent: Googlebot\nDisallow: /creatorhub/\\*\nDisallow: /rss/people/\\*/reviews\nDisallow: /gp/pdp/rss/\\*/reviews\nDisallow: /gp/cdp/member-reviews/\nDisallow: /gp/aw/cr/\n</code></pre></p> <p>We need to respect that file and avoid crawling the pages specified in there. We can cache it to avoid downloading it all the time.</p>"},{"location":"system-design-interview/chapter10/#performance-optimization","title":"Performance optimization","text":"<p>Some performance optimizations we can consider for the HTML downloader.</p> <ul> <li>Distributed crawl - We can parallelize crawl jobs to multiple machines which run multiple threads to crawl more efficiently. </li> <li>Cache DNS Resolver - We can maintain our own DNS cache to avoid making requests to the DNS resolver all the time, which can be costly. It's updated periodically by cron jobs.</li> <li>Locality - We can distribute crawl jobs based on geography. When crawlers are physically closer to website servers, latency is lower.</li> <li>Short timeout - We need to add a timeout in case servers are unresponsive beyond a given threshold. Otherwise, our crawlers can spend a lot of time waiting for pages which will never come.</li> </ul>"},{"location":"system-design-interview/chapter10/#robustness","title":"Robustness","text":"<p>Some approaches to achieve robustness:</p> <ul> <li>Consistent hashing - To enable easy rescaling of our workers/crawlers/etc, we can use consistent hashing when load balancing jobs among them.</li> <li>Save crawl state and data - In the event of server crashes, it would be useful to store intermediary results on disk so that other workers can pick up from where the last one left.</li> <li>Exception handling - We need to handle exceptions gracefully without crashing the servers as these are inevitable in a large enough system.</li> <li>Data validation - important safety measure to prevent system errors.</li> </ul>"},{"location":"system-design-interview/chapter10/#extensibility","title":"Extensibility","text":"<p>We need to ensure the crawler is extendable if we want to support new content types in the future: </p> <p>Example extensions:  - PNG Downloader is added in order to crawl PNG images.  - Web monitor is added to monitor for copyright infringements.</p>"},{"location":"system-design-interview/chapter10/#detect-and-avoid-problematic-content","title":"Detect and avoid problematic content","text":"<p>Some common types of problematic content to be aware of:</p> <ul> <li>Redundant content - ~30% of web pages on the internet are duplicates. We need to avoid processing them more than once using hashes/checksums.</li> <li>Spider traps - A web page which leads to an infinite loop on the crawler, eg an extremely deep directory structure. This can be avoided by specifying a max length for URLs. But there are other sorts of spider traps as well. We can introduce the ability to manually intervene and blacklist spider trap websites.</li> <li>Data noise - Some content has no value for our target use-case. Eg advertisements, code snippets, spam, etc. We need to filter those.</li> </ul>"},{"location":"system-design-interview/chapter10/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Characteristics of a good crawler - scalable, polite, extensible, robust.</p> <p>Other relevant talking points:</p> <ul> <li>Server-side rendering - Numerous sites dynamically generate HTML. If we parse the HTML without generating it first, we'll miss the information on the site. To solve this, we do server-side rendering first before parsing a page.</li> <li>Filter out unwanted pages - Anti-spam component is beneficial for filtering low quality pages.</li> <li>Database replication and sharding - useful techniques to improve the data layer's availability, scalability, reliability.</li> <li>Horizontal scaling - key is to keep servers stateless to enable horizontally scaling every type of server/worker/crawler/etc.</li> <li>Availability, consistency, reliability - concepts at the core of any large system's success.</li> <li>Analytics - We might also have to collect and analyze data in order to fine tune our system further.</li> </ul>"},{"location":"system-design-interview/chapter11/","title":"Design a Notification System","text":"<p>Notification systems are a popular feature in many applications - it alerts a user for important news, product updates, events, etc.</p> <p>There are multiple flavors of a notification:  * Mobile push notification  * SMS  * Email</p>"},{"location":"system-design-interview/chapter11/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What types of notifications does the system support?</li> <li>I: Push notifications, SMS, Email</li> <li>C: Is it a real-time system?</li> <li>I: Soft real-time. We want user to receive notification as soon as possible, but delays are okay if system is under high load.</li> <li>C: What are the supported devices?</li> <li>I: iOS devices, android devices, laptop/desktop.</li> <li>C: What triggers notifications?</li> <li>I: Notifications can be triggered by client applications or on the server-side.</li> <li>C: Will users be able to opt-out?</li> <li>I: Yes</li> <li>C: How many notifications per day?</li> <li>I: 10mil mobile push, 1mil SMS, 5mil email</li> </ul>"},{"location":"system-design-interview/chapter11/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>This section explores the high-level design of the notification system.</p>"},{"location":"system-design-interview/chapter11/#different-types-of-notifications","title":"Different types of notifications","text":"<p>How do the different notification types work at a high level?</p>"},{"location":"system-design-interview/chapter11/#ios-push-notification","title":"iOS push notification","text":"<p>  * Provider - builds and sends notification requests to Apple Push Notification Service (APNS). To do that, it needs some inputs:    * Device token - unique identifier used for sending push notifications     * Payload - JSON payload for the notification, eg: <pre><code>{\n   \"aps\":{\n      \"alert\":{\n         \"title\":\"Game Request\",\n         \"body\":\"Bob wants to play chess\",\n         \"action-loc-key\":\"PLAY\"\n      },\n      \"badge\":5\n   }\n}\n</code></pre>  * APNS - service, provided by Apple for sending mobile push notifications  * iOS Device - end client, which receives the push Notifications</p>"},{"location":"system-design-interview/chapter11/#android-push-notification","title":"Android Push Notification","text":"<p>Android adopts a similar approach. A common alternative to APNS is Firebase Cloud Messaging: </p>"},{"location":"system-design-interview/chapter11/#sms-message","title":"SMS Message","text":"<p>For SMS, third-party providers like Twilio are available: </p>"},{"location":"system-design-interview/chapter11/#email","title":"Email","text":"<p>Although clients can setup their own mail servers, most clients opt-in to use third-party services, like Mailchimp: </p> <p>Here's final design after including all notification providers: </p>"},{"location":"system-design-interview/chapter11/#contact-info-gathering-form","title":"Contact info gathering form","text":"<p>In order to send notifications, we need to gather some inputs from the user first. That is done at user signup: </p> <p>Example database tables for storing contact info: </p>"},{"location":"system-design-interview/chapter11/#notification-sendingreceiving-flow","title":"Notification sending/receiving flow","text":"<p>Here's the high-level design of our notification system:   * Service 1 to N - other services in the system or cron jobs which trigger notification sending events.  * Notification system - accepts notification sending messages and propagates to the correct provider.  * Third-party services - responsible for delivering the messages to the correct users via the appropriate medium. This part should be build \\w extensibility in case we change third-party service providers in the future.  * iOS, Android, SMS, Email - Users receive notifications on their devices.</p> <p>Some problems in this design:  * Single point of failure - only a single notification service  * Hard to scale - since notification system handles everything, it is hard to independently scale eg the cache/database/service layer/etc.  * Performance bottleneck - handling everything in one system can be a bottleneck especially for resource-intensive tasks such as building HTML pages.</p>"},{"location":"system-design-interview/chapter11/#high-level-design-improved","title":"High-level design (improved)","text":"<p>Some changes from the original naive design:  * Move database &amp; cache out of the notification service  * Add more notification servers &amp; setup autoscaling &amp; load balancing  * Introduce message queues to decouple system components</p> <p>  * Service 1 to N - services which send notifications within our system  * Notification servers - provide APIs for sending notifications. Visible to internal services or verified clients. Do basic validation. Fetch notification templates from database. Put notification data in message queues for parallel processing.  * Cache - user info, device info, notification templates  * DB - stores data about users, notifications, settings, etc.  * Message queues - Remove dependencies across components. They serve as buffers for notifications to be sent out. Each notification provider has a different message queue assigned to avoid outages in one third-party provider to affect the rest.  * Workers - pull notification events from message queues and send them to corresponding third-party services.  * Third-party services - already covered in initial design.  * iOS, Android, SMS, Email - already covered in initial design.</p> <p>Example API call to send an email: <pre><code>{\n   \"to\":[\n      {\n         \"user_id\":123456\n      }\n   ],\n   \"from\":{\n      \"email\":\"from_address@example.com\"\n   },\n   \"subject\":\"Hello World!\",\n   \"content\":[\n      {\n         \"type\":\"text/plain\",\n         \"value\":\"Hello, World!\"\n      }\n   ]\n}\n</code></pre></p> <p>Example lifecycle of a notification:  * Service makes a call to make a notification  * Notification service fetch metadata (user info, settings, etc) from database/cache   * Notification event is sent to corresponding queue for processing for each third-party provider.  * Workers pull notifications from the message queues and send them to third-party services.  * Third-party services deliver nofications to end users.</p>"},{"location":"system-design-interview/chapter11/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>In this section, we discuss some additional considerations for our improved design.</p>"},{"location":"system-design-interview/chapter11/#reliability","title":"Reliability","text":"<p>Some questions to consider in terms of making the system reliable:  * What happens in the event of data loss?  * Will recipients receive notifications exactly once?</p> <p>To avoid data loss, we can persist notifications in a notification log database on the workers, which retry them in case a notification doesn't go through: </p> <p>What about duplicate notifications?</p> <p>It will occasionally happen as we can't guarantee exactly-once delivery (unless the third-party API provides idempotency keys). If they don't we can still try to reduce probability of this happening by having a dedup mechanism on our end, which discards an event id if it is already seen.</p>"},{"location":"system-design-interview/chapter11/#additional-components-and-considerations","title":"Additional components and considerations","text":""},{"location":"system-design-interview/chapter11/#notification-templates","title":"Notification templates","text":"<p>To avoid building every notification from scratch on the client side, we'll introduce notification templates as many notifications can reuse them: <pre><code>BODY:\nYou dreamed of it. We dared it. [ITEM NAME] is back \u2014 only until [DATE].\n\nCTA:\nOrder Now. Or, Save My [ITEM NAME]\n</code></pre></p>"},{"location":"system-design-interview/chapter11/#notification-setting","title":"Notification setting","text":"<p>Before sending any notification, we first check if user has opted in for the given communication channel via this database table: <pre><code>user_id bigInt\nchannel varchar # push notification, email or SMS\nopt_in boolean # opt-in to receive notification\n</code></pre></p>"},{"location":"system-design-interview/chapter11/#rate-limiting","title":"Rate limiting","text":"<p>To avoid overwhelming users with too many notifications, we can introduce some client-side rate limiting (on our end) so that they don't opt out of notifications immediately once they get bombarded.</p>"},{"location":"system-design-interview/chapter11/#retry-mechanism","title":"Retry mechanism","text":"<p>If a third-party provider fails to send a notification, it will be put into a retry queue. If problem persists, developers are notified.</p>"},{"location":"system-design-interview/chapter11/#security-in-push-notifications","title":"Security in push notifications","text":"<p>Only verified and authenticated clients are allowed to send push notifications through our APIs. We do this by requiring an appKey and appSecret, inspired by Android/Apple notification servers.</p>"},{"location":"system-design-interview/chapter11/#monitor-queued-notifications","title":"Monitor queued notifications","text":"<p>A critical metric to keep track of is number of queued notifications. If it gets too big, we might have to add more workers: </p>"},{"location":"system-design-interview/chapter11/#events-tracking","title":"Events tracking","text":"<p>We might have to track certain events related to a notification, eg open rate/click rate/etc.</p> <p>Usually, this is done by integrating with an Analytics service, so we'll need to integrate our notification system with one. </p>"},{"location":"system-design-interview/chapter11/#updated-design","title":"Updated design","text":"<p>Putting everything together, here's our final design: </p> <p>Other features we've added:  * Notification servers are equipped with authentication and rate limiting.  * Added a retry mechanism to handle notification failures.  * Notification templates are added to provide a coherent notification experience.  * Monitoring and tracking systems are added to keep track of system health for future improvements.</p>"},{"location":"system-design-interview/chapter11/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We introduced a robust notification system which supports push notifications, sms and email. We introduced message queues to decouple system components.</p> <p>We also dug deeper into some components and optimizations:  * Reliability - added robust retry mechanism in case of failures  * Security - Appkey/appSecret is used to ensure only verified clients can make notifications.  * Tracking and monitoring - implemented to monitor important stats.  * Respect user settings - Users can opt-out of receiving notifications. Service checks the user settings first, before sending notifications.  * Rate limiting - Users would appreciate if we don't bombard them with a dozen of notifications all of a sudden.</p>"},{"location":"system-design-interview/chapter12/","title":"Design a News Feed System","text":"<p>News feed == constantly updating list of stories on your home page.</p> <p>It includes status updates, photos, videos, links, etc.</p> <p>Similar interview questions - design facebook news feed, twitter timeline, instagram feed, etc.</p>"},{"location":"system-design-interview/chapter12/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>First step is to clarify what the interviewer has in mind exactly:  * C: Mobile, web app?  * I: Both  * C: What are the important features?  * I: User can publish posts and see friends' posts on news feed.  * C: Is news feed sorted in reverse chronological order or based on rank, eg best friends' posts first.  * I: To keep it simple, let's assume reverse chrono order  * C: Max number of friends?  * I: 5000  * C: Traffic volume?  * I: 10mil DAU  * C: Can the feed contain media?  * I: It can contain images and video</p>"},{"location":"system-design-interview/chapter12/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>There are two parts to the design:  * Feed publishing - when user publishes a post, corresponding data is written to cache and DB. Post is populated to friends' news feed.  * Newsfeed building - built by aggregating friends' posts in news feed.</p>"},{"location":"system-design-interview/chapter12/#newsfeed-api","title":"Newsfeed API","text":"<p>The Newsfeed API is the primary gateway for users to the news feed services.</p> <p>Here's some of the main endpoints.  * <code>POST /v1/me/feed</code> - publish a post. Payload includes <code>content</code> + <code>auth_token</code>.  * <code>GET /v1/me/feed</code> - retrieve news feed. Payload includes <code>auth_token</code>.</p>"},{"location":"system-design-interview/chapter12/#feed-publishing","title":"Feed publishing","text":"<p>  * User makes a new post via API.  * Load balancer - distributes traffic to web servers.  * Web servers - redirect traffic to internal services.  * Post service - persist post in database and cache.  * Fanout service - push posts to friends' news feeds.  * Notification service - inform new friends that content is available.</p>"},{"location":"system-design-interview/chapter12/#newsfeed-building","title":"Newsfeed building","text":"<p>  * User sends request to retrieve news feed.  * Load balancer redirects traffic to web servers.  * Web servers - route requests to newsfeed service.  * Newsfeed service - fetch news feed from cache.  * Newsfeed cache - store pre-computed news feeds for fast retrieval.</p>"},{"location":"system-design-interview/chapter12/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>Let's discuss the two flows we covered in more depth.</p>"},{"location":"system-design-interview/chapter12/#feed-publishing-deep-dive","title":"Feed publishing deep dive","text":""},{"location":"system-design-interview/chapter12/#web-servers","title":"Web servers","text":"<p>besides a gateway to the internal services, these do authentication and apply rate limits, in order to prevent spam.</p>"},{"location":"system-design-interview/chapter12/#fanout-service","title":"Fanout service","text":"<p>This is the process of delivering posts to friends. There are two types of fanouts - fanout on write (push model) and fanout on read (pull model).</p> <p>Fanout on write (push model) - posts are pre-computed during post publishing.</p> <p>Pros:  * news feed is generated in real-time and can be delivered instantly to friends' news feed.  * fetching the news feed is fast as it's precomputed</p> <p>Cons:  * if a friend has many friends, generating the news feed takes a lot of time, which slows down post publishing speed. This is the hotkey problem.  * for inactive users, pre-computing the news feed is a waste.</p> <p>Fanout on read (pull model) - news feed is generated during read time.</p> <p>Pros:  * Works better for inactive users, as news feeds are not generated for them.  * Data is not pushed to friends, hence, no hotkey problem.</p> <p>Cons:  * Fetching the news feed is slow as it's not pre-computed.</p> <p>We'll adopt a hybrid approach - we'll pre-compute the news feed for people without many friends and use the pull model for celebrities and users with many friends/followers.</p> <p>System diagram of fanout service:   * Fetch friend IDs from graph database. They're suited for managing friend relationships and recommendations.  * Get friends info from user cache. Filtering is applied here for eg muted/blocked friends.  * Send friends list and post ID to the message queue.  * Fanout workers fetch the messages and store the news feed data in a cache. They store a <code>&lt;post_id, user_id&gt;</code> mappings** in it which can later be retrieved.</p> <p>** I think there is some kind of error in this part of the book. It doesn't make sense to store a <code>&lt;post_id, user_id&gt;</code> mapping in the cache. Instead, it should be a <code>&lt;user_id, post_id&gt;</code> mapping as that allows one to quickly fetch all posts for a given user, which are part of their news feed. In addition to that, the example in the book shows that you can store multiple user_ids or post_ids as keys in the cache, which is typically not supported in eg a hashmap, but it is actually supported when you use the <code>Redis Sets</code> feature, but that is not explicitly mentioned in the chapter.</p>"},{"location":"system-design-interview/chapter12/#news-feed-retrieval-deep-dive","title":"News feed retrieval deep dive","text":"<p>  * user sends request to retrieve news feed.  * Load balancer distributes request to a set of web servers.  * Web servers call news feed service.  * News feed service gets a list of <code>post_id</code> from the news feed cache.  * Then, the posts in the news feed are hydrated with usernames, content, media files, etc.  * Fully hydrated news feed is returned as a JSON to the user.  * Media files are also stored in CDN and fetched from there for better user experience.</p>"},{"location":"system-design-interview/chapter12/#cache-architecture","title":"Cache architecture","text":"<p>Cache is very important for a news feed service. We divided it into 5 layers:   * news feed - stores ids of news feeds  * content - stores every post data. Popular content is stored in hot cache.  * social graph - store user relationship data.  * action - store info about whether a user liked, replied or took actions on a post.  * counters - counters for replies, likes, followers, following, etc.</p>"},{"location":"system-design-interview/chapter12/#step-4-wrap-up","title":"Step 4 - wrap up","text":"<p>In this chapter, we designed a news feed system and we covered two main use-cases - feed publishing and feed retrieval.</p> <p>Talking points, related to scalability:  * vertical vs. horizontal database scaling  * SQL vs. NoSQL  * Master-slave replication  * Read replicas  * Consistency models  * Database sharding</p> <p>Other talking points:  * keep web tier stateless  * cache data as much as possible  * multiple data center setup  * Loose coupling components via message queues  * Monitoring key metrics - QPS and latency.</p>"},{"location":"system-design-interview/chapter13/","title":"Design a Chat System","text":"<p>We'll be designing a chat system similar to Messenger, WhatsApp, etc.</p> <p>In this case, it is very important to nail down the exact requirements because chat systems can differ a lot - eg ones focused on group chats vs. one-on-one conversations.</p>"},{"location":"system-design-interview/chapter13/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What kind of chat app should we design? One-on-one convos or group chat?</li> <li>I: It should support both cases.</li> <li>C: Mobile app, web app, both?</li> <li>I: Both</li> <li>C: What's the app scale? Startup or massive application?</li> <li>I: It should support 50mil DAU</li> <li>C: For group chat, what is the member limit?</li> <li>I: 100 people</li> <li>C: What features are important? Eg attachments?</li> <li>I: 1-on-1 and group chats. Online indicator. Text messages only.</li> <li>C: Is there message size limit?</li> <li>I: Text length is less than 100,000 chars long.</li> <li>C: End-to-end encryption required?</li> <li>I: Not required, but will discuss if time permits.</li> <li>C: How long should chat history be stored?</li> <li>I: Forever</li> </ul> <p>Summary of features we'll focus on:  * One-on-one chat with low delivery latency  * Small group chats (100 ppl)  * Online presence  * Same account can be logged in via multiple devices.  * Push notifications  * Scale of 50mil DAU</p>"},{"location":"system-design-interview/chapter13/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>Let's understand how clients and servers communicate first.  * In this system, clients can be mobile devices or web browsers.  * They don't connect to each other directly. They are connected to a server.</p> <p>Main functions the chat service should support:  * Receive messages from clients  * Find the right recipients for a message and relay it  * If recipient is not online, hold messages for them until they get back online. </p> <p>When clients connect to the server, they can do it via one or more network protocols. One option is HTTP. That is okay for the sender-side, but not okay for receiver-side.</p> <p>There are multiple options to handle a server-initiated message for the client - polling, long-polling, web sockets.</p>"},{"location":"system-design-interview/chapter13/#polling","title":"Polling","text":"<p>Polling requires the client to periodically ask the server for status updates: </p> <p>This is easy to implement but it can be costly as there are many requests, which often yield no results</p>"},{"location":"system-design-interview/chapter13/#long-polling","title":"Long polling","text":"<p>With long polling, clients hold the connection open while waiting for an event to occur on the server-side. This still has some wasted requests if users don't chat much, but it is more efficient than polling.</p> <p>Other caveats:  * Server has no good way to determine if client is disconnected.  * senders and receivers might be connected to different servers.</p>"},{"location":"system-design-interview/chapter13/#websocket","title":"WebSocket","text":"<p>Most common approach when bidirectional communication is needed: </p> <p>The connection is initiated by the client and starts as HTTP, but can be upgraded after handshake. In this setup, both clients and servers can initiate messages.</p> <p>One caveat with web sockets is that this is a persistent protocol, making the servers stateful. Efficient connection management is necessary when using it.</p>"},{"location":"system-design-interview/chapter13/#high-level-design","title":"High-level design","text":"<p>Although we mentioned how web sockets can be useful for exchanging messages, most other standard features of our chat can use the normal request/response protocol over HTTP.</p> <p>Given this remark, our service can be broken down into three parts - stateless API, stateful websocket API and third-party integration for notifications: </p>"},{"location":"system-design-interview/chapter13/#stateless-services","title":"Stateless Services","text":"<p>Traditional public-facing request/response services are used to manage login, signup, user profile, etc.</p> <p>These services sit behind a load-balancer, which distributes requests across a set of service replicas.</p> <p>The service discovery service, in particular, is interesting and will be discussed more in-depth in the deep dive.</p>"},{"location":"system-design-interview/chapter13/#stateful-service","title":"Stateful Service","text":"<p>The only stateful service is our chat service. It is stateful as it maintain a persistent connection with clients which connect to it.</p> <p>In this case, a client doesn't switch to other chat services as long as the existing one stays alive.</p> <p>Service discovery coordinates closely with the chat services to avoid overload.</p>"},{"location":"system-design-interview/chapter13/#third-party-integration","title":"Third-party Integration","text":"<p>It is important for a chat application to support push notifications in order to get notified when someone sends you a message.</p> <p>This component won't be discussed extensively as it's already covered in the Design a notification system chapter.</p>"},{"location":"system-design-interview/chapter13/#scalability","title":"Scalability","text":"<p>On a small scale, we can fit everything in a single server.</p> <p>With 1mil concurrent users, assuming each connection takes up 10k memory, a single server will need to use 10GB of memory to service them all.</p> <p>Despite this, we shouldn't propose a single-server setup as it raises a red flag in the interviewer. One big drawback of a single server design is the single point of failure.</p> <p>It is fine, however, to start from a single-server design and extend it later as long as you explicitly state that during the interview.</p> <p>Here's our refined high-level design:   * clients maintain a persistent web socket connection with a chat server for real-time messaging  * The chat servers facilitate message sending/receiving  * Presense servers manage online/offline status  * API servers handle traditional request/response-based responsibilities - login, sign up, change profile, etc.  * Notification servers manage push notifications  * Key-value store is used for storing chat history. When offline user goes online, they will see their chat history and missed messages.</p>"},{"location":"system-design-interview/chapter13/#storage","title":"Storage","text":"<p>One important decision for the storage/data layer is whether we should go with a SQL or NoSQL database.</p> <p>To make the decision, we need to examine the read/write access patterns.</p> <p>Traditional data such as user profile, settings, user friends list can be stored in a traditional relational database. Replication and sharding are common techniques to meet scalability needs for relational databases.</p> <p>Chat history data, on the other hand, is very specific kind of data of chat systems due to its read/write pattern:  * Amount of data is enormous, a study revealed that Facebook and WhatsApp process 60bil messages per day.  * Only recent chats are accessed frequently. Users typically don't go too far back in chat history.  * Although chat history is accessed infrequently, we should still be able to search within it as users can use a search bar for random access.  * Read to write ratio is 1:1 on chat apps.</p> <p>Selecting the correct storage system for this kind of data is crucial. Author recommends using a key-value store:  * they allow easy horizontal scaling  * they provide low latency access to data  * Relational databases don't handle long-tail (less-frequently accessed but large part of a distribution) of data well. When indexes grow large, random access is expensive.  * Key-value stores are widely adopted for chat systems. Facebook and Discord both use key-value stores. Facebook uses HBase, Discord uses Cassandra.</p>"},{"location":"system-design-interview/chapter13/#data-models","title":"Data models","text":"<p>Let's take a look at the data model for our messages.</p> <p>Message table for one-on-one chat: </p> <p>One caveat is that we'll use the primary key (message_id) instead of created_at to determine message sequence as messages can be sent at the same time.</p> <p>Message table for a group chat: </p> <p>In the above table, <code>(channel_id, message_id)</code> is the primary key, while <code>channel_id</code> is also the sharding key.</p> <p>One interesting discussion is how should the <code>message_id</code> be generated, as it is used for message ordering. It should have two important attributes:  * IDs must be unique  * IDs must be sortable by time</p> <p>One option is to use the <code>auto_increment</code> feature of relational databases. But that's not supported in key-value stores. An alternative is to use Snowflake - Twitter's algorithm for generating 64-byte IDs which are globally unique and sortable by time.</p> <p>Finally, we could also use a local sequence number generator, which is unique only within a group.  We can afford this because we only need to guarantee message sequence within a chat, but not between different chats.</p>"},{"location":"system-design-interview/chapter13/#step-3-design-deep-dive","title":"Step 3 - Design deep-dive","text":"<p>In a system design interview, typically you are asked to go deeper into some of the components.</p> <p>In this case, we'll go deeper into the service discovery component, messaging flows and online/offline indicator.</p>"},{"location":"system-design-interview/chapter13/#service-discovery","title":"Service discovery","text":"<p>The primary goal of service discovery is to choose the best server based on some criteria - eg geographic location, server capacity, etc.</p> <p>Apache Zookeeper is a popular open-source solution for service discovery. It registers all available chat servers and picks the best one based on a predefined criteria.   * User A tries to login to the app  * Load balancer sends request to API servers.  * After authentication, service discovery chooses the best chat server for user A. In this case, chat server 2 is chosen.  * User A connects to chat server 2 via web sockets protocol.</p>"},{"location":"system-design-interview/chapter13/#message-flows","title":"Message flows","text":"<p>The message flows are an interesting topic to deep dive into. We'll explore one on one chats, message synchronization and group chat.</p>"},{"location":"system-design-interview/chapter13/#1-on-1-chat-flow","title":"1 on 1 chat flow","text":"<p>  * User A sends a message to chat server 1  * Chat server 1 obtains a message_id from Id generator  * Chat server 1 sends the message to the \"message sync\" queue.  * Message is stored in a key-value store.  * If User B is online, message is forwarded to chat server 2, where User B is connected.  * If offline, push notification is sent via the push notification servers.  * Chat server 2 forwards the message to user B.</p>"},{"location":"system-design-interview/chapter13/#message-synchronization-across-devices","title":"Message synchronization across devices","text":"<p>  * When user A logs in via phone, a web socket is established for that device with chat server 1.  * Each device maintains a variable called <code>cur_max_message_id</code>, keeping track of latest message received on given device.  * Messages whose recipient ID is currently logged in (via any device) and whose message_id is greater than <code>cur_max_message_id</code> are considered new</p>"},{"location":"system-design-interview/chapter13/#small-group-chat-flow","title":"Small group chat flow","text":"<p>Group chats are a bit more complicated: </p> <p>Whenever User A sends a message, the message is copied across each message queue of participants in the group (User B and C).</p> <p>Using one inbox per user is a good choice for small group chats as:  * it simplifies message sync since each user only need to consult their own queue.  * storing a message copy in each participant's inbox is feasible for small group chats.</p> <p>This is not acceptable though, for larger group chats.</p> <p>As for the recipient, in their queue, they can receive messages from different group chats: </p>"},{"location":"system-design-interview/chapter13/#online-presence","title":"Online presence","text":"<p>Presence servers manage the online/offline indication in chat applications.</p> <p>Whenever the user logs in, their status is changed to \"online\": </p> <p>Once the user send a logout message to the presence servers (and subsequently disconnects), their status is changed to \"offline\": </p> <p>One caveat is handling user disconnection. A naive approach to handle that is to mark a user as \"offline\" when they disconnect from the presence server. This makes for a poor user experience as a user could frequently disconnect and reconnect to presence servers due to poor internet.</p> <p>To mitigate this, we'll introduce a heartbeat mechanism - clients periodically send a heartbeat to the presence servers to indicate online status. If a heartbeat is not received within a given time frame, user is marked offline: </p> <p>How does a user's friend find out about a user's presence status though?</p> <p>We'll use a fanout mechanism, where each friend pair have a queue assigned and status changes are sent to the respective queues: </p> <p>This is effective for small group chats. WeChat uses a similar approach and its user group is capped to 500 users.</p> <p>If we need to support larger groups, a possible mitigation is to fetch presence status only when a user enters a group or refreshes the members list.</p>"},{"location":"system-design-interview/chapter13/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We managed to build a chat system which supports both one-on-one and group chats.  * We used web sockets for real-time communication between clients and servers.</p> <p>System components:   * chat servers (real-time messages)  * presence servers (online/offline status)  * push notification servers  * key-value stores for chat history  * API servers for everything else</p> <p>Additional talking points:  * Extend chat app to support media - video, images, voice. Compression, cloud storage and thumbnails can be discussed.  * End-to-end encryption - only sender and receiver can read messages.  * Caching messages on client-side is effective to reduce server-client data transfer.  * Improve load time - Slack built a geographically distributed network to cache user data, channels, etc for better load time.  * Error handling  * Chat server error - what happens if a chat server goes down. Zookeeper can facilitate a hand off to another chat server.  * Message resend mechanism - retrying and queueing are common approaches for re-sending messages.</p>"},{"location":"system-design-interview/chapter14/","title":"Design A Search Autocomplete System","text":"<p>Search autocomplete is the feature provided by many platforms such as Amazon, Google and others when you put your cursor in your search bar and start typing something you're looking for: </p>"},{"location":"system-design-interview/chapter14/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: Is the matching only supported at the beginning of a search term or eg at the middle?</li> <li>I: Only at the beginning</li> <li>C: How many autocompletion suggestions should the system return?</li> <li>I: 5</li> <li>C: Which suggestions should the system choose?</li> <li>I: Determined by popularity based on historical query frequency</li> <li>C: Does system support spell check?</li> <li>I: Spell check or auto-correct is not supported.</li> <li>C: Are search queries in English?</li> <li>I: Yes, if time allows, we can discuss multi-language support</li> <li>C: Is capitalization and special characters supported?</li> <li>I: We assume all queries use lowercase characters</li> <li>C: How many users use the product?</li> <li>I: 10mil DAU</li> </ul> <p>Summary:  * Fast response time. An article about facebook autocomplete reviews that suggestions should be returned with 100ms delay at most to avoid stuttering  * Relevant - autocomplete suggestions should be relevant to search term  * Sorted - suggestions should be sorted by popularity  * Scalable - system can handle high traffic volumes  * Highly available - system should be up even if parts of the system are unresponsive</p>"},{"location":"system-design-interview/chapter14/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<ul> <li>Assume we have 10mil DAU</li> <li>On average, person performs 10 searches per day</li> <li>10mil * 10 = 100mil searches per day = 100 000 000 / 86400 = 1200 searches.</li> <li>given 4 works of 5 chars search on average -&gt; 1200 * 20 = 24000 QPS. Peak QPS = 48000 QPS.</li> <li>20% of daily queries are new -&gt; 100mil * 0.2 = 20mil new searches * 20 bytes = 400mb new data per day.</li> </ul>"},{"location":"system-design-interview/chapter14/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>At a high-level, the system has two components:  * Data gathering service - gathers user input queries and aggregates them in real-time.  * Query service - given search query, return topmost 5 suggestions.</p>"},{"location":"system-design-interview/chapter14/#data-gathering-service","title":"Data gathering service","text":"<p>This service is responsible for maintaining a frequency table: </p>"},{"location":"system-design-interview/chapter14/#query-service","title":"Query service","text":"<p>Given a frequency table like the one above, this service is responsible for returning the top 5 suggestions based on the frequency column: </p> <p>Querying the data set is a matter of running the following SQL query: </p> <p>This is acceptable for small data sets but becomes impractical for large ones.</p>"},{"location":"system-design-interview/chapter14/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>In this section, we'll deep dive into several components which will improve the initial high-level design.</p>"},{"location":"system-design-interview/chapter14/#trie-data-structure","title":"Trie data structure","text":"<p>We use relational databases in the high-level design, but to achieve a more optimal solution, we'll need to leverage a suitable data structure.</p> <p>We can use tries for fast string prefix retrieval.  * It is a tree-like data structure  * The root represents the empty string  * Each node has 26 children, representing each of the next possible characters. To save space, we don't store empty links.  * Each node represents a single word or prefix  * For this problem, apart from storing the strings, we'll need to store the frequency against each leaf </p> <p>To implement the algorithm, we need to:  * first find the node representing the prefix (time complexity O(p), where p = length of prefix)  * traverse subtree to find all leafs (time complexity O(c), where c = total children)  * sort retrieved children by their frequencies (time complexity O(clogc), where c = total children) </p> <p>This algorithm works, but there are ways to optimize it as we'll have to traverse the entire trie in the worst-case scenario.</p>"},{"location":"system-design-interview/chapter14/#limit-the-max-length-of-prefix","title":"Limit the max length of prefix","text":"<p>We can leverage the fact that users rarely use a very long search term to limit max prefix to 50 chars.</p> <p>This reduces the time complexity from <code>O(p) + O(c) + O(clogc)</code> -&gt; <code>O(1) + O(c) + O(clogc)</code>.</p>"},{"location":"system-design-interview/chapter14/#cache-top-search-queries-at-each-node","title":"Cache top search queries at each node","text":"<p>To avoid traversing the whole trie, we can cache the top k most frequently accessed works in each node: </p> <p>This reduces the time complexity to <code>O(1)</code> as top K search terms are already cached. The trade-off is that it takes much more space than a traditional trie.</p>"},{"location":"system-design-interview/chapter14/#data-gathering-service_1","title":"Data gathering service","text":"<p>In previous design, when user types in search term, data is updated in real-time. This is not practical on a bigger scale due to:  * billions of queries per day  * Top suggestions may not change much once trie is built</p> <p>Hence, we'll instead update the trie asynchronously based on analytics data: </p> <p>The analytics logs contain raw rows of data related to search terms \\w timestamps: </p> <p>The aggregators' responsibility is to map the analytics data into a suitable format and also aggregate it to lesser records.</p> <p>The cadence at which we aggregate depends on the use-case for our auto-complete functionality.  If we need the data to be relatively fresh &amp; updated real-time (eg twitter search), we can aggregate once every eg 30m. If, on the other hand, we don't need the data to be updated real-time (eg google search), we can aggregate once per week.</p> <p>Example weekly aggregated data: </p> <p>The workers are responsible for building the trie data structure, based on aggregated data, and storing it in DB.</p> <p>The trie cache keeps the trie loaded in-memory for fast read. It takes a weekly snapshot of the DB.</p> <p>The trie DB is the persistent storage. There are two options for this problem:  * Document store (eg MongoDB) - we can periodically build the trie, serialize it and store it in the DB.  * Key-value store (eg DynamoDB) - we can also store the trie in hashmap format. </p>"},{"location":"system-design-interview/chapter14/#query-service_1","title":"Query service","text":"<p>The query service fetches top suggestions from Trie Cache or fallbacks to Trie DB on cache miss: </p> <p>Some additional optimizations for the Query service:  * Using AJAX requests on client-side - these prevent the browser from refreshing the page.  * Data sampling - instead of logging all requests, we can log a sample of them to avoid too many logs.  * Browser caching - since auto-complete suggestions don't change often, we can leverage the browser cache to avoid extra calls to backend.</p> <p>Example with Google search caching search results on the browser for 1h: </p>"},{"location":"system-design-interview/chapter14/#trie-operations","title":"Trie operations","text":"<p>Let's briefly describe common trie operations.</p>"},{"location":"system-design-interview/chapter14/#create","title":"Create","text":"<p>The trie is created by workers using aggregated data, collected via analytics logs.</p>"},{"location":"system-design-interview/chapter14/#update","title":"Update","text":"<p>There are two options to handling updates:  * Not updating the trie, but reconstructing it instead. This is acceptable if we don't need real-time suggestions.  * Updating individual nodes directly - we prefer to avoid it as it's slow. Updating a single node required updating all parent nodes as well due to the cached suggestions: </p>"},{"location":"system-design-interview/chapter14/#delete","title":"Delete","text":"<p>To avoid showing suggestions including hateful content or any other content we don't want to show, we can add a filter between the trie cache and the API servers: </p> <p>The database is asynchronously updated to remove hateful content.</p>"},{"location":"system-design-interview/chapter14/#scale-the-storage","title":"Scale the storage","text":"<p>At some point, our trie won't be able to fit on a single server. We need to devise a sharding mechanism.</p> <p>One option to achieve this is to shard based on the letters of the alphabet - eg <code>a-m</code> goes on one shard, <code>n-z</code> on the other.</p> <p>This doesn't work well as data is unevenly distributed due to eg the letter <code>a</code> being much more frequent than <code>x</code>.</p> <p>To mitigate this, we can have a dedicated shard mapper, which is responsible for devising a smart sharding algorithm, which factors in the uneven distribution of search terms: </p>"},{"location":"system-design-interview/chapter14/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Other talking points:  * How to support multi-language - we store unicode characters in trie nodes, instead of ASCII.  * What if top search queries differ across countries - we can build different tries per country and leverage CDNs to improve response time.  * How can we support trending (real-time) search queries? - current design doesn't support this and improving it to support it is beyond the scope of the book. Some options:     * Reduce working data set via sharding    * Change ranking model to assign more weight to recent search queries    * Data may come as streams which you filter upon and use map-reduce technologies to process it - Hadoop, Apache Spark, Apache Storm, Apache Kafka, etc.</p>"},{"location":"system-design-interview/chapter15/","title":"Design YouTube","text":"<p>This chapter is about designing a video sharing platform such as youtube. Its solution can be applied to also eg designing Netflix, Hulu.</p>"},{"location":"system-design-interview/chapter15/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What features are important?</li> <li>I: Upload video + watch video</li> <li>C: What clients do we need to support?</li> <li>I: Mobile apps, web apps, smart TV</li> <li>C: How many DAUs do we have?</li> <li>I: 5mil</li> <li>C: Average time per day spend on YouTube?</li> <li>I: 30m</li> <li>C: Do we need to support international users?</li> <li>I: Yes</li> <li>C: What video resolutions do we need to support?</li> <li>I: Most of them</li> <li>C: Is encryption required?</li> <li>I: Yes</li> <li>C: File size requirement for videos?</li> <li>I: Max file size is 1GB</li> <li>C: Can we leverage existing cloud infra from Google, Amazon, Microsoft?</li> <li>I: Yes, building everything from scratch is not a good idea.</li> </ul> <p>Features, we'll focus on:  * Upload videos fast  * Smooth video streaming  * Ability to change video quality  * Low infrastructure cost  * High availability, scalability, reliability  * Supported clients - web, mobile, smart TV</p>"},{"location":"system-design-interview/chapter15/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<ul> <li>Assume product has 5mil DAU</li> <li>Users watch 5 videos per day</li> <li>10% of users upload 1 video per day</li> <li>Average video size is 300mb</li> <li>Daily storage cost needed - 5mil * 10% * 300mb = 150TB</li> <li>CDN Cost, assuming 0.02$ per GB - 5mil * 5 videos * 0.3GB * 0.02$ = USD 150k per day</li> </ul>"},{"location":"system-design-interview/chapter15/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>As previously discussed, we won't be building everything from scratch.</p> <p>Why?  * In a system design interview, choosing the right technology is more important than explaining how the technology works.  * Building scalable blob storage over CDN is complex and costly. Even big tech don't build everything from scratch. Netflix uses AWS and Facebook uses Akamai's CDN.</p> <p>Here's our system design at a high-level:   * Client - you can watch youtube on web, mobile and TV.  * CDN - videos are stored in CDN.  * API Servers - Everything else, except video streaming goes through the API servers. Feed recommendation, generating video URL, updating metadata db and cache, user signup.</p> <p>Let's explore high-level design of video streaming and uploading.</p>"},{"location":"system-design-interview/chapter15/#video-uploading-flow","title":"Video uploading flow","text":"<p>  * Users watch videos on a supported client  * Load balancer evenly distributes requests across API servers  * All user requests go through API servers, except video streaming  * Metadata DB - sharded and replicated to meet performance and availability requirements  * Metadata cache - for better performance, video metadata and user objects are cached  * A blob storage system is used to store the actual videos  * Transcoding/encoding servers - transform videos to various formats (eg MPEG, HLS, etc) which are suitable for different devices and bandwidth  * Transcoded storage stores result files from transcoding  * Videos are cached in CDN - clicking play streams the video from CDN  * Completion queue - stores events about video transcoding results  * Completion handler - a set of workers which pull event data from completion queue and update metadata cache and database</p> <p>Let's now explore the flow of uploading videos and video metadata. Metadata includes info about video URL, size, resolution, format, etc.</p> <p>Here's how the video uploading flow works:   * Videos are uploaded to original storage  * Transcoding servers fetch videos from storage and start transcoding  * Once transcoding is complete, two steps are executed in parallel:    * Transcoded videos are sent to transcoded storage and distributed to CDN    * Transcoding completion events are queued in completion queue, workers pick up the events and update metadata database &amp; cache  * API servers inform user that uploading is complete</p> <p>Here's how the metadata update flow works:   * While file is being uploaded, user sends a request to update the video metadata - file name, size, format, etc.  * API servers update metadata database &amp; cache</p>"},{"location":"system-design-interview/chapter15/#video-streaming-flow","title":"Video streaming flow","text":"<p>Whenever users watch videos on YouTube, they don't download the whole video at once. Instead, they download a little and start watching it while downloading the rest. This is referred to as streaming. Stream is served from closest CDN server for lowest latency.</p> <p>Some popular streaming protocols:  * MPEG-DASH - \"Moving Picture Experts Group\"-\"Dynamic Adaptive Streaming over HTTP\"  * Apple HLS - \"HTTP Live Streaming\"  * Microsoft Smooth Streaming  * Adobe HTTP Dynamic Streaming (HDS)</p> <p>You don't need to understand those protocols in detail. It is important to understand, though, that different streaming protocols support different video encodings and playback players.</p> <p>We need to choose the right streaming protocol to support our use-case.</p>"},{"location":"system-design-interview/chapter15/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>In this part, we'll deep dive into the video uploading and video streaming flows.</p>"},{"location":"system-design-interview/chapter15/#video-transcoding","title":"Video transcoding","text":"<p>Video transcoding is important for a few reasons:  * Raw video consumes a lot of storage space.  * Many browsers have constraints on the type of videos they can support. It is important to encode a video for compatibility reasons.  * To ensure good UX, you ought to serve HD videos to users with good network connection and lower-quality formats for the ones with slower connection.  * Network conditions can change, especially on mobile. It is important to be able to automatically switch video formats at runtime for smooth UX.</p> <p>Most transcoding formats consist of two parts:  * Container - the basket which contains the video file. Recognized by the file extension, eg .avi, .mov, .mp4  * Codecs - Compression and decompression algorithms, which reduce video size while preserving quality. Most popular ones - H.264, VP9, HEVC.</p>"},{"location":"system-design-interview/chapter15/#directed-acyclic-graph-dag-model","title":"Directed Acyclic Graph (DAG) model","text":"<p>Transcoding video is computationally expensive and time-consuming.  In addition to that, different creators have different inputs - some provide thumbnails, others do not, some upload HD, others don't.</p> <p>In order to support video processing pipelines, dev customisations, high parallelism, we adopt a DAG model: </p> <p>Some of the tasks applied on a video file:  * Ensure video has good quality and is not malformed  * Video is encoded to support different resolutions, codecs, bitrates, etc.  * Thumbnail is automatically added if a user doesn't specify it.  * Watermark - image overlay on video if specified by creator </p>"},{"location":"system-design-interview/chapter15/#video-transcoding-architecture","title":"Video transcoding architecture","text":""},{"location":"system-design-interview/chapter15/#preprocessor","title":"Preprocessor","text":"<p>The preprocessor's responsibilities:  * Video splitting - video is split in group of pictures (GOP) alignment, ie arranged groups of chunks which can be played independently  * Cache - intermediary steps are stored in persistent storage in order to retry on failure.  * DAG generation - DAG is generated based on config files specified by programmers.</p> <p>Example DAG configuration with two steps: </p>"},{"location":"system-design-interview/chapter15/#dag-scheduler","title":"DAG Scheduler","text":"<p>DAG scheduler splits a DAG into stages of tasks and puts them in a task queue, managed by a resource manager: </p> <p>In this example, a video is split into video, audio and metadata stages which are processed in parallel.</p>"},{"location":"system-design-interview/chapter15/#resource-manager","title":"Resource manager","text":"<p>Resource manager is responsible for optimizing resource allocation.   * Task queue is a priority queue of tasks to be executed  * Worker queue is a queue of available workers and worker utilization info  * Running queue contains info about currently running tasks and which workers they're assigned to</p> <p>How it works:  * task scheduler gets highest-priority task from queue  * task scheduler gets optimal task worker to run the task  * task scheduler instructs worker to start working on the task  * task scheduler binds worker to task &amp; puts task/worker info in running queue  * task scheduler removes the job from the running queue once the job is done</p>"},{"location":"system-design-interview/chapter15/#task-workers","title":"Task workers","text":"<p>The workers execute the tasks in the DAG. Different workers are responsible for different tasks and can be scaled independently. </p>"},{"location":"system-design-interview/chapter15/#temporary-storage","title":"Temporary storage","text":"<p>Multiple storage systems are used for different types of data. Eg temporary images/video/audio is put in blob storage. Metadata is put in an in-memory cache as data size is small.</p> <p>Data is freed up once processing is complete.</p>"},{"location":"system-design-interview/chapter15/#encoded-video","title":"Encoded video","text":"<p>Final output of the DAG. Example output - <code>funny_720p.mp4</code>.</p>"},{"location":"system-design-interview/chapter15/#system-optimizations","title":"System Optimizations","text":"<p>Now it's time to introduce some optimizations for speed, safety, cost-saving.</p>"},{"location":"system-design-interview/chapter15/#speed-optimization-parallelize-video-uploading","title":"Speed optimization - parallelize video uploading","text":"<p>We can split video uploading into separate units via GOP alignment: </p> <p>This enables fast resumable uploads if something goes wrong. Splitting the video file is done by the client.</p>"},{"location":"system-design-interview/chapter15/#speed-optimization-place-upload-centers-close-to-users","title":"Speed optimization - place upload centers close to users","text":"<p>This can be achieved by leveraging CDNs.</p>"},{"location":"system-design-interview/chapter15/#speed-optimization-parallelism-everywhere","title":"Speed optimization - parallelism everywhere","text":"<p>We can build a loosely coupled system and enable high parallelism.</p> <p>Currently, components rely on inputs from previous components in order to produce outputs: </p> <p>We can introduce message queues so that components can start doing their task independently of previous one once events are available: </p>"},{"location":"system-design-interview/chapter15/#safety-optimization-pre-signed-upload-url","title":"Safety optimization - pre-signed upload URL","text":"<p>To avoid unauthorized users from uploading videos, we introduce pre-signed upload URLs: </p> <p>How it works:  * client makes request to API server to fetch upload URL  * API servers generate the URL and return it to the client  * Client uploads the video using the URL</p>"},{"location":"system-design-interview/chapter15/#safety-optimization-protect-your-videos","title":"Safety optimization - protect your videos","text":"<p>To protect creators from having their original content stolen, we can introduce some safety options:  * Digital right management (DRM) systems - Apple FairPlay, Google Widevine, Microsoft PlayReady  * AES encryption - you can encrypt a video and configure an authorization policy. It is decrypted on playback.  * Visual watermarking - image overlay on top of video which contains your identifying information, eg company name.</p>"},{"location":"system-design-interview/chapter15/#cost-saving-optimization","title":"Cost-saving optimization","text":"<p>CDN is expensive, as we've seen in our back of the envelope estimation.</p> <p>We can piggyback on the fact that video streams follow a long-tail distribution - ie a few popular videos are accessed frequently, but everything else is not.</p> <p>Hence, we can store popular videos in CDN and serve everything else from high capacity storage servers: </p> <p>Other cost-saving optimizations:  * We might not need to store many encoded versions for less popular videos. Short videos can be encoded on-demand.  * Some videos are only popular in certain regions. We can avoid distributing them in all regions.  * Build your own CDN. Can make sense for large streaming companies like Netflix.</p>"},{"location":"system-design-interview/chapter15/#error-handling","title":"Error Handling","text":"<p>For a large-scale system, errors are unavoidable. To make a fault-tolerant system, we need to handle errors gracefully and recover from them.</p> <p>There are two types of errors:  * Recoverable error - can be mitigated by retrying a few times. If retrying fails, a proper error code is returned to the client.  * Non-recoverable error - system stops running related tasks and returns proper error code to the client.</p> <p>Other typical errors and their resolution:  * Upload error - retry a few times  * Split video error - entire video is passed to server if older clients don't support GOP alignment.  * Transcoding error - retry  * Preprocessor error - regenerate DAG  * DAG scheduler error - retry scheduling  * Resource manager queue down - use a replica  * Task worker down - retry task on different worker  * API server down - they're stateless so requests can be redirected to other servers  * Metadata db/cache server down - replicate data across multiple nodes  * Master is down - Promote one of the slaves to become master  * Slave is down - If slave goes down, you can use another slave for reads and bring up another slave instance</p>"},{"location":"system-design-interview/chapter15/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Additional talking points:  * Scaling the API layer - easy to scale horizontally as API layer is stateless  * Scale the database - replication and sharding  * Live streaming - our system is not designed for live streams, but it shares some similarities, eg uploading, encoding, streaming. Notable differences:    * Live streaming has higher latency requirements so it might demand a different streaming protocol    * Lower requirement for parallelism as small chunks of data are already processed in real time    * different error handling, as there is a timeout after which we need to stop retrying    * Video takedowns - videos that violate copyrights, pornography, any other illegal acts need to be removed either during upload flow or based on user flagging.</p>"},{"location":"system-design-interview/chapter16/","title":"Design Google Drive","text":"<p>Google Drive is a cloud file storage product, which helps you store documents, videos, etc from the cloud.</p> <p>You can access them from any device and share them with friends and family.</p>"},{"location":"system-design-interview/chapter16/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What are most important features?</li> <li>I: Upload/download files, file sync and notifications</li> <li>C: Mobile or web?</li> <li>I: Both</li> <li>C: What are the supported file formats?</li> <li>I: Any file type</li> <li>C: Do files need to be encrypted?</li> <li>I: Yes, files in storage need to be encrypted</li> <li>C: Is the a file size limit?</li> <li>I: Yes, files need to be 10 gb or smaller</li> <li>C: How many users does the app have?</li> <li>I: 10mil DAU</li> </ul> <p>Features we'll focus on:  * Adding files  * Downloading files  * Sync files across devices  * See file revisions  * Share file with friends  * Send a notification when file is edited/deleted/shared</p> <p>Features not discussed:  * Collaborative editing</p> <p>Non-functional requirements:  * Reliability - data loss is unacceptable  * Fast sync speed  * Bandwidth usage - users will get unhappy if app consumes too much network traffic or battery  * Scalability - we need to handle a lot of traffic  * High availability - users should be able to use the system even when some services are down</p>"},{"location":"system-design-interview/chapter16/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<ul> <li>Assume 50mil sign ups and 10mil DAU</li> <li>Users get 10 gb free space</li> <li>Users upload 2 files per day, average size is 500kb</li> <li>1:1 read-write ratio</li> <li>Total space allocated - 50mil * 10gb = 500pb</li> <li>QPS for upload API - 10mil * 2 uploads / 24h / 3600s = ~240</li> <li>Peak QPS = 480</li> </ul>"},{"location":"system-design-interview/chapter16/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - propose high-level design and get buy-in","text":"<p>In this chapter, we'll use a different approach than other ones - we'll start building the design from a single server and scale out from there.</p> <p>We'll start from:  * A web server to upload and download files  * A database to keep track of metadata - user data, login info, files info, etc  * Storage system to store the files</p> <p>Example storage we could use: </p>"},{"location":"system-design-interview/chapter16/#apis","title":"APIs","text":"<p>Upload file: <pre><code>https://api.example.com/files/upload?uploadType=resumable\n</code></pre></p> <p>This endpoint is used for uploading files with support for simple upload and resumable upload, which is used for large files. Resumable upload is achieved by retrieving an upload URL and uploading the file while monitoring upload state. If disturbed, resume the upload.</p> <p>Download file: <pre><code>https://api.example.com/files/download\n</code></pre></p> <p>The payload specifies which file to download: <pre><code>{\n    \"path\": \"/recipes/soup/best_soup.txt\"\n}\n</code></pre></p> <p>Get file revisions: <pre><code>https://api.example.com/files/list_revisions\n</code></pre></p> <p>params:  * path to file for which revision history is retrieved  * maximum number of revisions to return</p> <p>All the APIs require authentication and use HTTPS.</p>"},{"location":"system-design-interview/chapter16/#move-away-from-single-server","title":"Move away from single server","text":"<p>As more files are uploaded, at some point, you reach your storage's capacity.</p> <p>One option to scale your storage server is by implementing sharing - each user's data is stored on separate servers: </p> <p>This solves your issue but you're still worried about potential data loss.</p> <p>A good option to address that is to use an off-the-shelf solution like Amazon S3 which offers replication (same-region/cross-region) out of the box: </p> <p>Other areas you could improve:  * Load balancing - this ensures evenly distributed network traffic to your web server replicas.  * More web servers - with the advent of a load balancer, you can easily scale your web server layer by adding more servers.  * Metadata database - move the database away from the server to avoid single points of failure. You can also setup replication and sharding to meet scalability requirements.  * File storage - Amazon S3 for storage. To ensure availability and durability, files are replicated in two separate geographical regions.</p> <p>Here's the updated design: </p>"},{"location":"system-design-interview/chapter16/#sync-conflicts","title":"Sync conflicts","text":"<p>Once the user base grows sufficiently, sync conflicts are unavoidable.</p> <p>To address this, we can apply a strategy where the first who manages to modify a file first wins: </p> <p>What happens once you get a conflict? We generate a second version of the file which represents the alternative file version and it's up to the user to merge it: </p>"},{"location":"system-design-interview/chapter16/#high-level-design","title":"High-level design","text":"<p>  * User uses the application through a browser or a mobile app  * Block servers upload files to cloud storage. Block storage is a technology which allows you to split a big file in blocks and store the blocks in a backing storage. Dropbox, for example, stores blocks of size 4mb.  * Cloud storage - a file split into multiple blocks is stored in cloud storage  * Cold storage - used for storing inactive files, infrequently accessed.  * Load balancer - evenly distributes requests among API servers.  * API servers - responsible for anything other than uploading files. Authentication, user profile management, updating file metadata, etc.  * Metadata database - stores metadata about files uploaded to cloud storage.  * Metadata cache - some of the metadata is cached for fast retrieval.  * Notification service - Publisher/subscriber system which notifies users when a file is updated/edited/removed so that they can pull the latest changes.  * Offline backup queue - used to queue file changes for users who are offline so that they can pull them once they come back online.</p>"},{"location":"system-design-interview/chapter16/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>Let's explore:  * block servers  * metadata database  * upload/download flow  * notification service  * saving storage space  * failure handling</p>"},{"location":"system-design-interview/chapter16/#block-servers","title":"Block servers","text":"<p>For large files, it's infeasible to send the whole file on each update as it consumes a lot of bandwidth.</p> <p>Two optimizations we're going to explore:  * Delta sync - once a file is modified, only modified blocks are sent to the block servers instead of the whole file.  * Compression - applying compression on blocks can significantly reduce data size. Different algorithms are suitable for different file types, eg for text files, we'll use gzip/bzip2.</p> <p>Apart from splitting files in blocks, the block servers also apply encryption prior to storing files in file storage: </p> <p>Example delta sync: </p>"},{"location":"system-design-interview/chapter16/#high-consistency-requirement","title":"High consistency requirement","text":"<p>Our system requires strong consistency as it's unacceptable to show different versions of a file to different people.</p> <p>This is mainly problematic when we use caches, in particular the metadata cache in our example.  To sustain strong consistency, we need to:  * keep cache master and replicas consistent  * invalidate caches on database write</p> <p>For the database, strong consistency is guaranteed as long as we use a relational database, which supports ACID (all typically do).</p>"},{"location":"system-design-interview/chapter16/#metadata-database","title":"Metadata database","text":"<p>Here's a simplified table schema for the metadata db (only interesting fields are shown):   * User table contains basic information about the user such as username, email, profile photo, etc.  * Device table stores device info. Push_id is used for sending push notifications. Users can have multiple devices.  * Namespace - root directory of a user  * File table stores everything related to a file  * File_version stores the version history of a file. Existing fields are read-only to sustain file integrity.  * Block - stores everything related to a file block. A file version can be reconstructed by joining all blocks in the correct version.</p>"},{"location":"system-design-interview/chapter16/#upload-flow","title":"Upload flow","text":"<p>In the above flow, two requests are sent in parallel - updating file metadata and uploading the file to cloud storage.</p> <p>Add file metadata:  * Client 1 sends request to update file metadata  * New file metadata is stored and upload status is set to \"pending\"  * Notify the notification service that a new file is being added.  * Notification service notifies relevant clients about the file upload.</p> <p>Upload files to cloud storage:  * Client 1 uploads file contents to block servers  * Block servers chunk the file in blocks, compresses, encrypts them and uploads to cloud storage  * Once file is uploaded, upload completion callback is triggered. Request is sent to API servers.  * File status is changed to \"uploaded\" in Metadata DB.  * Notification service is notified of file uploaded event and client 2 is notified about the new file.</p>"},{"location":"system-design-interview/chapter16/#download-flow","title":"Download flow","text":"<p>Download flow is triggered when file is added or edited elsewhere. Client is notified via:  * Notification if online  * New changes are cached until user comes online if offline at the moment</p> <p>Once a client is notified of the changes, it requests the file metadata and then downloads the blocks to reconstruct the file:   * Notification service informs client 2 of file changes  * Client 2 fetches metadata from API servers  * API servers fetch metadata from metadata DB  * Client 2 gets the metadata  * Once client receives the metadata, it sends requests to block servers to download blocks  * Block servers download blocks from cloud storage and forwards them to the client</p>"},{"location":"system-design-interview/chapter16/#notification-service","title":"Notification service","text":"<p>The notification service enables file changes to be communicated to clients as they happen.</p> <p>Clients can communicate with the notification service via:  * long polling (eg Dropbox uses this approach)  * Web sockets - communication is persistent and bidirectional</p> <p>Both options work well but we opt for long polling because:  * Communication for notification service is not bi-directional. Server sends information to clients, not vice versa.  * WebSocket is meant for real-time bidirectional communication. For google drive, notifications are sent infrequently.</p> <p>With long polling, the client sends a request to the server which stays open until a change is received or timeout is reached.  After that, a subsequent request is sent for next couple of changes.</p>"},{"location":"system-design-interview/chapter16/#save-storage-space","title":"Save storage space","text":"<p>To support file version history and ensure reliability, multiple versions of a file are stored across multiple data centers.</p> <p>Storage space can be filled up quickly. Three techniques can be applied to save storage space:  * De-duplicate data blocks - if two blocks have the same hash, we can only store them once.  * Adopt an intelligent backup strategy - set a limit on max version history and aggregate frequent edits into a single version.  * Move infrequently accessed data to cold storage - eg Amazon S3 glacier is a good option for this, which is much cheaper than Amazon S3.</p>"},{"location":"system-design-interview/chapter16/#failure-handling","title":"Failure handling","text":"<p>Some typical failures and how you could resolve them:  * Load balancer failure - If a load balancer fails, a secondary becomes active and picks up the traffic.  * Block server failure - If a block server fails, other replicas pick up the traffic and finish the job.  * Cloud storage failure - S3 buckets are replicated across regions. If one region fails, traffic is redirected to the other one.  * API server failure - Traffic is redirected to other service instances by the load balancer.  * Metadata cache failure - Metadata cache servers are replicated multiple times. If one goes down, other nodes are still available.  * Metadata DB failure - if master is down, promote one of the slaves to be master. If slave is down, use another one for read operations.  * Notification service failure - If long polling connections are lost, clients reconnect to a different service replica, but reconnection of millions of clients will take some time.  * Offline backup queue failure - Queues are replicated multiple times. If one queue fails, consumers need to resubscribe to the backup queue.</p>"},{"location":"system-design-interview/chapter16/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Properties of our Google Drive system design in a nutshell:  * Strongly consistent  * Low network bandwidth  * Fast sync</p> <p>Our design contains two flows - file upload and file sync.</p> <p>If time permits, you could discuss alternative design approaches as there is no perfect design. For example, we can upload blocks directly to cloud storage instead of going through block servers. </p> <p>This is faster than our approach but has drawbacks:  * Chunking, compression, encryption need to be implemented on different platforms (Android, iOS, Web).   * Client can be hacked so implementing encryption client-side is not ideal.</p> <p>Another interesting discussion is moving online/offline logic to separate service so that other services can reuse it to implement interesting functionality.</p>"},{"location":"system-design-interview/chapter17/","title":"Proximity Service","text":"<p>A proximity service enables you to discover nearby places such as restaurants, hotels, theatres, etc.</p>"},{"location":"system-design-interview/chapter17/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>Sample questions to understand the problem better:  * C: Can a user specify a search radius? What if there are not enough businesses within the search area?  * I: We only care about businesses within a certain area. If time permits, we can discuss enhancing the functionality.  * C: What's the max radius allowed? Can I assume it's 20km?  * I: Yes, that is a reasonable assumption  * C: Can a user change the search radius via the UI?  * I: Yes, let's say we have the options - 0.5km, 1km, 2km, 5km, 20km  * C: How is business information modified? Do we need to reflect changes in real-time?  * I: Business owners can add/delete/update a business. Assume changes are going to be propagated on the next day.  * C: How do we handle search results while the user is moving?  * I: Let's assume we don't need to constantly update the page since users are moving slowly.</p>"},{"location":"system-design-interview/chapter17/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Return all businesses based on user's location</li> <li>Business owners can add/delete/update a business. Information is not reflected in real-time.</li> <li>Customers can view detailed information about a business</li> </ul>"},{"location":"system-design-interview/chapter17/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Low latency - users should be able to see nearby businesses quickly</li> <li>Data privacy - Location info is sensitive data and we should take this into consideration in order to comply with regulations</li> <li>High availability and scalability requirements - We should ensure system can handle spike in traffic during peak hours in densely populated areas</li> </ul>"},{"location":"system-design-interview/chapter17/#back-of-the-envelope-calculation","title":"Back-of-the-envelope calculation","text":"<ul> <li>Assuming 100mil daily active users and 200mil businesses</li> <li>Search QPS == 100mil * 5 (average searches per day) / 10^5 (seconds in day) == 5000</li> </ul>"},{"location":"system-design-interview/chapter17/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and get Buy-In","text":""},{"location":"system-design-interview/chapter17/#api-design","title":"API Design","text":"<p>We'll use a RESTful API convention to design a simplified version of the APIs. <pre><code>GET /v1/search/nearby\n</code></pre></p> <p>This endpoint returns businesses based on search criteria, paginated.</p> <p>Request parameters - latitude, longitude, radius</p> <p>Example response: <pre><code>{\n  \"total\": 10,\n  \"businesses\":[{business object}]\n}\n</code></pre></p> <p>The endpoint returns everything required to render a search results page, but a user might require additional details about a particular business, fetched via other endpoints.</p> <p>Here's some other business APIs we'll need:  * <code>GET /v1/businesses/{:id}</code> - return business detailed info  * <code>POST /v1/businesses</code> - create a new business  * <code>PUT /v1/businesses/{:id}</code> - update business details  * <code>DELETE /v1/businesses/{:id}</code> - delete a business</p>"},{"location":"system-design-interview/chapter17/#data-model","title":"Data model","text":"<p>In this problem, the read volume is high because these features are commonly used:  * Search for nearby businesses  * View the detailed information of a business</p> <p>On the other hand, write volume is low because we rarely change business information. Hence for a read-heavy workflow, a relational database such as MySQL is ideal.</p> <p>In terms of schema, we'll need one main <code>business</code> table which holds information about a business: </p> <p>We'll also need a geo-index table so that we efficiently process spatial operations. This table will be discussed later when we introduce the concept of geohashes.</p>"},{"location":"system-design-interview/chapter17/#high-level-design","title":"High-level design","text":"<p>Here's a high-level overview of the system:   * The load balancer automatically distributes incoming traffic across multiple services. A company typically provides a single DNS entry point and internally routes API calls to appropriate services based on URL paths.  * Location-based service (LBS) - read-heavy, stateless service, responsible for serving read requests for nearby businesses  * Business service - supports CRUD operations on businesses.  * Database cluster - stores business information and replicates it in order to scale reads. This leads to some inconsistency for LBS to read business information, which is not an issue for our use-case  * Scalability of business service and LBS - since both services are stateless, we can easily scale them horizontally</p>"},{"location":"system-design-interview/chapter17/#algorithms-to-fetch-nearby-businesses","title":"Algorithms to fetch nearby businesses","text":"<p>In real life, one might use a geospatial database, such as Geohash in Redis or Postgres with PostGIS extension.</p> <p>Let's explore how these databases work and what other alternative algorithms there are for this type of problem.</p>"},{"location":"system-design-interview/chapter17/#two-dimensional-search","title":"Two-dimensional search","text":"<p>The most intuitive and naive approach to solving this problem is to draw a circle around the person and fetch all businesses within the circle's radius: </p> <p>This can easily be translated to a SQL query: <pre><code>SELECT business_id, latitude, longitude,\nFROM business\nWHERE (latitude BETWEEN {:my_lat} - radius AND {:my_lat} + radius) AND\n      (longitude BETWEEN {:my_long} - radius AND {:my_long} + radius)\n</code></pre></p> <p>This query is not efficient because we need to query the whole table. An alternative is to build an index on the longitude and latitude columns but that won't improve performance by much.</p> <p>This is because we still need to subsequently filter a lot of data regardless of whether we index by long or lat: </p> <p>We can, however, build 2D indexes and there are different approaches to that: </p> <p>We'll discuss the ones highlighted in purple - geohash, quadtree and google S2 are the most popular approaches.</p>"},{"location":"system-design-interview/chapter17/#evenly-divided-grid","title":"Evenly divided grid","text":"<p>Another option is to divide the world in small grids: </p> <p>The major flaw with this approach is that business distribution is uneven as there are a lot of businesses concentrated in new york and close to zero in the sahara desert.</p>"},{"location":"system-design-interview/chapter17/#geohash","title":"Geohash","text":"<p>Geohash works similarly to the previous approach, but it recursively divides the world into smaller and smaller grids, where each two bits correspond to a single quadrant: </p> <p>Geohashes are typically represented in base32. Here's the example geohash of google headquarters: <pre><code>1001 10110 01001 10000 11011 11010 (base32 in binary) \u2192 9q9hvu (base32)\n</code></pre></p> <p>It supports 12 levels of precision, but we only need up to 6 levels for our use-case: </p> <p>Geohashes enable us to quickly locate neighboring regions based on a substring of the geohash: </p> <p>However, one issue \\w geohashes is that there can be places which are very close to each other which don't share any prefix, because they're on different sides of the equator or meridian: </p> <p>Another issue is that two businesses can be very close but not share a common prefix because they're in different quadrants: </p> <p>This can be mitigated by fetching neighboring geohashes, not just the geohash of the user.</p> <p>A benefit of using geohashes is that we can use them to easily implement the bonus problem of increasing search radius in case insufficient businesses are fetched via query: </p> <p>This can be done by removing the last letter of the target geohash to increase radius.</p>"},{"location":"system-design-interview/chapter17/#quadtree","title":"Quadtree","text":"<p>A quadtree is a data structure, which recursively subdivides quadrants as deep as it needs to, based on business needs: </p> <p>This is an in-memory solution which can't easily be implemented in a database.</p> <p>Here's how it might look conceptually: </p> <p>Example pseudocode to build a quadtree: <pre><code>public void buildQuadtree(TreeNode node) {\n    if (countNumberOfBusinessesInCurrentGrid(node) &gt; 100) {\n        node.subdivide();\n        for (TreeNode child : node.getChildren()) {\n            buildQuadtree(child);\n        }\n    }\n}\n</code></pre></p> <p>In a leaf node, we store:  * Top-left, bottom-right coordinates to identify the quadrant dimensions  * List of business IDs in the grid</p> <p>In an internalt node we store:  * Top-left, bottom-right coordinates of quadrant dimensions  * 4 pointers to children</p> <p>The total memory to represent the quadtree is calculated as ~1.7GB in the book if we assume that we operate with 200mil businesses.</p> <p>Hence, a quadtree can be stored in a single server, in-memory, although we can of course replicate it for redundancy and load balancing purposes.</p> <p>One consideration to take into consideration if this approach is adopted - startup time of server can be a couple of minutes while the quadtree is being built.</p> <p>Hence, this should be taken into account during the deployment process. Eg a healthcheck endpoint can be exposed and queried to signal when the quadtree build is finished.</p> <p>Another consideration is how to update the quadtree. Given our requirements, a good option would be to update it every night using a nightly job due to our commitment of reflecting changes at start of next day.</p> <p>It is nevertheless possible to update the quadtree on the fly, but that would complicate the implementation significantly.</p> <p>Example quadtree of Denver: </p>"},{"location":"system-design-interview/chapter17/#google-s2","title":"Google S2","text":"<p>Google S2 is a geometry library, which supports mapping 2D points on a 1D plane using hilbert curves. Objects close to each other on the 2D plane are close on the hilbert curve as well: </p> <p>This library is great for geofencing, which supports covering arbitrary areas vs. confining yourself to specific quadrants. </p> <p>This functionality can be used to support more advanced use-cases than nearby businesses.</p> <p>Another benefit of Google S2 is its region cover algorithm, which enables us to define more granular precision levels, than those provided by geohashes.</p>"},{"location":"system-design-interview/chapter17/#recommendation","title":"Recommendation","text":"<p>There is no perfect solution, different companies adopt different solutions: </p> <p>Author suggest choosing geohashes or quadtree in an interview as those are easier to explain than Google S2.</p> <p>Here's a quick summary of geohashes:  * Easy to use and implement, no need to build a tree  * supports returning businesses within a specified radius  * Geohash precision is fixed. More complex logic is required if a more granular precision is needed  * Updating the index is easy</p> <p>Here's a quick summary of quadtrees:  * Slightly harder to implement as it requires us to build a tree  * Supports fetching k-nearest neighbors instead of businesses within radius, which can be a good use-case for certain features  * Grid size can be dynamically adjusted based on population density  * Updating the index is more complicated than updating the geohash variant. All problems with updating and balancing trees are present when working with quad trees.</p>"},{"location":"system-design-interview/chapter17/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's dive deeper into some areas of the design.</p>"},{"location":"system-design-interview/chapter17/#scale-the-database","title":"Scale the database","text":"<p>The business table can be scaled by sharding it in case it doesn't fit in a single server instance.</p> <p>The geohash table can be represented by two columns: </p> <p>We don't need to shard the geohash table as we don't have that much data. We calculated that it takes ~1.7gb to build a quad tree and geohash space usage is similar.</p> <p>We can, however, replicate the table to scale the read load.</p>"},{"location":"system-design-interview/chapter17/#caching","title":"Caching","text":"<p>Before using caching, we should ask ourselves if it is really necessary. In our case, the workflow is read-heavy and data can fit into a single server, so this kind of data is ripe for caching.</p> <p>We should be careful when choosing the cache key. Location coordinates are not a good cache key as they often change and can be inaccurate.</p> <p>Using the geohash is a more suitable key candidate.</p> <p>Here's how we could query all businesses in a geohash: <pre><code>SELECT business_id FROM geohash_index WHERE geohash LIKE `{:geohash}%`\n</code></pre></p> <p>Here's example code to cache the data in redis: <pre><code>public List&lt;String&gt; getNearbyBusinessIds(String geohash) {\n    String cacheKey = hash(geohash);\n    List&lt;string&gt; listOfBusinessIds = Redis.get(cacheKey);\n    if (listOfBusinessIDs  == null) {\n        listOfBusinessIds = Run the select SQL query above;\n        Cache.set(cacheKey, listOfBusinessIds, \"1d\");\n    }\n    return listOfBusinessIds;\n}\n</code></pre></p> <p>We can cache the data on all precisions we support, which are not a lot, ie <code>geohash_4, geohash_5, geohash_6</code>.</p> <p>As we already discussed, the storage requirements are not high and can fit into a single redis server, but we could replicate it for redundancy purposes as well as to scale reads.</p> <p>We can even deploy multiple redis replicas across different data centers.</p> <p>We could also cache <code>business_id -&gt; business_data</code> as users could often query the details of the same popular restaurant.</p>"},{"location":"system-design-interview/chapter17/#region-and-availability-zones","title":"Region and availability zones","text":"<p>We can deploy multiple LBS service instances across the globe so that users query the instance closest to them. This leads to reduced latency; </p> <p>It also enables us to spread traffic evenly across the globe. This could also be required in order to comply with certain data privacy laws.</p>"},{"location":"system-design-interview/chapter17/#follow-up-question-filter-businesses-by-type-or-time","title":"Follow-up question - filter businesses by type or time","text":"<p>Once businesses are filtered, the result set is going to be small, hence, it is acceptable to filter the data in-memory.</p>"},{"location":"system-design-interview/chapter17/#final-design-diagram","title":"Final design diagram","text":"<p>  * Client tries to locate restaurants within 500meters of their location  * Load balancer forwards the request to the LBS  * LBS maps the radius to geohash with length 6  * LBS calculates neighboring geohashes and adds them to the list  * For each geohash, LBS calls the redis server to fetch corresponding business IDs. This can be done in parallel.  * Finally, LBS hydrates the business ids, filters the result and returns it to the user  * Business-related APIs are separated from the LBS into the business service, which checks the cache first for any read requests before consulting the database  * Business updates are handled via a nightly job, which updates the geohash store</p>"},{"location":"system-design-interview/chapter17/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Summary of some of the more interesting topics we covered:  * Discussed several indexing options - 2d search, evenly divided grid, geohash, quadtree, google S2  * Discussed caching, replication, sharding, cross-DC deployments in the deep dive section</p>"},{"location":"system-design-interview/chapter18/","title":"Nearby Friends","text":"<p>This chapter focuses on designing a scalable backend for an application which enables user to share their location and discover friends who are nearby.</p> <p>The major difference with the proximity chapter is that in this problem, locations constantly change, whereas in that one, business addresses more or less stay the same.</p>"},{"location":"system-design-interview/chapter18/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>Some questions to drive the interview:  * C: How geographically close is considered to be \"nearby\"?  * I: 5 miles, this number should be configurable  * C: Is distance calculated as straight-line distance vs. taking into consideration eg a river in-between friends  * I: Yes, that is a reasonable assumption  * C: How many users does the app have?  * I: 1bil users and 10% of them use the nearby friends feature  * C: Do we need to store location history?  * I: Yes, it can be valuable for eg machine learning  * C: Can we assume inactive friends will disappear from the feature in 10min  * I: Yes  * C: Do we need to worry about GDPR, etc?  * I: No, for simlicity's sake</p>"},{"location":"system-design-interview/chapter18/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Users should be able to see nearby friends on their mobile app. Each friend has a distance and timestamp, indicating when the location was updated</li> <li>Nearby friends list should be updated every few seconds</li> </ul>"},{"location":"system-design-interview/chapter18/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Low latency - it's important to receive location updates without too much delay</li> <li>Reliability - Occassional data point loss is acceptable, but system should be generally available</li> <li>Eventual consistency - Location data store doesn't need strong consistency. Few seconds delay in receiving location data in different replicas is acceptable</li> </ul>"},{"location":"system-design-interview/chapter18/#back-of-the-envelope","title":"Back-of-the-envelope","text":"<p>Some estimations to determine potential scale:  * Nearby friends are friends within 5mile radius  * Location refresh interval is 30s. Human walking speed is slow, hence, no need to update location too frequently.  * On average, 100mil users use the feature every day \\w 10% concurrent users, ie 10mil  * On average, a user has 400 friends, all of them use the nearby friends feature  * App displays 20 nearby friends per page  * Location Update QPS = 10mil / 30 == ~334k updates per second</p>"},{"location":"system-design-interview/chapter18/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>Before exploring API and data model design, we'll study the communication protocol we'll use as it's less ubiquitous than traditional request-response communication model.</p>"},{"location":"system-design-interview/chapter18/#high-level-design","title":"High-level design","text":"<p>At a high-level we'd want to establish effective message passing between peers. This can be done via a peer-to-peer protocol, but that's not practical for a mobile app with flaky connection and tight power consumption constraints.</p> <p>A more practical approach is to use a shared backend as a fan-out mechanism towards friends you want to reach: </p> <p>What does the backend do?  * Receives location updates from all active users  * For each location update, find all active users which should receive it and forward it to them  * Do not forward location data if distance between friends is beyond the configured threshold</p> <p>This sounds simple but the challenge is to design the system for the scale we're operating with.</p> <p>We'll start with a simpler design at first and discuss a more advanced approach in the deep dive:   * The load balancer spreads traffic across rest API servers as well as bidirectional web socket servers  * The rest API servers handles auxiliary tasks such as managing friends, updating profiles, etc  * The websocket servers are stateful servers, which forward location update requests to respective clients. It also manages seeding the mobile client with nearby friends locations at initialization (discussed in detail later).  * Redis location cache is used to store most recent location data for each active user. There is a TTL set on each entry in the cache. When the TTL expires, user is no longer active and their data is removed from the cache.  * User database stores user and friendship data. Either a relational or NoSQL database can be used for this purpose.  * Location history database stores a history of user location data, not necessarily used directly within nearby friends feature, but instead used to track historical data for analytical purposes  * Redis pubsub is used as a lightweight message bus which enables different topics for each user channel for location updates. </p> <p>In the above example, websocket servers subscribe to channels for the users which are connected to them &amp; forward location updates whenever they receive them to appropriate users.</p>"},{"location":"system-design-interview/chapter18/#periodic-location-update","title":"Periodic location update","text":"<p>Here's how the periodic location update flow works:   * Mobile client sends a location update to the load balancer  * Load balancer forwards location update to the websocket server's persistent connection for that client  * Websocket server saves location data to location history database  * Location data is updated in location cache. Websocket server also saves location data in-memory for subsequent distance calculations for that user  * Websocket server publishes location data in user's channel via redis pub sub  * Redis pubsub broadcasts location update to all subscribers for that user channel, ie servers responsible for the friends of that user  * Subscribed web socket servers receive location update, calculate which users the update should be sent to and sends it</p> <p>Here's a more detailed version of the same flow: </p> <p>On average, there's going to be 40 location updates to forward as a user has 400 friends on average and 10% of them are online at a time.</p>"},{"location":"system-design-interview/chapter18/#api-design","title":"API Design","text":"<p>Websocket Routines we'll need to support:  * periodic location update - user sends location data to websocket server  * client receives location update - server sends friend location data and timestamp  * websocket client initialization - client sends user location, server sends back nearby friends location data  * Subscribe to a new friend - websocket server sends a friend ID mobile client is supposed to track eg when friend appears online for the first time  * Unsubscribe a friend - websocket server sends a friend ID, mobile client is supposed to unsubscribe from due to eg friend going offline</p> <p>HTTP API - traditional request/response payloads for auxiliary responsibilities.</p>"},{"location":"system-design-interview/chapter18/#data-model","title":"Data model","text":"<ul> <li>The location cache will store a mapping between <code>user_id</code> and <code>lat,long,timestamp</code>. Redis is a great choice for this cache as we only care about current location and it supports TTL eviction which we need for our use-case.</li> <li>Location history table stores the same data but in a relational table \\w the four columns stated above. Cassandra can be used for this data as it is optimized for write-heavy loads.</li> </ul>"},{"location":"system-design-interview/chapter18/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's discuss how we scale the high-level design so that it works at the scale we're targetting.</p>"},{"location":"system-design-interview/chapter18/#how-well-does-each-component-scale","title":"How well does each component scale?","text":"<ul> <li>API servers - can be easily scaled via autoscaling groups and replicating server instances</li> <li>Websocket servers - we can easily scale out the ws servers, but we need to ensure we gracefully shutdown existing connections when tearing down a server. Eg we can mark a server as \"draining\" in the load balancer and stop sending connections to it, prior to being finally removed from the server pool</li> <li>Client initialization - when a client first connects to a server, it fetches the user's friends, subscribes to their channels on redis pubsub, fetches their location from cache and finally forwards to client</li> <li>User database - We can shard the database based on user_id. It might also make sense to expose user/friends data via a dedicated service and API, managed by a dedicated team</li> <li>Location cache - We can shard the cache easily by spinning up several redis nodes. Also, the TTL puts a limit on the max memory we could have taken up at a time. But we still want to handle the large write load</li> <li>Redis pub/sub server - we leverage the fact that no memory is consumed if there are channels initialized but are not in use. Hence, we can pre-allocate channels for all users who use the nearby friends feature to avoid having to deal with eg bringing up a new channel when a user comes online and notifying active websocket servers</li> </ul>"},{"location":"system-design-interview/chapter18/#scaling-deep-dive-on-redis-pubsub-component","title":"Scaling deep-dive on redis pub/sub component","text":"<p>We will need around 200gb of memory to maintain all pub/sub channels. This can be achieved by using 2 redis servers with 100gb each.</p> <p>Given that we need to push ~14mil location updates per second, we will however need at least 140 redis servers to handle that amount of load, assuming that a single server can handle ~100k pushes per second.</p> <p>Hence, we'll need a distributed redis server cluster to handle the intense CPU load.</p> <p>In order to support a distributed redis cluster, we'll need to utilize a service discovery component, such as zookeeper or etcd, to keep track of which servers are alive.</p> <p>What we need to encode in the service discovery component is this data: </p> <p>Web socket servers use that encoded data, fetched from zookeeper to determine where a particular channel lives. For efficiency, the hash ring data can be cached in-memory on each websocket server.</p> <p>In terms of scaling the server cluster up or down, we can setup a daily job to scale the cluster as needed based on historical traffic data. We can also overprovision the cluster to handle spikes in loads.</p> <p>The redis cluster can be treated as a stateful storage server as there is some state maintained for the channels and there is a need for coordination with subscribers so that they hand-off to newly provisioned nodes in the cluster.</p> <p>We have to be mindful of some potential issues during scaling operations:  * There will be a lot of resubscription requests from the web socket servers due to channels being moved around  * Some location updates might be missed from clients during the operation, which is acceptable for this problem, but we should still minimize it from happening. Consider doing such operation when traffic is at lowest point of the day.  * We can leverage consistent hashing to minimize amount of channels moved in the event of adding/removing servers </p>"},{"location":"system-design-interview/chapter18/#addingremoving-friends","title":"Adding/removing friends","text":"<p>Whenever a friend is added/removed, websocket server responsible for affected user needs to subscribe/unsubscribe from the friend's channel.</p> <p>Since the \"nearby friends\" feature is part of a larger app, we can assume that a callback on the mobile client side can be registered whenever any of the events occur and the client will send a message to the websocket server to do the appropriate action.</p>"},{"location":"system-design-interview/chapter18/#users-with-many-friends","title":"Users with many friends","text":"<p>We can put a cap on the total number of friends one can have, eg facebook has a cap of 5000 max friends.</p> <p>The websocket server handling the \"whale\" user might have a higher load on its end, but as long as we have enough web socket servers, we should be okay.</p>"},{"location":"system-design-interview/chapter18/#nearby-random-person","title":"Nearby random person","text":"<p>What if the interviewer wants to update the design to include a feature where we can occasionally see a random person pop up on our nearby friends map?</p> <p>One way to handle this is to define a pool of pubsub channels, based on geohash: </p> <p>Anyone within the geohash subscribes to the appropriate channel to receive location updates for random users: </p> <p>We could also subscribe to several geohashes to handle cases where someone is close but in a bordering geohash: </p>"},{"location":"system-design-interview/chapter18/#alternative-to-redis-pubsub","title":"Alternative to Redis pub/sub","text":"<p>An alternative to using Redis for pub/sub is to leverage Erlang - a general programming language, optimized for distributed computing applications.</p> <p>With it, we can spawn millions of small, erland processes which communicate with each other. We can handle both websocket connections and pub/sub channels within the distributed erlang application.</p> <p>A challenge with using Erlang, though, is that it's a niche programming language and it could be hard to source strong erlang developers.</p>"},{"location":"system-design-interview/chapter18/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>We successfully designed a system, supporting the nearby friends features.</p> <p>Core components:  * Web socket servers - real-time comms between client and server  * Redis - fast read and write of location data + pub/sub channels</p> <p>We also explored how to scale restful api servers, websocket servers, data layer, redis pub/sub servers and we also explored an alternative to using Redis Pub/Sub. We also explored a \"random nearby person\" feature.</p>"},{"location":"system-design-interview/chapter19/","title":"Google Maps","text":"<p>We'll design a simple version of Google Maps.</p> <p>Some facts about google maps:  * Started in 2005  * Provides various services - satellite imagery, street maps, real-time traffic conditions, route planning  * By 2021, had 1bil daily active users, 99% coverage of the world, 25mil updates daily of real-time location info</p>"},{"location":"system-design-interview/chapter19/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>Sample Q&amp;A between candidate and interviewer:  * C: How many daily active users are we dealing with?  * I: 1bil DAU  * C: What features should we focus on?  * I: Location update, navigation, ETA, map rendering  * C: How large is road data? Do we have access to it?  * I: We obtained road data from various sources, it's TBs of raw data  * C: Should we take traffic conditions into consideration?  * I: Yes, we should for accurate time estimations  * C: How about different travel modes - by foot, biking, driving?  * I: We should support those  * C: How about multi-stop directions?  * I: Let's not focus on that for scope of interview  * C: Business places and photos?  * I: Good question, but no need to consider those</p> <p>We'll focus on three key features - user location update, navigation service including ETA, map rendering.</p>"},{"location":"system-design-interview/chapter19/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Accuracy - user shouldn't get wrong directions</li> <li>Smooth navigation - Users should experience smooth map rendering</li> <li>Data and battery usage - Client should use as little data and battery as possible. Important for mobile devices.</li> <li>General availability and scalability requirements</li> </ul>"},{"location":"system-design-interview/chapter19/#map-101","title":"Map 101","text":"<p>Before jumping into the design, there are some map-related concepts we should understand.</p>"},{"location":"system-design-interview/chapter19/#positioning-system","title":"Positioning system","text":"<p>World is a sphere, rotating on its axis. Positiions are defined by latitude (how far north/south you are) and longitude (how far east/west you are): </p>"},{"location":"system-design-interview/chapter19/#going-from-3d-to-2d","title":"Going from 3D to 2D","text":"<p>The process of translating points from 3D to 2D plane is called \"map projection\".</p> <p>There are different ways to do it and each comes with its pros and cons. Almost all distort the actual geometry. </p> <p>Google maps selected a modified version of Mercator projection called \"Web Mercator\".</p>"},{"location":"system-design-interview/chapter19/#geocoding","title":"Geocoding","text":"<p>Geocoding is the process of converting addresses to geographic coordinates. </p> <p>The reverse process is called \"reverse geocoding\".</p> <p>One way to achieve this is to use interpolation - leveraging data from different sources (eg GIS-es) where street network is mapped to geo coordinate space.</p>"},{"location":"system-design-interview/chapter19/#geohashing","title":"Geohashing","text":"<p>Geohashing is an encoding system which encodes a geographic area into a string of letters and digits.</p> <p>It depicts the world as a flattened surface and recursively sub-divides it into four quadrants: </p>"},{"location":"system-design-interview/chapter19/#map-rendering","title":"Map rendering","text":"<p>Map rendering happens via tiling. Instead of rendering entire map as one big custom image, world is broken up into smaller tiles.</p> <p>Client only downloads relevant tiles and renders them like stitching together a mosaic.</p> <p>There are different tiles for different zoom levels. Client chooses appropriate tiles based on the client's zoom level.</p> <p>Eg, zooming out the entire world would download only a single 256x256 tile, representing the whole world.</p>"},{"location":"system-design-interview/chapter19/#road-data-processing-for-navigation-algorithms","title":"Road data processing for navigation algorithms","text":"<p>In most routing algorithms, intersections are represented as nodes and roads are represented as edges: </p> <p>Most navigation algorithms use a modified version of Djikstra or A* algorithms.</p> <p>Pathfinding performance is sensitive to the size of the graph. To work at scale, we can't represent the whole world as a graph and run the algorithm on it.</p> <p>Instead, we use a technique similar to tiling - we subdivide the world into smaller and smaller graphs.</p> <p>Routing tiles hold references to neighboring tiles and algorithms can stitch together a bigger road graph as it traverses interconnected tiles: </p> <p>This technique enables us to significantly reduce memory bandwidth and only load the tiles we need for the given source/destination pair.</p> <p>However, for larger routes, stitching together small, detailed routing tiles would still be time/memory consuming. Instead, there are routing tiles with different level of detail and the algorithm uses the appropriately-detailed tiles, based on the destination we're headed for: </p>"},{"location":"system-design-interview/chapter19/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>For storage, we need to store:  * map of the world - estimated as ~70pb based on all the tiles we need to store, but factoring in compression of very similar tiles (eg vast desert)  * metadata - negligible in size, so we can skip it from calculation  * Road info - stored as routing tiles</p> <p>Estimated QPS for navigation requests - 1bil DAU at 35min of usage per week -&gt; 5bil minutes per day.  Assuming gps update requests are batched, we arrive at 200k QPS and 1mil QPS at peak load</p>"},{"location":"system-design-interview/chapter19/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"system-design-interview/chapter19/#location-service","title":"Location service","text":"<p>It is responsible for recording a user's location updates:  * location updates are sent every <code>t</code> seconds  * location data streams can be used to improve the service over time, eg provide more accurate ETAs, monitor traffic data, detect closed roads, analyze user behavior, etc</p> <p>Instead of sending location updates to the server all the time, we can batch the updates on the client-side and send batches instead: </p> <p>Despite this optimization, for a system of Google Maps scale, load will still be significant. Therefore, we can leverage a database, optimized for heavy writes such as Cassandra.</p> <p>We can also leverage Kafka for efficient stream processing of location updates, meant for further analysis.</p> <p>Example location update request payload: <pre><code>POST /v1/locations\nParameters\n  locs: JSON encoded array of (latitude, longitude, timestamp) tuples.\n</code></pre></p>"},{"location":"system-design-interview/chapter19/#navigation-service","title":"Navigation service","text":"<p>This component is responsible for finding fast routes between A and B in a reasonable time (a little bit of latency is okay). Route need not be the fastest, but accuracy is important.</p> <p>Example request payload: <pre><code>GET /v1/nav?origin=1355+market+street,SF&amp;destination=Disneyland\n</code></pre></p> <p>Example response: <pre><code>{\n  \"distance\": {\"text\":\"0.2 mi\", \"value\": 259},\n  \"duration\": {\"text\": \"1 min\", \"value\": 83},\n  \"end_location\": {\"lat\": 37.4038943, \"Ing\": -121.9410454},\n  \"html_instructions\": \"Head &lt;b&gt;northeast&lt;/b&gt; on &lt;b&gt;Brandon St&lt;/b&gt; toward &lt;b&gt;Lumin Way&lt;/b&gt;&lt;div style=\\\"font-size:0.9em\\\"&gt;Restricted usage road&lt;/div&gt;\",\n  \"polyline\": {\"points\": \"_fhcFjbhgVuAwDsCal\"},\n  \"start_location\": {\"lat\": 37.4027165, \"lng\": -121.9435809},\n  \"geocoded_waypoints\": [\n    {\n       \"geocoder_status\" : \"OK\",\n       \"partial_match\" : true,\n       \"place_id\" : \"ChIJwZNMti1fawwRO2aVVVX2yKg\",\n       \"types\" : [ \"locality\", \"political\" ]\n    },\n    {\n       \"geocoder_status\" : \"OK\",\n       \"partial_match\" : true,\n       \"place_id\" : \"ChIJ3aPgQGtXawwRLYeiBMUi7bM\",\n       \"types\" : [ \"locality\", \"political\" ]\n    }\n  ],\n  \"travel_mode\": \"DRIVING\"\n}\n</code></pre></p> <p>Traffic changes and reroutes are not taken into consideration yet, those will be tackled in the deep dive section.</p>"},{"location":"system-design-interview/chapter19/#map-rendering_1","title":"Map rendering","text":"<p>Holding the entire data set of mapping tiles on the client-side is not feasible as it's petabytes in size.</p> <p>They need to be fetched on-demand from the server, based on the client's location and zoom level.</p> <p>When should new tiles be fetched - while user is zooming in/out and during navigation, while they're going towards a new tile.</p> <p>How should the map tiles be served to the client?  * They can be built dynamically, but that puts a huge load on the server and also makes caching hard  * Map tiles are served statically, based on their geohash, which a client can calculate. They can be statically stored &amp; served from a CDN </p> <p>CDNs enable users to fetch map tiles from point-of-presence servers (POP) which are closest to users in order to minimize latency: </p> <p>Options to consider for determining map tiles:  * geohash for map tile can be calculated on the client-side. If that's the case, we should be careful that we commit to this type of map tile calculation for the long-term as forcing clients to update is hard  * alternatively, we can have simple API which calculates the map tile URLs on behalf of the clients at the cost of additional API call </p>"},{"location":"system-design-interview/chapter19/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":""},{"location":"system-design-interview/chapter19/#data-model","title":"Data model","text":"<p>Let's discuss how we store the different types of data we're dealing with.</p>"},{"location":"system-design-interview/chapter19/#routing-tiles","title":"Routing tiles","text":"<p>Initial road data set is obtained from different sources. It is improved over time based on location updates data.</p> <p>The road data is unstructured. We have a periodic offline processing pipeline, which transforms this raw data into the graph-based routing tiles our app needs.</p> <p>Instead of storing these tiles in a database as we don't need any database features. We can store them in S3 object storage, while caching them agressively.</p> <p>We can also leverage libraries to compress adjacency lists into binary files efficiently.</p>"},{"location":"system-design-interview/chapter19/#user-location-data","title":"User location data","text":"<p>User location data is very useful for updaring traffic conditions and doing all sorts of other analysis.</p> <p>We can use Cassandra for storing this kind of data as its nature is to be write-heavy.</p> <p>Example row: </p>"},{"location":"system-design-interview/chapter19/#geocoding-database","title":"Geocoding database","text":"<p>This database stores a key-value pair of lat/long pairs and places.</p> <p>We can use Redis for its fast read access speed, as we have frequent read and infrequent writes.</p>"},{"location":"system-design-interview/chapter19/#precomputed-images-of-the-world-map","title":"Precomputed images of the world map","text":"<p>As we discussed, we will precompute map tiling images and store them in CDN. </p>"},{"location":"system-design-interview/chapter19/#services","title":"Services","text":""},{"location":"system-design-interview/chapter19/#location-service_1","title":"Location service","text":"<p>Let's focus on the database design and how user location is stored in detail for this service. </p> <p>We can use a NoSQL database to facilitate the heavy write load we have on location updates. We prioritize availability over consistency as user location data often changes and becomes stale as new updates arrive.</p> <p>We'll choose Cassandra as our database choice as it nicely fits all our requirements.</p> <p>Example row we're going to store:   * <code>user_id</code> is the partition key in order to quickly access all location updates for a particular user  * <code>timestamp</code> is the clustering key in order to store the data sorted by the time a location update is received</p> <p>We also leverage Kafka to stream location updates to various other service which need the location updates for various purposes: </p>"},{"location":"system-design-interview/chapter19/#rendering-map","title":"Rendering map","text":"<p>Map tiles are stored at various zoom levels. At the lowest zoom level, the entire world is represented by a single 256x256 tile.</p> <p>As zoom levels increase, the number of map tiles quadruples: </p> <p>One optimization we can use is to not send the entire image information over the network, but instead represent tiles as vectors (paths &amp; polygons) and let the client render the tiles dynamically.</p> <p>This will have substantial bandwidth savings.</p>"},{"location":"system-design-interview/chapter19/#navigation-service_1","title":"Navigation service","text":"<p>This service is responsible for finding the fastest routes: </p> <p>Let's go through each component in this sub-system.</p> <p>First, we have the geocoding service which resolves an address to a location of lat/long pair.</p> <p>Example request: <pre><code>https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA\n</code></pre></p> <p>Example response: <pre><code>{\n   \"results\" : [\n      {\n         \"formatted_address\" : \"1600 Amphitheatre Parkway, Mountain View, CA 94043, USA\",\n         \"geometry\" : {\n            \"location\" : {\n               \"lat\" : 37.4224764,\n               \"lng\" : -122.0842499\n            },\n            \"location_type\" : \"ROOFTOP\",\n            \"viewport\" : {\n               \"northeast\" : {\n                  \"lat\" : 37.4238253802915,\n                  \"lng\" : -122.0829009197085\n               },\n               \"southwest\" : {\n                  \"lat\" : 37.4211274197085,\n                  \"lng\" : -122.0855988802915\n               }\n            }\n         },\n         \"place_id\" : \"ChIJ2eUgeAK6j4ARbn5u_wAGqWA\",\n         \"plus_code\": {\n            \"compound_code\": \"CWC8+W5 Mountain View, California, United States\",\n            \"global_code\": \"849VCWC8+W5\"\n         },\n         \"types\" : [ \"street_address\" ]\n      }\n   ],\n   \"status\" : \"OK\"\n}\n</code></pre></p> <p>The route planner service computes a suggested route, optimized for travel time according to current traffic conditions.</p> <p>The shortest-path service runs a variation of the A* algorithm against the routing tiles in object storage to compute an optimal path:  * It receives the source/destination pairs, converts them to lat/long pairs and derives the geohashes from those pairs to derive the routing tiles  * The algorithm starts from the initial routing tile and starts traversing it until a good enough path is found to the destination tile </p> <p>The ETA service is called by the route planner to get estimated time based on machine learning algorithms, predicting ETA based on traffic data.</p> <p>The ranker service is responsible to rank different possible paths based on filters, passed by the user, ie flags to avoid toll roads or freeways.</p> <p>The updater service asynchronously update some of the important databases to keep them up-to-date.</p>"},{"location":"system-design-interview/chapter19/#improvement-adaptive-eta-and-rerouting","title":"Improvement - adaptive ETA and rerouting","text":"<p>One improvement we can do is to adaptively update in-flight routes based on newly available traffic data.</p> <p>One way to implement this is to store users who are currently navigating through a route in the database by storing all the tiles they're supposed to go through.</p> <p>Data might look like this: <pre><code>user_1: r_1, r_2, r_3, \u2026, r_k\nuser_2: r_4, r_6, r_9, \u2026, r_n\nuser_3: r_2, r_8, r_9, \u2026, r_m\n...\nuser_n: r_2, r_10, r21, ..., r_l\n</code></pre></p> <p>If a traffic accident happens on some tile, we can identify all users whose path goes through that tile and re-route them.</p> <p>To reduce the amount of tiles we store in the database, we can instead store the origin routing tile and several routing tiles in different resolution levels until the destination tile is also included: <pre><code>user_1, r_1, super(r_1), super(super(r_1)), ...\n</code></pre></p> <p></p> <p>Using this, we only need to check if the final tile of a user includes the traffic accident tile to see if user is impacted.</p> <p>We can also keep track of all possible routes for a navigating user and notify them if a faster re-route is available.</p>"},{"location":"system-design-interview/chapter19/#delivery-protocols","title":"Delivery protocols","text":"<p>We have several options, which enable us to proactively push data to clients from the server:  * Mobile push notifications don't work because payload is limited and it's not available for web apps  * WebSocket is generally a better option than long-polling as it has less compute footprint on servers  * We can also use server-sent events (SSE) but lean towards web sockets as they support bi-directional communication which can come in handy for eg a last-mile delivery feature</p>"},{"location":"system-design-interview/chapter19/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>This is our final design: </p> <p>One additional feature we could provide is multi-stop navigation which can be sold to enterprise customers such as Uber or Lyft in order to determine optimal path for visiting a set of locations.</p>"},{"location":"system-design-interview/chapter20/","title":"Distributed Message Queue","text":"<p>We'll be designing a distributed message queue in this chapter.</p> <p>Benefits of message queues:  * Decoupling - Eliminates tight coupling between components. Let them update separately.  * Improved scalability - Producers and consumers can be scaled independently based on traffic.  * Increased availability - If one part of the system goes down, other parts continue interacting with the queue.  * Better performance - Producers can produce messages without waiting for consumer confirmation.</p> <p>Some popular message queue implementations - Kafka, RabbitMQ, RocketMQ, Apache Pulsar, ActiveMQ, ZeroMQ.</p> <p>Strictly speaking, Kafka and Pulsar are not message queues. They are event streaming platforms. There is however a convergence of features which blurs the distinction between message queues and event streaming platforms.</p> <p>In this chapter, we'll be building a message queue with support for more advanced features such as long data retention, repeated message consumption, etc.</p>"},{"location":"system-design-interview/chapter20/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>Message queues ought to support few basic features - producers produce messages and consumers consume them. There are, however, different considerations with regards to performance, message delivery, data retention, etc.</p> <p>Here's a set of potential questions between Candidate and Interviewer:  * C: What's the format and average message size? Is it text only?  * I: Messages are text-only and usually a few KBs  * C: Can messages be repeatedly consumed?  * I: Yes, messages can be repeatedly consumed by different consumers. This is an added requirement, which traditional message queues don't support.  * C: Are messages consumed in the same order they were produced?  * I: Yes, order guarantee should be preserved. This is an added requirement, traditional message queues don't support this.  * C: What are the data retention requirements?  * I: Messages need to have a retention of two weeks. This is an added requirement.  * C: How many producers and consumers do we want to support?  * I: The more, the better.  * C: What data delivery semantic do we want to support? At-most-once, at-least-once, exactly-once?  * I: We definitely want to support at-least-once. Ideally, we can support all and make them configurable.  * C: What's the target throughput for end-to-end latency?  * I: It should support high throughput for use cases like log aggregation and low throughput for more traditional use cases.</p> <p>Functional requirements:  * Producers send messages to a message queue  * Consumers consume messages from the queue  * Messages can be consumed once or repeatedly  * Historical data can be truncated  * Message size is in the KB range  * Order of messages needs to be preserved  * Data delivery semantics is configurable - at-most-once/at-least-once/exactly-once.</p> <p>Non-functional requirements:  * High throughput or low latency. Configurable based on use-case  * Scalable - system should be distributed and support a sudden surge in message volume  * Persistent and durable - data should be persisted on disk and replicated among nodes</p> <p>Traditional message queues typically don't support data retention and don't provide ordering guarantees. This greatly simplifies the design and we'll discuss it.</p>"},{"location":"system-design-interview/chapter20/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>Key components of a message queue:   * Producer sends messages to a queue  * Consumer subscribes to a queue and consumes the subscribed messages  * Message queue is a service in the middle which decouples producers from consumers, letting them scale independently.  * Producer and consumer are both clients, while the message queue is the server.</p>"},{"location":"system-design-interview/chapter20/#messaging-models","title":"Messaging models","text":"<p>The first type of messaging model is point-to-point and it's commonly found in traditional message queues:   * A message is sent to a queue and it's consumed by exactly one consumer.  * There can be multiple consumers, but a message is consumed only once.  * Once message is acknowledged as consumed, it is removed from the queue.  * There is no data retention in the point-to-point model, but there is such in our design.</p> <p>On the other hand, the publish-subscribe model is more common for event streaming platforms:   * In this model, messages are associated to a topic.  * Consumers are subscribed to a topic and they receive all messages sent to this topic.</p>"},{"location":"system-design-interview/chapter20/#topics-partitions-and-brokers","title":"Topics, partitions and brokers","text":"<p>What if the data volume for a topic is too large? One way to scale is by splitting a topic into partitions (aka sharding):   * Messages sent to a topic are evenly distributed across partitions  * The servers that host partitions are called brokers  * Each topic operates like a queue using FIFO for message processing. Message order is preserved within a partition.  * The position of a message within the partition is called an offset.  * Each message produced is sent to a specific partition. A partition key specifies which partition a message should land in.     * Eg a <code>user_id</code> can be used as a partition key to guarantee order of messages for the same user.  * Each consumer subscribes to one or more partitions. When there are multiple consumers for the same messages, they form a consumer group.</p>"},{"location":"system-design-interview/chapter20/#consumer-groups","title":"Consumer groups","text":"<p>Consumer groups are a set of consumers working together to consume messages from a topic:   * Messages are replicated per consumer group (not per consumer).  * Each consumer group maintains its own offset.  * Reading messages in parallel by a consumer group improves throughput but hampers the ordering guarantee.  * This can be mitigated by only allowing one consumer from a group to be subscribed to a partition.   * This means that we can't have more consumers in a group than there are partitions.</p>"},{"location":"system-design-interview/chapter20/#high-level-architecture","title":"High-level architecture","text":"<p>  * Clients are producer and consumer. Producer pushes messages to a designated topic. Consumer group subscribes to messages from a topic.  * Brokers hold multiple partitions. A partition holds a subset of messages for a topic.  * Data storage stores messages in partitions.  * State storage keeps the consumer states.  * Metadata storage stores configuration and topic properties  * The coordination service is responsible for service discovery (which brokers are alive) and leader election (which broker is leader, responsible for assigning partitions).</p>"},{"location":"system-design-interview/chapter20/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>In order to achieve high throughput and preserve the high data retention requirement, we made some important design choices:  * We chose an on-disk data structure which takes advantage of the properties of modern HDD and disk caching strategies of modern OS-es.  * The message data structure is immutable to avoid extra copying, which we want to avoid in a high volume/high traffic system.  * We designed our writes around batching as small I/O is an enemy of high throughput.</p>"},{"location":"system-design-interview/chapter20/#data-storage","title":"Data storage","text":"<p>In order to find the best data store for messages, we must examine a message's properties:  * Write-heavy, read-heavy  * No update/delete operations. In traditional message queues, there is a \"delete\" operation as messages are not retained.  * Predominantly sequential read/write access pattern.</p> <p>What are our options:  * Database - not ideal as typical databases don't support well both write and read heavy systems.  * Write-ahead log (WAL) - a plain text file which only supports appending to it and is very HDD-friendly.     * We split partitions into segments to avoid maintaining a very large file.    * Old segments are read-only. Writes are accepted by latest segment only. </p> <p>WAL files are extremely efficient when used with traditional HDDs. </p> <p>There is a misconception that HDD acces is slow, but that hugely depends on the access pattern. When the access pattern is sequential (as in our case), HDDs can achieve several MB/s write/read speed which is sufficient for our needs. We also piggyback on the fact that the OS caches disk data in memory aggressively.</p>"},{"location":"system-design-interview/chapter20/#message-data-structure","title":"Message data structure","text":"<p>It is important that the message schema is compliant between producer, queue and consumer to avoid extra copying. This allows much more efficient processing.</p> <p>Example message structure: </p> <p>The key of the message specifies which partition a message belongs to. An example mapping is <code>hash(key) % numPartitions</code>. For more flexibility, the producer can override default keys in order to control which partitions messages are distributed to.</p> <p>The message value is the payload of a message. It can be plaintext or a compressed binary block.</p> <p>Note: Message keys, unlike traditional KV stores, need not be unique. It is acceptable to have duplicate keys and for it to even be missing.</p> <p>Other message files:  * Topic - topic the message belongs to  * Partition - The ID of the partition a message belongs to  * Offset - The position of the message in a partition. A message can be located via <code>topic</code>, <code>partition</code>, <code>offset</code>.  * Timestamp - When the message is stored  * Size - the size of this message  * CRC - checksum to ensure message integrity</p> <p>Additional features such as filtering can be supported by adding additional fields.</p>"},{"location":"system-design-interview/chapter20/#batching","title":"Batching","text":"<p>Batching is critical for the performance of our system. We apply it in the producer, consumer and message queue.</p> <p>It is critical because:  * It allows the operating system to group messages together, amortizing the cost of expensive network round trips  * Messages are written to the WAL in groups sequentially, which leads to a lot of sequential writes and disk caching.</p> <p>There is a trade-off between latency and throughput:  * High batching leads to high throughput and higher latency.   * Less batching leads to lower throughput and lower latency.</p> <p>If we need to support lower latency since the system is deployed as a traditional message queue, the system could be tuned to use a smaller batch size.</p> <p>If tuned for throughput, we might need more partitions per topic to compensate for the slower sequential disk write throughput.</p>"},{"location":"system-design-interview/chapter20/#producer-flow","title":"Producer flow","text":"<p>If a producer wants to send a message to a partition, which broker should it connect to?</p> <p>One option is to introduce a routing layer, which route messages to the correct broker. If replication is enabled, the correct broker is the leader replica:   * Routing layer reads the replication plan from the metadata store and caches it locally.  * Producer sends a message to the routing layer.  * Message is forwarded to broker 1 who is the leader of the given partition  * Follower replicas pull the new message from the leader. Once enough confirmations are received, the leader commits the data and responds to the producer.</p> <p>The reason for having replicas is to enable fault tolerance.</p> <p>This approach works but has some drawbacks:  * Additional network hops due to the extra component  * The design doesn't enable batching messages</p> <p>To mitigate these issues, we can embed the routing layer into the producer:   * Fewer network hops lead to lower latency  * Producers can control which partition a message is routed to  * The buffer allows us to batch messages in-memory and send out larger batches in a single request, which increases throughput.</p> <p>The batch size choice is a classical trade-off between throughput and latency.    * Larger batch size leads to longer wait time before batch is committed.   * Smaller batch size leads to request being sent sooner and having lower latency but lower throughput.</p>"},{"location":"system-design-interview/chapter20/#consumer-flow","title":"Consumer flow","text":"<p>The consumer specifies its offset in a partition and receives a chunk of messages, beginning from that offset: </p> <p>One important consideration when designing the consumer is whether to use a push or a pull model:  * Push model leads to lower latency as broker pushes messages to consumer as it receives them.    * However, if rate of consumption falls behind the rate of production, the consumer can be overwhelmed.    * It is challenging to deal with consumers with varying processing power as the broker controls the rate of consumption.  * Pull model leads to the consumer controlling the consumption rate.     * If rate of consumption is slow, consumer will not be overwhelmed and we can scale it to catch up.    * The pull model is more suitable for batch processing, because with the push model, the broker can't know how many messages a consumer can handle.     * With the pull model, on the other hand, consumers can aggressively fetch large message batches.    * The down side is the higher latency and extra network calls when there are no new messages. Latter issue can be mitigated using long polling.</p> <p>Hence, most message queues (and us) choose the pull model.   * A new consumer subscribes to topic A and joins group 1.  * The correct broker node is found by hashing the group name. This way, all consumers in a group connect to the same broker.  * Note that this consumer group coordinator is different from the coordination service (ZooKeeper).  * Coordinator confirms that the consumer has joined the group and assigns partition 2 to that consumer.  * There are different partition assignment strategies - round-robin, range, etc.  * Consumer fetches latest messages from the last offset. The state storage keeps the consumer offsets.  * Consumer processes messages and commits the offset to the broker. The order of those operations affects the message delivery semantics.</p>"},{"location":"system-design-interview/chapter20/#consumer-rebalancing","title":"Consumer rebalancing","text":"<p>Consumer rebalancing is responsible for deciding which consumers are responsible for which partition.</p> <p>This process occurs when a consumer joins/leaves or a partition is added/removed.</p> <p>The broker, acting as a coordinator plays a huge role in orchestrating the rebalancing workflow.   * All consumers from the same group are connected to the same coordinator. The coordinator is found by hashing the group name.  * When the consumer list changes, the coordinator chooses a new leader of the group.  * The leader of the group calculates a new partition dispatch plan and reports it back to the coordinator, which broadcasts it to the other consumers.</p> <p>When the coordinator stops receiving heartbeats from the consumers in a group, a rebalancing is triggered: </p> <p>Let's explore what happens when a consumer joins a group:   * Initially, only consumer A is in the group and it consumes all partitions.  * Consumer B sends a request to join the group.  * The coordinator notifies all group members that it's time to rebalance passively - as a response to the heartbeat.  * Once all consumers rejoin the group, the coordinator chooses a leader and notifies the rest about the election result.  * The leader generates the partition dispatch plan and sends it to the coordinator. Others wait for the dispatch plan.  * Consumers start consuming from the newly assigned partitions.</p> <p>Here's what happens when a consumer leaves the group:   * Consumer A and B are in the same group  * Consumer B asks to leave the group  * When coordinator receives A's heartbeat, it informs them that it's time to rebalance.  * The rest of the steps are the same.</p> <p>The process is similar when a consumer doesn't send a heartbeat for a long time: </p>"},{"location":"system-design-interview/chapter20/#state-storage","title":"State storage","text":"<p>The state storage stores mapping between partitions and consumers, as well as the last consumed offsets for a partition. </p> <p>Group 1's offset is at 6, meaning all previous messages are consumed. If a consumer crashes, the new consumer will continue from that message on wards.</p> <p>Data access patterns for consumer states:  * Frequent read/write operations, but low volume  * Data is updated frequently, but rarely deleted  * Random read/write  * Data consistency is important</p> <p>Given these requirements, a fast KV storage like Zookeeper is ideal.</p>"},{"location":"system-design-interview/chapter20/#metadata-storage","title":"Metadata storage","text":"<p>The metadata storage stores configuration and topic properties - partition number, retention period, replica distribution.</p> <p>Metadata doesn't change often and volume is small, but there is a high consistency requirement. Zookeeper is a good choice for this storage.</p>"},{"location":"system-design-interview/chapter20/#zookeeper","title":"ZooKeeper","text":"<p>Zookeeper is essential for building distributed message queues.</p> <p>It is a hierarchical key-value store, commonly used for a distributed configuration, synchronization service and naming registry (ie service discovery). </p> <p>With this change, the broker only needs to maintain data for the messages. Metadata and state storage is in Zookeeper.</p> <p>Zookeeper also helps with leader election of the broker replicas.</p>"},{"location":"system-design-interview/chapter20/#replication","title":"Replication","text":"<p>In distributed systems, hardware issues are inevitable. We can tackle this via replication to achieve high availability.   * Each partition is replicated across multiple brokers, but there is only one leader replica.  * Producers send messages to leader replicas  * Followers pull the replicated messages from the leader  * Once enough replicas are synchronized, the leader returns acknowledgment to the producer  * Distribution of replicas for each partition is called the replica distribution plan.  * The leader for a given partition creates the replica distribution plan and saves it in Zookeeper</p>"},{"location":"system-design-interview/chapter20/#in-sync-replicas","title":"In-sync replicas","text":"<p>One problem we need to tackle is keeping messages in-sync between the leader and the followers for a given partition.</p> <p>In-sync replicas (ISR) are replicas for a partition that stay in-sync with the leader.</p> <p>The <code>replica.lag.max.messages</code> defines how many messages can a replica be lagging behind the leader to be considered in-sync.</p> <p>  * Committed offset is 13  * Two new messages are written to the leader, but not committed yet.  * A message is committed once all replicas in the ISR have synchronized that message  * Replica 2 and 3 have fully caught up with leader, hence, they are in ISR  * Replica 4 has lagged behind, hence, is removed from ISR for now</p> <p>ISR reflects a trade-off between performance and durability.  * In order for producers not to lose messages, all replicas should be in sync before sending an acknowledgment  * But a slow replica will cause the whole partition to become unavailable</p> <p>Acknowledgment handling is configurable.</p> <p><code>ACK=all</code> means that all replicas in ISR have to sync a message. Message sending is slow, but message durability is highest. </p> <p><code>ACK=1</code> means that producer receives acknowledgment once leader receives the message. Message sending is fast, but message durability is low. </p> <p><code>ACK=0</code> means that producer sends messages without waiting for any acknowledgment from leader. Message sending is fastest, message durability is lowest. </p> <p>On the consumer side, we can connect all consumers to the leader for a partition and let them read messages from it:  * This makes for the simplest design and easiest operation  * Messages in a partition are sent to only one consumer in a group, which limits the connections to the leader replica  * The number of connections to leader replica is typically not high as long as the topic is not super hot  * We can scale a hot topic by increasing the number of partitions and consumers  * In certain scenarios, it might make sense to let a consumer lead from an ISR, eg if they're located in a separate DC</p> <p>The ISR list is maintained by the leader who tracks the lag between itself and each replica.</p>"},{"location":"system-design-interview/chapter20/#scalability","title":"Scalability","text":"<p>Let's evaluate how we can scale different parts of the system.</p>"},{"location":"system-design-interview/chapter20/#producer","title":"Producer","text":"<p>The producer is much smaller than the consumer. Its scalability can easily be achieved by adding/removing new producer instances.</p>"},{"location":"system-design-interview/chapter20/#consumer","title":"Consumer","text":"<p>Consumer groups are isolated from each other. It is easy to add/remove consumer groups at will.</p> <p>Rebalancing help handle the case when consumers are added/removed from a group gracefully.</p> <p>Consumer groups are rebalancing help us achieve scalability and fault tolerance.</p>"},{"location":"system-design-interview/chapter20/#broker","title":"Broker","text":"<p>How do brokers handle failure?   * Once a broker fails, there are still enough replicas to avoid partition data loss  * A new leader is elected and the broker coordinator redistributes partitions which were at the failed broker to existing replicas  * Existing replicas pick up the new partitions and act as followers until they're caught up with the leader and become ISR</p> <p>Additional considerations to make the broker fault-tolerant:  * The minimum number of ISRs balances latency and safety. You can fine-tune it to meet your needs.  * If all replicas of a partition are in the same node, then it's a waste of resources. Replicas should be across different brokers.  * If all replicas of a partition crash, then the data is lost forever. Spreading replicas across data centers can help, but it adds up a lot of latency. One option is to use data mirroring as a work around.</p> <p>How do we handle redistribution of replicas when a new broker is added?   * We can temporarily allow more replicas than configured, until new broker catches up  * Once it does, we can remove the partition replica which is no longer needed</p>"},{"location":"system-design-interview/chapter20/#partition","title":"Partition","text":"<p>Whenever a new partition is added, the producer is notified and consumer rebalancing is triggered.</p> <p>In terms of data storage, we can only store new messages to the new partition vs. trying to copy all old ones: </p> <p>Decreasing the number of partitions is more involved:   * Once a partition is decommissioned, new messages are only received by remaining partitions  * The decommissioned partition isn't removed immediately as messages can still be consumed from it  * Once a pre-configured retention period passes, do we truncate the data and storage space is freed up  * During the transitional period, producers only send messages to active partitions, but consumers read from all  * Once retention period expires, consumers are rebalanced</p>"},{"location":"system-design-interview/chapter20/#data-delivery-semantics","title":"Data delivery semantics","text":"<p>Let's discuss different delivery semantics.</p>"},{"location":"system-design-interview/chapter20/#at-most-once","title":"At-most once","text":"<p>With this guarantee, messages are delivered not more than once and could not be delivered at all.   * Producer sends a message asynchronously to a topic. If message delivery fails, there is no retry.  * Consumer fetches message and immediately commits offset. If consumer crashes before processing the message, the message will not be processed.</p>"},{"location":"system-design-interview/chapter20/#at-least-once","title":"At-least once","text":"<p>A message can be sent more than once and no message should be left unprocessed.   * Producer sends message with <code>ack=1</code> or <code>ack=all</code>. If there is any issue, it will keep retrying.  * Consumer fetches the message and consumes the offset only after it's done processing it.  * It is possible for a message to be delivered more than once if eg consumer crashes before committing offset but after processing it.  * This is why, this is good for use-cases where data duplication is acceptable or deduplication is possible.</p>"},{"location":"system-design-interview/chapter20/#exactly-once","title":"Exactly once","text":"<p>Extremely costly to implement for the system, albeit it's the friendliest guarantee to users: </p>"},{"location":"system-design-interview/chapter20/#advanced-features","title":"Advanced features","text":"<p>Let's discuss some advanced features, we might discuss in the interview.</p>"},{"location":"system-design-interview/chapter20/#message-filtering","title":"Message filtering","text":"<p>Some consumers might want to only consume messages of a certain type within a partition.</p> <p>This can be achieved by building separate topics for each subset of messages, but this can be costly if systems have too many differing use-cases.  * It is a waste of resources to store the same message on different topics  * Producer is now tightly coupled to consumers as it changes with each new consumer requirement</p> <p>We can resolve this using message filtering.  * A naive approach would be to do the filtering on the consumer-side, but that introduces unnecessary consumer traffic  * Alternatively, messages can have tags attached to them and consumers can specify which tags they're subscribed to  * Filtering could also be done via the message payloads but that can be challenging and unsafe for encrypted/serialized messages  * For more complex mathematical formulaes, the broker could implement a grammar parser or script executor, but that can be heavyweight for the message queue </p>"},{"location":"system-design-interview/chapter20/#delayed-messages-scheduled-messages","title":"Delayed messages &amp; scheduled messages","text":"<p>For some use-cases, we might want to delay or schedule message delivery.  For example, we might submit a payment verification check for 30m from now, which triggers the consumer to see if a payment was successful.</p> <p>This can be achieved by sending messages to temporary storage in the broker and moving the message to the partition at the right time:   * The temporary storage can be one or more special message topics  * The timing function can be achieved using dedicated delay queues or a hierarchical time wheel</p>"},{"location":"system-design-interview/chapter20/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Additional talking points:  * Protocol of communication. Important considerations - support all use-cases and high data volume, as well as verify message integrity. Popular protocols - AMQP and Kafka protocol.  * Retry consumption - if we can't process a message immediately, we could send it to a dedicated retry topic to be attempted later.  * Historical data archive - old messages can be backed up in high-capacity storages such as HDFS or object storage (eg S3).</p>"},{"location":"system-design-interview/chapter21/","title":"Metrics Monitoring and Alerting System","text":"<p>This chapter focuses on designing a highly scalable metrics monitoring and alerting system, which is critical for ensuring high availability and reliability.</p>"},{"location":"system-design-interview/chapter21/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>A metrics monitoring system can mean a lot of different things - eg you don't want to design a logs aggregation system, when the interviewer is interested in infra metrics only.</p> <p>Let's try to understand the problem first:  * C: Who are we building the system for? An in-house monitoring system for a big tech company or a SaaS like DataDog?  * I: We are building for internal use only.  * C: Which metrics do we want to collect?  * I: Operational system metrics - CPU load, Memory, Data disk space. But also high-level metrics like requests per second. Business metrics are not in scope.  * C: What is the scale of the infrastructure we're monitoring?  * I: 100mil daily active users, 1000 server pools, 100 machines per pool  * C: How long should we keep the data?  * I: Let's assume 1y retention.  * C: May we reduce metrics data resolution for long-term storage?  * I: Keep newly received metrics for 7 days. Roll them up to 1m resolution for next 30 days. Further roll them up to 1h resolution after 30 days.  * C: What are the supported alert channels?  * I: Email, phone, PagerDuty or webhooks.  * C: Do we need to collect logs such as error or access logs?  * I: No  * C: Do we need to support distributed system tracing?  * I: No</p>"},{"location":"system-design-interview/chapter21/#high-level-requirements-and-assumptions","title":"High-level requirements and assumptions","text":"<p>The infrastructure being monitored is large-scale:  * 100mil DAU  * 1000 server pools * 100 machines * ~100 metrics per machine -&gt; ~10mil metrics  * 1-year data retention  * Data retention policy - raw for 7d, 1-minute resolution for 30d, 1h resolution for 1y</p> <p>A variety of metrics can be monitored:  * CPU load  * Request count  * Memory usage  * Message count in message queues</p>"},{"location":"system-design-interview/chapter21/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Scalability - System should be scalable to accommodate more metrics and alerts</li> <li>Low latency - System needs to have low query latency for dashboards and alerts</li> <li>Reliability - System should be highly reliable to avoid missing critical alerts</li> <li>Flexibility - System should be able to easily integrate new technologies in the future</li> </ul> <p>What requirements are out of scope?  * Log monitoring - the ELK stack is very popular for this use-case  * Distributed system tracing - this refers to collecting data about a request lifecycle as it flows through multiple services within the system</p>"},{"location":"system-design-interview/chapter21/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"system-design-interview/chapter21/#fundamentals","title":"Fundamentals","text":"<p>There are five core components involved in a metrics monitoring and alerting system:   * Data collection - collect metrics data from different sources  * Data transmission - transfer data from sources to the metrics monitoring system  * Data storage - organize and store incoming data  * Alerting - Analyze incoming data, detect anomalies and generate alerts  * Visualization - Present data in graphs, charts, etc</p>"},{"location":"system-design-interview/chapter21/#data-model","title":"Data model","text":"<p>Metrics data is usually recorded as a time-series, which contains a set of values with timestamps. The series can be identified by name and an optional set of tags.</p> <p>Example 1 - What is the CPU load on production server instance i631 at 20:00? </p> <p>The data can be identified by the following table: </p> <p>The time series is identified by the metric name, labels and a single point in at a specific time.</p> <p>Example 2 - What is the average CPU load across all web servers in the us-west region for the last 10min? <pre><code>CPU.load host=webserver01,region=us-west 1613707265 50\n\nCPU.load host=webserver01,region=us-west 1613707265 62\n\nCPU.load host=webserver02,region=us-west 1613707265 43\n\nCPU.load host=webserver02,region=us-west 1613707265 53\n\n...\n\nCPU.load host=webserver01,region=us-west 1613707265 76\n\nCPU.load host=webserver01,region=us-west 1613707265 83\n</code></pre></p> <p>This is an example data we might pull from storage to answer that question. The average CPU load can be calculated by averaging the values in the last column of the rows.</p> <p>The format shown above is called the line protocol and is used by many popular monitoring software in the market - eg Prometheus, OpenTSDB.</p> <p>What every time series consists of: </p> <p>A good way to visualize how data looks like:   * The x axis is the time  * the y axis is the dimension you're querying - eg metric name, tag, etc.</p> <p>The data access pattern is write-heavy and spiky reads as we collect a lot of metrics, but they are infrequently accessed, although in bursts when eg there are ongoing incidents.</p> <p>The data storage system is the heart of this design.   * It is not recommended to use a general-purpose database for this problem, although you could achieve good scale \\w expert-level tuning.  * Using a NoSQL database can work in theory, but it is hard to devise a scalable schema for effectively storing and querying time-series data.</p> <p>There are many databases, specifically tailored for storing time-series data. Many of them support custom query interfaces which allow for effective querying of time-series data.  * OpenTSDB is a distributed time-series database, but it is based on Hadoop and HBase. If you don't have that infrastructure provisioned, it would be hard to use this tech.  * Twitter uses MetricsDB, while Amazon offers Timestream.  * The two most popular time-series databases are InfluxDB and Prometheus.   * They are designed to store large volumes of time-series data. Both of them are based on in-memory cache + on-disk storage.</p> <p>Example scale of InfluxDB - more than 250k writes per second when provisioned with 8 cores and 32gb RAM: </p> <p>It is not expected for you to understand the internals of a metrics database as it is niche knowledge. You might be asked only if you've mentioned it on your resume.</p> <p>For the purposes of the interview, it is sufficient to understand that metrics are time-series data and to be aware of popular time-series databases, like InfluxDB.</p> <p>One nice feature of time-series databases is the efficient aggregation and analysis of large amounts of time-series data by labels. InfluxDB, for example, builds indexes for each label.</p> <p>It is critical, however, to keep the cardinality of labels low - ie, not using too many unique labels.</p>"},{"location":"system-design-interview/chapter21/#high-level-design","title":"High-level Design","text":"<p>  * Metrics source - can be application servers, SQL databases, message queues, etc.  * Metrics collector - Gathers metrics data and writes to time-series database  * Time-series database - stores metrics as time-series. Provides a custom query interface for analyzing large amounts of metrics.  * Query service - Makes it easy to query and retrieve data from the time-series DB. Could be replaced entirely by the DB's interface if it's sufficiently powerful.  * Alerting system - Sends alert notifications to various alerting destinations.  * Visualization system - Shows metrics in the form of graphs/charts.</p>"},{"location":"system-design-interview/chapter21/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's deep dive into several of the more interesting parts of the system.</p>"},{"location":"system-design-interview/chapter21/#metrics-collection","title":"Metrics collection","text":"<p>For metrics collection, occasional data loss is not critical. It's acceptable for clients to fire and forget. </p> <p>There are two ways to implement metrics collection - pull or push.</p> <p>Here's how the pull model might look like: </p> <p>For this solution, the metrics collector needs to maintain an up-to-date list of services and metrics endpoints. We can use Zookeeper or etcd for that purpose - service discovery.</p> <p>Service discovery contains contains configuration rules about when and where to collect metrics from: </p> <p>Here's a detailed explanation of the metrics collection flow:   * Metrics collector fetches configuration metadata from service discovery. This includes pulling interval, IP addresses, timeout &amp; retry params.  * Metrics collector pulls metrics data via a pre-defined http endpoint (eg <code>/metrics</code>). This is typically done by a client library.  * Alternatively, the metrics collector can register a change event notification with the service discovery to be notified once the service endpoint changes.  * Another option is for the metrics collector to periodically poll for metrics endpoint configuration changes.</p> <p>At our scale, a single metrics collector is not enough. There must be multiple instances.  However, there must also be some kind of synchronization among them so that two collectors don't collect the same metrics twice.</p> <p>One solution for this is to position collectors and servers on a consistent hash ring and associate a set of servers with a single collector only: </p> <p>With the push model, on the other hand, services push their metrics to the metrics collector proactively: </p> <p>In this approach, typically a collection agent is installed alongside service instances.  The agent collects metrics from the server and pushes them to the metrics collector. </p> <p>With this model, we can potentially aggregate metrics before sending them to the collector, which reduces the volume of data processed by the collector.</p> <p>On the flip side, metrics collector can reject push requests as it can't handle the load.  It is important, hence, to add the collector to an auto-scaling group behind a load balancer.</p> <p>so which one is better? There are trade-offs between both approaches and different systems use different approaches:  * Prometheus uses a pull architecture  * Amazon Cloud Watch and Graphite use a push architecture</p> <p>Here are some of the main differences between push and pull: |                                        | Pull                                                                                                                                                                                                    | Push                                                                                                                                                                                                                                    | |----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Easy debugging                         | The /metrics endpoint on application servers used for pulling metrics can be used to view metrics at any time. You can even do this on your laptop. Pull wins.                                          | If the metrics collector doesn\u2019t receive metrics, the problem might be caused by network issues.                                                                                                                                        | | Health check                           | If an application server doesn\u2019t respond to the pull, you can quickly figure out if an application server is down. Pull wins.                                                                           | If the metrics collector doesn\u2019t receive metrics, the problem might be caused by network issues.                                                                                                                                        | | Short-lived jobs                       |                                                                                                                                                                                                         | Some of the batch jobs might be short-lived and don\u2019t last long enough to be pulled. Push wins. This can be fixed by introducing push gateways for the pull model [22].                                                                 | | Firewall or complicated network setups | Having servers pulling metrics requires all metric endpoints to be reachable. This is potentially problematic in multiple data center setups. It might require a more elaborate network infrastructure. | If the metrics collector is set up with a load balancer and an auto-scaling group, it is possible to receive data from anywhere. Push wins.                                                                                             | | Performance                            | Pull methods typically use TCP.                                                                                                                                                                         | Push methods typically use UDP. This means the push method provides lower-latency transports of metrics. The counterargument here is that the effort of establishing a TCP connection is small compared to sending the metrics payload. | | Data authenticity                      | Application servers to collect metrics from are defined in config files in advance. Metrics gathered from those servers are guaranteed to be authentic.                                                 | Any kind of client can push metrics to the metrics collector. This can be fixed by whitelisting servers from which to accept metrics, or by requiring authentication.                                                                   |</p> <p>There is no clear winner. A large organization probably needs to support both. There might not be a way to install a push agent in the first place.</p>"},{"location":"system-design-interview/chapter21/#scale-the-metrics-transmission-pipeline","title":"Scale the metrics transmission pipeline","text":"<p>The metrics collector is provisioned in an auto-scaling group, regardless if we use the push or pull model.</p> <p>There is a chance of data loss if the time-series DB is down, however. To mitigate this, we'll provision a queuing mechanism:   * Metrics collectors push metrics data into kafka  * Consumers or stream processing services such as Apache Storm, Flink or Spark process the data and push it to the time-series DB</p> <p>This approach has several advantages:  * Kafka is used as a highly-reliable and scalable distributed message platform  * It decouples data collection and data processing from one another  * It can prevent data loss by retaining the data in Kafka</p> <p>Kafka can be configured with one partition per metric name, so that consumers can aggregate data by metric names. To scale this, we can further partition by tags/labels and categorize/prioritize metrics to be collected first. </p> <p>The main downside of using Kafka for this problem is the maintenance/operation overhead. An alternative is to use a large-scale ingestion system like Gorilla. It can be argued that using that would be as scalable as using Kafka for queuing.</p>"},{"location":"system-design-interview/chapter21/#where-aggregations-can-happen","title":"Where aggregations can happen","text":"<p>Metrics can be aggregated at several places. There are trade-offs between different choices:  * Collection agent - client-side collection agent only supports simple aggregation logic. Eg collect a counter for 1m and send it to the metrics collector.  * Ingestion pipeline - To aggregate data before writing to the DB, we need a stream processing engine like Flink. This reduces write volume, but we lose data precision as we don't store raw data.  * Query side - We can aggregate data when we run queries via our visualization system. There is no data loss, but queries can be slow due to a lot of data processing.</p>"},{"location":"system-design-interview/chapter21/#query-service","title":"Query Service","text":"<p>Having a separate query service from the time-series DB decouples the visualization and alerting system from the database, which enables us to decouple the DB from clients and change it at will.</p> <p>We can add a Cache layer here to reduce the load to the time-series database: </p> <p>We can also avoid adding a query service altogether as most visualization and alerting systems have powerful plugins to integrate with most time-series databases. With a well-chosen time-series DB, we might not need to introduce our own caching layer as well.</p> <p>Most time-series DBs don't support SQL simply because it is ineffective for querying time-series data. Here's an example SQL query for computing an exponential moving average: <pre><code>select id,\n       temp,\n       avg(temp) over (partition by group_nr order by time_read) as rolling_avg\nfrom (\n  select id,\n         temp,\n         time_read,\n         interval_group,\n         id - row_number() over (partition by interval_group order by time_read) as group_nr\n  from (\n    select id,\n    time_read,\n    \"epoch\"::timestamp + \"900 seconds\"::interval * (extract(epoch from time_read)::int4 / 900) as interval_group,\n    temp\n    from readings\n  ) t1\n) t2\norder by time_read;\n</code></pre></p> <p>Here's the same query in Flux - query language used in InfluxDB: <pre><code>from(db:\"telegraf\")\n  |&gt; range(start:-1h)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"foo\")\n  |&gt; exponentialMovingAverage(size:-10s)\n</code></pre></p>"},{"location":"system-design-interview/chapter21/#storage-layer","title":"Storage layer","text":"<p>It is important to choose the time-series database carefully.</p> <p>According to research published by Facebook, ~85% of queries to the operational store were for data from the past 26h.</p> <p>If we choose a database, which harnesses this property, it could have significant impact on system performance. InfluxDB is one such option.</p> <p>Regardless of the database we choose, there are some optimizations we might employ.</p> <p>Data encoding and compression can significantly reduce the size of data. Those features are usually built into a good time-series database. </p> <p>In the above example, instead of storing full timestamps, we can store timestamp deltas.</p> <p>Another technique we can employ is down-sampling - converting high-resolution data to low-resolution in order to reduce disk usage.</p> <p>We can use that for old data and make the rules configurable by data scientists, eg:  * 7d - no down-sampling  * 30d - down-sample to 1min  * 1y - down-sample to 1h</p> <p>For example, here's a 10-second resolution metrics table: | metric | timestamp            | hostname | Metric_value | |--------|----------------------|----------|--------------| | cpu    | 2021-10-24T19:00:00Z | host-a   | 10           | | cpu    | 2021-10-24T19:00:10Z | host-a   | 16           | | cpu    | 2021-10-24T19:00:20Z | host-a   | 20           | | cpu    | 2021-10-24T19:00:30Z | host-a   | 30           | | cpu    | 2021-10-24T19:00:40Z | host-a   | 20           | | cpu    | 2021-10-24T19:00:50Z | host-a   | 30           |</p> <p>down-sampled to 30-second resolution: | metric | timestamp            | hostname | Metric_value (avg) | |--------|----------------------|----------|--------------------| | cpu    | 2021-10-24T19:00:00Z | host-a   | 19                 | | cpu    | 2021-10-24T19:00:30Z | host-a   | 25                 |</p> <p>Finally, we can also use cold storage to use old data, which is no longer used. The financial cost for cold storage is much lower.</p>"},{"location":"system-design-interview/chapter21/#alerting-system","title":"Alerting system","text":"<p>Configuration is loaded to cache servers. Rules are typically defined in YAML format. Here's an example: <pre><code>- name: instance_down\n  rules:\n\n  # Alert for any instance that is unreachable for &gt;5 minutes.\n  - alert: instance_down\n    expr: up == 0\n    for: 5m\n    labels:\n      severity: page\n</code></pre></p> <p>The alert manager fetches alert configurations from cache. Based on configuration rules, it also calls the query service at a predefined interval. If a rule is met, an alert event is created.</p> <p>Other responsibilities of the alert manager are:  * Filtering, merging and deduplicating alerts. Eg if an alert of a single instance is triggered multiple times, only one alert event is generated.  * Access control - it is important to restrict alert-management operations to certain individuals only  * Retry - the manager ensures that the alert is propagated at least once.</p> <p>The alert store is a key-value database, like Cassandra, which keeps the state of all alerts. It ensures a notification is sent at least once. Once an alert is triggered, it is published to Kafka.</p> <p>Finally, alert consumers pull alerts data from Kafka and send notifications over to different channels - Email, text message, PagerDuty, webhooks.</p> <p>In the real-world, there are many off-the-shelf solutions for alerting systems. It is difficult to justify building your own system in-house.</p>"},{"location":"system-design-interview/chapter21/#visualization-system","title":"Visualization system","text":"<p>The visualization system shows metrics and alerts over a time period. Here's an dashboard built with Grafana: </p> <p>A high-quality visualization system is very hard to build. It is hard to justify not using an off-the-shelf solution like Grafana.</p>"},{"location":"system-design-interview/chapter21/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Here's our final design: </p>"},{"location":"system-design-interview/chapter22/","title":"Ad Click Event Aggregation","text":"<p>Digital advertising is a big industry with the rise of Facebook, YouTube, TikTok, etc.</p> <p>Hence, tracking ad click events is important. In this chapter, we explore how to design an ad click event aggregation system at Facebook/Google scale.</p> <p>Digital advertising has a process called real-time bidding (RTB), where digital advertising inventory is bought and sold: </p> <p>Speed of RTB is important as it usually occurs within a second. Data accuracy is also very important as it impacts how much money advertisers pay.</p> <p>Based on ad click event aggregations, advertisers can make decisions such as adjust target audience and keywords.</p>"},{"location":"system-design-interview/chapter22/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: What is the format of the input data?</li> <li>I: 1bil ad clicks per day and 2mil ads in total. Number of ad-click events grows 30% year-over-year.</li> <li>C: What are some of the most important queries our system needs to support?</li> <li>I: Top queries to take into consideration:</li> <li>Return number of click events for ad X in last Y minutes</li> <li>Return top 100 most clicked ads in the past 1min. Both parameters should be configurable. Aggregation occurs every minute.</li> <li>Support data filtering by <code>ip</code>, <code>user_id</code>, <code>country</code> for the above queries</li> <li>C: Do we need to worry about edge cases? Some of the ones I can think of:</li> <li>There might be events that arrive later than expected</li> <li>There might be duplicate events</li> <li>Different parts of the system might be down, so we need to consider system recovery</li> <li>I: That's a good list, take those into consideration</li> <li>C: What is the latency requirement?</li> <li>I: A few minutes of e2e latency for ad click aggregation. For RTB, it is less than a second. It is ok to have that latency for ad click aggregation as those are usually used for billing and reporting.</li> </ul>"},{"location":"system-design-interview/chapter22/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Aggregate the number of clicks of <code>ad_id</code> in the last Y minutes</li> <li>Return top 100 most clicked <code>ad_id</code> every minute</li> <li>Support aggregation filtering by different attributes</li> <li>Dataset volume is at Facebook or Google scale</li> </ul>"},{"location":"system-design-interview/chapter22/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Correctness of the aggregation result is important as it's used for RTB and ads billing</li> <li>Properly handle delayed or duplicate events</li> <li>Robustness - system should be resilient to partial failures</li> <li>Latency - a few minutes of e2e latency at most</li> </ul>"},{"location":"system-design-interview/chapter22/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>1bil DAU</li> <li>Assuming user clicks 1 ad per day -&gt; 1bil ad clicks per day</li> <li>Ad click QPS = 10,000</li> <li>Peak QPS is 5 times the number = 50,000</li> <li>A single ad click occupies 0.1KB storage. Daily storage requirement is 100gb</li> <li>Monthly storage = 3tb</li> </ul>"},{"location":"system-design-interview/chapter22/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>In this section, we discuss query API design, data model and high-level design.</p>"},{"location":"system-design-interview/chapter22/#query-api-design","title":"Query API Design","text":"<p>The API is a contract between the client and the server. In our case, the client is the dashboard user - data scientist/analyst, advertiser, etc.</p> <p>Here's our functional requirements:  * Aggregate the number of clicks of <code>ad_id</code> in the last Y minutes  * Return top N most clicked <code>ad_id</code> in the last M minutes  * Support aggregation filtering by different attributes</p> <p>We need two endpoints to achieve those requirements. Filtering can be done via query parameters on one of them.</p> <p>Aggregate number of clicks of ad_id in the last M minutes: <pre><code>GET /v1/ads/{:ad_id}/aggregated_count\n</code></pre></p> <p>Query parameters:  * from - start minute. Default is now - 1 min  * to - end minute. Default is now  * filter - identifier for different filtering strategies. Eg 001 means \"non-US clicks\".</p> <p>Response:  * ad_id - ad identifier  * count - aggregated count between start and end minutes</p> <p>Return top N most clicked ad_ids in the last M minutes <pre><code>GET /v1/ads/popular_ads\n</code></pre></p> <p>Query parameters:  * count - top N most clicked ads  * window - aggregation window size in minutes  * filter - identifier for different filtering strategies</p> <p>Response:  * list of ad_ids</p>"},{"location":"system-design-interview/chapter22/#data-model","title":"Data model","text":"<p>In our system, we have raw and aggregated data.</p> <p>Raw data looks like this: <pre><code>[AdClickEvent] ad001, 2021-01-01 00:00:01, user 1, 207.148.22.22, USA\n</code></pre></p> <p>Here's an example in a structured format: | ad_id | click_timestamp     | user  | ip            | country | |-------|---------------------|-------|---------------|---------| | ad001 | 2021-01-01 00:00:01 | user1 | 207.148.22.22 | USA     | | ad001 | 2021-01-01 00:00:02 | user1 | 207.148.22.22 | USA     | | ad002 | 2021-01-01 00:00:02 | user2 | 209.153.56.11 | USA     |</p> <p>Here's the aggregated version: | ad_id | click_minute | filter_id | count | |-------|--------------|-----------|-------| | ad001 | 202101010000 | 0012      | 2     | | ad001 | 202101010000 | 0023      | 3     | | ad001 | 202101010001 | 0012      | 1     | | ad001 | 202101010001 | 0023      | 6     |</p> <p>The <code>filter_id</code> helps us achieve our filtering requirements. | filter_id | region | IP        | user_id | |-----------|--------|-----------|---------| | 0012      | US     | *         | *       | | 0013      | *      | 123.1.2.3 | *       |</p> <p>To support quickly returning top N most clicked ads in the last M minutes, we'll also maintain this structure: | most_clicked_ads   |           |                                                  | |--------------------|-----------|--------------------------------------------------| | window_size        | integer   | The aggregation window size (M) in minutes       | | update_time_minute | timestamp | Last updated timestamp (in 1-minute granularity) | | most_clicked_ads   | array     | List of ad IDs in JSON format.                   |</p> <p>What are some pros and cons between storing raw data and storing aggregated data?  * Raw data enables using the full data set and supports data filtering and recalculation  * On the other hand, aggregated data allows us to have a smaller data set and a faster query  * Raw data means having a larger data store and a slower query  * Aggregated data, however, is derived data, hence there is some data loss.</p> <p>In our design, we'll use a combination of both approaches:  * It's a good idea to keep the raw data around for debugging. If there is some bug in aggregation, we can discover the bug and backfill.  * Aggregated data should be stored as well for faster query performance.  * Raw data can be stored in cold storage to avoid extra storage costs.</p> <p>When it comes to the database, there are several factors to take into consideration:  * What does the data look like? Is it relational, document or blob?  * Is the workload read-heavy, write-heavy or both?  * Are transactions needed?  * Do the queries rely on OLAP functions like SUM and COUNT?</p> <p>For the raw data, we can see that the average QPS is 10k and peak QPS is 50k, so the system is write-heavy. On the other hand, read traffic is low as raw data is mostly used as backup if anything goes wrong.</p> <p>Relational databases can do the job, but it can be challenging to scale the writes.  Alternatively, we can use Cassandra or InfluxDB which have better native support for heavy write loads.</p> <p>Another option is to use Amazon S3 with a columnar data format like ORC, Parquet or AVRO. Since this setup is unfamiliar, we'll stick to Cassandra.</p> <p>For aggregated data, the workload is both read and write heavy as aggregated data is constantly queried for dashboards and alerts. It is also write-heavy as data is aggregated and written every minute by the aggregation service.  Hence, we'll use the same data store (Cassandra) here as well.</p>"},{"location":"system-design-interview/chapter22/#high-level-design","title":"High-level design","text":"<p>Here's how our system looks like: </p> <p>Data flows as an unbounded data stream on both inputs and outputs.</p> <p>In order to avoid having a synchronous sink, where a consumer crashing can cause the whole system to stall,  we'll leverage asynchronous processing using message queues (Kafka) to decouple consumers and producers. </p> <p>The first message queue stores ad click event data: | ad_id | click_timestamp | user_id | ip | country | |-------|-----------------|---------|----|---------|</p> <p>The second message queue contains ad click counts, aggregated per-minute: | ad_id | click_minute | count | |-------|--------------|-------|</p> <p>As well as top N clicked ads aggregated per minute: | update_time_minute | most_clicked_ads | |--------------------|------------------|</p> <p>The second message queue is there in order to achieve end to end exactly-once atomic commit semantics: </p> <p>For the aggregation service, using the MapReduce framework is a good option:  </p> <p>Each node is responsible for one single task and it sends the processing result to the downstream node.</p> <p>The map node is responsible for reading from the data source, then filtering and transforming the data.</p> <p>For example, the map node can allocate data across different aggregation nodes based on the <code>ad_id</code>: </p> <p>Alternatively, we can distribute ads across Kafka partitions and let the aggregation nodes subscribe directly within a consumer group. However, the mapping node enables us to sanitize or transform the data before subsequent processing.</p> <p>Another reason might be that we don't have control over how data is produced,  so events related to the same <code>ad_id</code> might go on different partitions.</p> <p>The aggregate node counts ad click events by <code>ad_id</code> in-memory every minute.</p> <p>The reduce node collects aggregated results from aggregate node and produces the final result: </p> <p>This DAG model uses the MapReduce paradigm. It takes big data and leverages parallel distributed computing to turn it into regular-sized data.</p> <p>In the DAG model, intermediate data is stored in-memory and different nodes communicate with each other using TCP or shared memory.</p> <p>Let's explore how this model can now help us to achieve our various use-cases.</p> <p>Use-case 1 - aggregate the number of clicks:   * Ads are partitioned using <code>ad_id % 3</code></p> <p>Use-case 2 - return top N most clicked ads:   * In this case, we're aggregating the top 3 ads, but this can be extended to top N ads easily  * Each node maintains a heap data structure for fast retrieval of top N ads</p> <p>Use-case 3 - data filtering: To support fast data filtering, we can predefine filtering criterias and pre-aggregate based on it: | ad_id | click_minute | country | count | |-------|--------------|---------|-------| | ad001 | 202101010001 | USA     | 100   | | ad001 | 202101010001 | GPB     | 200   | | ad001 | 202101010001 | others  | 3000  | | ad002 | 202101010001 | USA     | 10    | | ad002 | 202101010001 | GPB     | 25    | | ad002 | 202101010001 | others  | 12    |</p> <p>This technique is called the star schema and is widely used in data warehouses. The filtering fields are called dimensions.</p> <p>This approach has the following benefits:  * Simple to undertand and build  * Current aggregation service can be reused to create more dimensions in the star schema.  * Accessing data based on filtering criteria is fast as results are pre-calculated</p> <p>A limitation of this approach is that it creates many more buckets and records, especially when we have lots of filtering criterias.</p>"},{"location":"system-design-interview/chapter22/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's dive deeper into some of the more interesting topics.</p>"},{"location":"system-design-interview/chapter22/#streaming-vs-batching","title":"Streaming vs. Batching","text":"<p>The high-level architecture we proposed is a type of stream processing system.  Here's a comparison between three types of systems: |                         | Services (Online system)      | Batch system (offline system)                          | Streaming system (near real-time system)     | |-------------------------|-------------------------------|--------------------------------------------------------|----------------------------------------------| | Responsiveness          | Respond to the client quickly | No response to the client needed                       | No response to the client needed             | | Input                   | User requests                 | Bounded input with finite size. A large amount of data | Input has no boundary (infinite streams)     | | Output                  | Responses to clients          | Materialized views, aggregated metrics, etc.           | Materialized views, aggregated metrics, etc. | | Performance measurement | Availability, latency         | Throughput                                             | Throughput, latency                          | | Example                 | Online shopping               | MapReduce                                              | Flink [13]                                   |</p> <p>In our design, we used a mixture of batching and streaming. </p> <p>We used streaming for processing data as it arrives and generates aggregated results in near real-time. We used batching, on the other hand, for historical data backup.</p> <p>A system which contains two processing paths - batch and streaming, simultaneously, this architecture is called lambda. A disadvantage is that you have two processing paths with two different codebases to maintain.</p> <p>Kappa is an alternative architecture, which combines batch and stream processing in one processing path. The key idea is to use a single stream processing engine.</p> <p>Lambda architecture: </p> <p>Kappa architecture: </p> <p>Our high-level design uses Kappa architecture as reprocessing of historical data also goes through the aggregation service.</p> <p>Whenever we have to recalculate aggregated data due to eg a major bug in aggregation logic, we can recalculate the aggregation from the raw data we store.  * Recalculation service retrieves data from raw storage. This is a batch job.  * Retrieved data is sent to a dedicated aggregation service, so that the real-time processing aggregation service is not impacted.  * Aggregated results are sent to the second message queue, after which we update the results in the aggregation database. </p>"},{"location":"system-design-interview/chapter22/#time","title":"Time","text":"<p>We need a timestamp to perform aggregation. It can be generated in two places:  * event time - when ad click occurs  * Processing time - system time when the server processes the event</p> <p>Due to the usage of async processing (message queues) and network delays, there can be significant difference between event time and processing time.  * If we use processing time, aggregation results can be inaccurate  * If we use event time, we have to deal with delayed events</p> <p>There is no perfect solution, we need to consider trade-offs: |                 | Pros                                  | Cons                                                                                 | |-----------------|---------------------------------------|--------------------------------------------------------------------------------------| | Event time      | Aggregation results are more accurate | Clients might have the wrong time or timestamp might be generated by malicious users | | Processing time | Server timestamp is more reliable     | The timestamp is not accurate if event is late                                       |</p> <p>Since data accuracy is important, we'll use the event time for aggregation.</p> <p>To mitigate the issue of delayed events, a technique called \"watermark\" can be leveraged.</p> <p>In the example below, event 2 misses the window where it needs to be aggregated: </p> <p>However, if we purposefully extend the aggregation window, we can reduce the likelihood of missed events. The extended part of a window is called a \"watermark\":   * Short watermark increases likelihood of missed events, but reduces latency  * Longer watermark reduces likelihood of missed events, but increases latency</p> <p>There is always likelihood of missed events, regardless of the watermark's size. But there is no use in optimizing for such low-probability events.</p> <p>We can instead resolve such inconsistencies by doing end-of-day reconciliation.</p>"},{"location":"system-design-interview/chapter22/#aggregation-window","title":"Aggregation window","text":"<p>There are four types of window functions:  * Tumbling (fixed) window  * Hopping window  * Sliding window  * Session window</p> <p>In our design, we leverage a tumbling window for ad click aggregations: </p> <p>As well as a sliding window for the top N clicked ads in M minutes aggregation: </p>"},{"location":"system-design-interview/chapter22/#delivery-guarantees","title":"Delivery guarantees","text":"<p>Since the data we're aggregating is going to be used for billing, data accuracy is a priority.</p> <p>Hence, we need to discuss:  * How to avoid processing duplicate events  * How to ensure all events are processed</p> <p>There are three delivery guarantees we can use - at-most-once, at-least-once and exactly once.</p> <p>In most circumstances, at-least-once is sufficient when a small amount of duplicates is acceptable. This is not the case for our system, though, as a difference in small percent can result in millions of dollars of discrepancy. Hence, we'll need to use exactly-once delivery semantics.</p>"},{"location":"system-design-interview/chapter22/#data-deduplication","title":"Data deduplication","text":"<p>One of the most common data quality issues is duplicated data.</p> <p>It can come from a wide range of sources:  * Client-side - a client might resend the same event multiple times. Duplicated events sent with malicious intent are best handled by a risk engine.  * Server outage - An aggregation service node goes down in the middle of aggregation and the upstream service hasn't received an acknowledgment so event is resent.</p> <p>Here's an example of data duplication occurring due to failure to acknowledge an event on the last hop: </p> <p>In this example, offset 100 will be processed and sent downstream multiple times.</p> <p>One option to try and mitigate this is to store the last seen offset in HDFS/S3, but this risks the result never reaching downstream: </p> <p>Finally, we can store the offset while interacting with downstream atomically. To achieve this, we need to implement a distributed transaction: </p> <p>Personal side-note: Alternatively, if the downstream system handles the aggregation result idempotently, there is no need for a distributed transaction.</p>"},{"location":"system-design-interview/chapter22/#scale-the-system","title":"Scale the system","text":"<p>Let's discuss how we scale the system as it grows.</p> <p>We have three independent components - message queue, aggregation service and database. Since they are decoupled, we can scale them independently.</p> <p>How do we scale the message queue:  * We don't put a limit on producers, so they can be scaled easily  * Consumers can be scaled by assigning them to consumer groups and increasing the number of consumers.  * For this to work, we also need to ensure there are enough partitions created preemptively  * Also, consumer rebalancing can take a while when there are thousands of consumers so it is recommended to do it off peak hours  * We could also consider partitioning the topic by geography, eg <code>topic_na</code>, <code>topic_eu</code>, etc. </p> <p>How do we scale the aggregation service:   * The map-reduce nodes can easily be scaled by adding more nodes  * The throughput of the aggregation service can be scaled by by utilising multi-threading  * Alternatively, we can leverage resource providers such as Apache YARN to utilize multi-processing  * Option 1 is easier, but option 2 is more widely used in practice as it's more scalable  * Here's the multi-threading example: </p> <p>How do we scale the database:  * If we use Cassandra, it natively supports horizontal scaling utilizing consistent hashing  * If a new node is added to the cluster, data automatically gets rebalanced across all (virtual) nodes  * With this approach, no manual (re)sharding is required </p> <p>Another scalability issue to consider is the hotspot issue - what if an ad is more popular and gets more attention than others?   * In the above example, aggregation service nodes can apply for extra resources via the resource manager  * The resource manager allocates more resources, so the original node isn't overloaded  * The original node splits the events into 3 groups and each of the aggregation nodes handles 100 events  * Result is written back to the original aggregation node</p> <p>Alternative, more sophisticated ways to handle the hotspot problem:  * Global-Local Aggregation  * Split Distinct Aggregation</p>"},{"location":"system-design-interview/chapter22/#fault-tolerance","title":"Fault Tolerance","text":"<p>Within the aggregation nodes, we are processing data in-memory. If a node goes down, the processed data is lost.</p> <p>We can leverage consumer offsets in kafka to continue from where we left off once another node picks up the slack. However, there is additional intermediary state we need to maintain, as we're aggregating the top N ads in M minutes.</p> <p>We can make snapshots at a particular minute for the on-going aggregation: </p> <p>If a node goes down, the new node can read the latest committed consumer offset, as well as the latest snapshot to continue the job: </p>"},{"location":"system-design-interview/chapter22/#data-monitoring-and-correctness","title":"Data monitoring and correctness","text":"<p>As the data we're aggregating is critical as it's used for billing, it is very important to have rigorous monitoring in place in order to ensure correctness.</p> <p>Some metrics we might want to monitor:  * Latency - Timestamps of different events can be tracked in order to understand the e2e latency of the system  * Message queue size - If there is a sudden increase in queue size, we need to add more aggregation nodes. As Kafka is implemented via a distributed commit log, we need to keep track of records-lag metrics instead.  * System resources on aggregation nodes - CPU, disk, JVM, etc.</p> <p>We also need to implement a reconciliation flow which is a batch job, running at the end of the day.  It calculates the aggregated results from the raw data and compares them against the actual data stored in the aggregation database: </p>"},{"location":"system-design-interview/chapter22/#alternative-design","title":"Alternative design","text":"<p>In a generalist system design interview, you are not expected to know the internals of specialized software used in big data processing.</p> <p>Explaining the thought process and discussing trade-offs is more important than knowing specific tools, which is why the chapter covers a generic solution.</p> <p>An alternative design, which leverages off-the-shelf tooling, is to store ad click data in Hive with an ElasticSearch layer on top built for faster queries.</p> <p>Aggregation is typically done in OLAP databases such as ClickHouse or Druid. </p>"},{"location":"system-design-interview/chapter22/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Things we covered:  * Data model and API Design  * Using MapReduce to aggregate ad click events  * Scaling the message queue, aggregation service and database  * Mitigating the hotspot issue  * Monitoring the system continuously  * Using reconciliation to ensure correctness  * Fault tolerance</p> <p>The ad click event aggregation is a typical big data processing system.</p> <p>It would be easier to understand and design it if you have prior knowledge of related technologies:  * Apache Kafka  * Apache Spark  * Apache Flink</p>"},{"location":"system-design-interview/chapter23/","title":"Hotel Reservation System","text":"<p>In this chapter, we're designing a hotel reservation system, similar to Marriott International.</p> <p>Applicable to other types of systems as well - Airbnb, flight reservation, movie ticket booking.</p>"},{"location":"system-design-interview/chapter23/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>Before diving into designing the system, we should ask the interviewer questions to clarify the scope:  * C: What is the scale of the system?  * I: We're building a website for a hotel chain \\w 5000 hotels and 1mil rooms  * C: Do customers pay when they make a reservation or when they arrive at the hotel?  * I: They pay in full when making reservations.  * C: Do customers book hotel rooms through the website only? Do we have to support other reservation options such as phone calls?  * I: They make bookings through the website or app only.  * C: Can customers cancel reservations?  * I: Yes  * C: Other things to consider?  * I: Yes, we allow overbooking by 10%. Hotel will sell more rooms than there actually are. Hotels do this in anticipation that clients will cancel bookings.  * C: Since not much time, we'll focus on - show hotel-related page, hotel-room details page, reserve a room, admin panel, support overbooking.  * I: Sounds good.  * I: One more thing - hotel prices change all the time. Assume a hotel room's price changes every day.  * C: OK.</p>"},{"location":"system-design-interview/chapter23/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Support high concurrency - there might be a lot of customers trying to book the same hotel during peak season.</li> <li>Moderate latency - it's ideal to have low latency when a user makes a reservation, but it's acceptable if the system takes a few seconds to process it.</li> </ul>"},{"location":"system-design-interview/chapter23/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>5000 hotels and 1mil rooms in total</li> <li>Assume 70% of rooms are occupied and average stay duration is 3 days</li> <li>Estimated daily reservations - 1mil * 0.7 / 3 = ~240k reservations per day</li> <li>Reservations per second - 240k / 10^5 seconds in a day = ~3. Average reservation TPS is low.</li> </ul> <p>Let's estimate the QPS. If we assume that there are three steps to reach the reservation page and there is a 10% conversion rate per page, we can estimate that if there are 3 reservations, then there must be 30 views of reservation page and 300 views of hotel room detail page. </p>"},{"location":"system-design-interview/chapter23/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>We'll explore - API Design, Data model, high-level design.</p>"},{"location":"system-design-interview/chapter23/#api-design","title":"API Design","text":"<p>This API Design focuses on the core endpoints (using RESTful practices), we'll need in order to support a hotel reservation system.</p> <p>A fully-fledged system would require a more extensive API with support for searching for rooms based on lots of criteria, but we won't be focusing on that in this section. Reason is that they aren't technically challenging, so they're out of scope.</p> <p>Hotel-related API  * <code>GET /v1/hotels/{id}</code> - get detailed info about a hotel  * <code>POST /v1/hotels</code> - add a new hotel. Only available to ops  * <code>PUT /v1/hotels/{id}</code> - update hotel info. Only available to ops  * <code>DELETE /v1/hotels/{id}</code> - delete a hotel. API is only available to ops</p> <p>Room-related API  * <code>GET /v1/hotels/{id}/rooms/{id}</code> - get detailed information about a room  * <code>POST /v1/hotels/{id}/rooms</code> - Add a room. Only available to ops  * <code>PUT /v1/hotels/{id}/rooms/{id}</code> - Update room info. Only available to ops  * <code>DELETE /v1/hotels/{id}/rooms/{id}</code> - Delete a room. Only available to ops</p> <p>Reservation-related API  * <code>GET /v1/reservations</code> - get reservation history of current user  * <code>GET /v1/reservations/{id}</code> - get detailed info about a reservation  * <code>POST /v1/reservations</code> - make a new reservation  * <code>DELETE /v1/reservations/{id}</code> - cancel a reservation</p> <p>Here's an example request to make a reservation: <pre><code>{\n  \"startDate\":\"2021-04-28\",\n  \"endDate\":\"2021-04-30\",\n  \"hotelID\":\"245\",\n  \"roomID\":\"U12354673389\",\n  \"reservationID\":\"13422445\"\n}\n</code></pre></p> <p>Note that the <code>reservationID</code> is an idempotency key to avoid double booking. Details explained in concurrency section</p>"},{"location":"system-design-interview/chapter23/#data-model","title":"Data model","text":"<p>Before we choose what database to use, let's consider our access patterns.</p> <p>We need to support the following queries:  * View detailed info about a hotel  * Find available types of rooms given a date range  * Record a reservation  * Look up a reservation or past history of reservations</p> <p>From our estimations, we know the scale of the system is not large, but we need to prepare for traffic surges.</p> <p>Given this knowledge, we'll choose a relational database because:  * Relational DBs work well with read-heavy and less write-heavy systems.  * NoSQL databases are normally optimized for writes, but we know we won't have many as only a fraction of users who visit the site make a reservation.  * Relational DBs provide ACID guarantees. These are important for such a system as without them, we won't be able to prevent problems such as negative balance, double charge, etc.  * Relational DBs can easily model the data as the structure is very clear.</p> <p>Here is our schema design: </p> <p>Most fields are self-explanatory. Only field worth mentioning is the <code>status</code> field which represents the state machine of a given room: </p> <p>This data model works well for a system like Airbnb, but not for hotels where users don't reserve a particular room but a room type. They reserve a type of room and a room number is chosen at the point of reservation.</p> <p>This shortcoming will be addressed in the Improved Data Model section.</p>"},{"location":"system-design-interview/chapter23/#high-level-design","title":"High-level Design","text":"<p>We've chosen a microservice architecture for this design. It has gained great popularity in recent years:   * Users book a hotel room on their phone or computer  * Admin perform administrative functions such as refunding/cancelling a payment, etc  * CDN caches static resources such as JS bundles, images, videos, etc  * Public API Gateway - fully-managed service which supports rate limiting, authentication, etc.  * Internal APIs - only visible to authorized personnel. Usually protected by a VPN.  * Hotel service - provides detailed information about hotels and rooms. Hotel and room data is static, so it can be cached aggressively.  * Rate service - provides room rates for different future dates. An interesting note about this domain is that prices depend on how full a hotel is at a given day.  * Reservation service - receives reservation requests and reserves hotel rooms. Also tracks room inventory as reservations are made/cancelled.  * Payment service - processes payments and updates reservation statuses on success.  * Hotel management service - available to authorized personnel only. Allows certain administrative functions for managing and viewing reservations, hotels, etc.</p> <p>Inter-service communication can be facilitated via a RPC framework, such as gRPC.</p>"},{"location":"system-design-interview/chapter23/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's dive deeper into:  * Improved data model  * Concurrency issues  * Scalability  * Resolving data inconsistency in microservices</p>"},{"location":"system-design-interview/chapter23/#improved-data-model","title":"Improved data model","text":"<p>As mentioned in a previous section, we need to amend our API and schema to enable reserving a type of room vs. a particular one.</p> <p>For the reservation API, we no longer reserve a <code>roomID</code>, but we reserve a <code>roomTypeID</code>: <pre><code>POST /v1/reservations\n{\n  \"startDate\":\"2021-04-28\",\n  \"endDate\":\"2021-04-30\",\n  \"hotelID\":\"245\",\n  \"roomTypeID\":\"12354673389\",\n  \"roomCount\":\"3\",\n  \"reservationID\":\"13422445\"\n}\n</code></pre></p> <p>Here's the updated schema:   * room - contains information about a room  * room_type_rate - contains information about prices for a given room type  * reservation - records guest reservation data  * room_type_inventory - stores inventory data about hotel rooms. </p> <p>Let's take a look at the <code>room_type_inventory</code> columns as that table is more interesting:  * hotel_id - id of hotel  * room_type_id - id of a room type  * date - a single date  * total_inventory - total number of rooms minus those that are temporarily taken off the inventory.  * total_reserved - total number of rooms booked for given (hotel_id, room_type_id, date)</p> <p>There are alternative ways to design this table, but having one room per (hotel_id, room_type_id, date) enables easy  reservation management and easier queries.</p> <p>The rows in the table are pre-populated using a daily CRON job.</p> <p>Sample data: | hotel_id | room_type_id | date       | total_inventory | total_reserved | |----------|--------------|------------|-----------------|----------------| | 211      | 1001         | 2021-06-01 | 100             | 80             | | 211      | 1001         | 2021-06-02 | 100             | 82             | | 211      | 1001         | 2021-06-03 | 100             | 86             | | 211      | 1001         | ...        | ...             |                | | 211      | 1001         | 2023-05-31 | 100             | 0              | | 211      | 1002         | 2021-06-01 | 200             | 16             | | 2210     | 101          | 2021-06-01 | 30              | 23             | | 2210     | 101          | 2021-06-02 | 30              | 25             |</p> <p>Sample SQL query to check the availability of a type of room: <pre><code>SELECT date, total_inventory, total_reserved\nFROM room_type_inventory\nWHERE room_type_id = ${roomTypeId} AND hotel_id = ${hotelId}\nAND date between ${startDate} and ${endDate}\n</code></pre></p> <p>How to check availability for a specified number of rooms using that data (note that we support overbooking): <pre><code>if (total_reserved + ${numberOfRoomsToReserve}) &lt;= 110% * total_inventory\n</code></pre></p> <p>Now let's do some estimation about the storage volume.  * We have 5000 hotels.  * Each hotel has 20 types of rooms.  * 5000 * 20 * 2 (years) * 365 (days) = 73mil rows</p> <p>73 million rows is not a lot of data and a single database server can handle it. It makes sense, however, to setup read replication (potentially across different zones) to enable high availability.</p> <p>Follow-up question - if reservation data is too large for a single database, what would you do?  * Store only current and future reservation data. Reservation history can be moved to cold storage.  * Database sharding - we can shard our data by <code>hash(hotel_id) % servers_cnt</code> as we always select the <code>hotel_id</code> in our queries.</p>"},{"location":"system-design-interview/chapter23/#concurrency-issues","title":"Concurrency issues","text":"<p>Another important problem to address is double booking.</p> <p>There are two issues to address:  * Same user clicks on \"book\" twice  * Multiple users try to book a room at the same time</p> <p>Here's a visualization of the first problem: </p> <p>There are two approaches to solving this problem:  * Client-side handling - front-end can disable the book button once clicked. If a user disabled javascript, however, they won't see the button becoming grayed out.  * Idemptent API - Add an idempotency key to the API, which enables a user to execute an action once, regardless of how many times the endpoint is invoked: </p> <p>Here's how this flow works:  * A reservation order is generated once you're in the process of filling in your details and making a booking. The reservation order is generated using a globally unique identifier.  * Submit reservation 1 using the <code>reservation_id</code> generated in the previous step.  * If \"complete booking\" is clicked a second time, the same <code>reservation_id</code> is sent and the backend detects that this is a duplicate reservation.  * The duplication is avoided by making the <code>reservation_id</code> column have a unique constraint, preventing multiple records with that id being stored in the DB. </p> <p>What if there are multiple users making the same reservation?   * Let's assume the transaction isolation level is not serializable  * User 1 and 2 attempt to book the same room at the same time.  * Transaction 1 checks if there are enough rooms - there are  * Transaction 2 check if there are enough rooms - there are  * Transaction 2 reserves the room and updates the inventory  * Transaction 1 also reserves the room as it still sees there are 99 <code>total_reserved</code> rooms out of 100.  * Both transactions successfully commit the changes</p> <p>This problem can be solved using some form of locking mechanism:  * Pessimistic locking  * Optimistic locking  * Database constraints</p> <p>Here's the SQL we use to reserve a room: <pre><code># step 1: check room inventory\nSELECT date, total_inventory, total_reserved\nFROM room_type_inventory\nWHERE room_type_id = ${roomTypeId} AND hotel_id = ${hotelId}\nAND date between ${startDate} and ${endDate}\n\n# For every entry returned from step 1\nif((total_reserved + ${numberOfRoomsToReserve}) &gt; 110% * total_inventory) {\n  Rollback\n}\n\n# step 2: reserve rooms\nUPDATE room_type_inventory\nSET total_reserved = total_reserved + ${numberOfRoomsToReserve}\nWHERE room_type_id = ${roomTypeId}\nAND date between ${startDate} and ${endDate}\n\nCommit\n</code></pre></p>"},{"location":"system-design-interview/chapter23/#option-1-pessimistic-locking","title":"Option 1: Pessimistic locking","text":"<p>Pessimistic locking prevents simultaneous updates by putting a lock on a record while it's being updated.</p> <p>This can be done in MySQL by using the <code>SELECT... FOR UPDATE</code> query, which locks the rows selected by the query until the transaction is committed. </p> <p>Pros:  * Prevents applications from updating data that is being changed  * Easy to implement and avoids conflict by serializing updates. Useful when there is heavy data contention.</p> <p>Cons:  * Deadlocks may occur when multiple resources are locked.  * This approach is not scalable - if transaction is locked for too long, this has impact on all other transactions trying to access the resource.  * The impact is severe when the query selects a lot of resources and the transaction is long-lived.</p> <p>The author doesn't recommend this approach due to its scalability issues.</p>"},{"location":"system-design-interview/chapter23/#option-2-optimistic-locking","title":"Option 2: Optimistic locking","text":"<p>Optimistic locking allows multiple users to attempt to update a record at the same time.</p> <p>There are two common ways to implement it - version numbers and timestamps. Version numbers are recommended as server clocks can be inaccurate.   * A new <code>version</code> column is added to the database table  * Before a user modifies a database row, the version number is read  * When the user updates the row, the version number is increased by 1 and written back to the database  * Database validation prevents the insert if the new version number doesn't exceed the previous one</p> <p>Optimistic locking is usually faster than pessimistic locking as we're not locking the database.  Its performance tends to degrade when concurrency is high, however, as that leads to a lot of rollbacks.</p> <p>Pros:  * It prevents applications from editing stale data  * We don't need to acquire a lock in the database  * Preferred option when data contention is low, ie rarely are there update conflicts</p> <p>Cons:  * Performance is poor when data contention is high</p> <p>Optimistic locking is a good option for our system as reservation QPS is not extremely high.</p>"},{"location":"system-design-interview/chapter23/#option-3-database-constraints","title":"Option 3: Database constraints","text":"<p>This approach is very similar to optimistic locking, but the guardrails are implemented using a database constraint: <pre><code>CONSTRAINT `check_room_count` CHECK((`total_inventory - total_reserved` &gt;= 0))\n</code></pre> </p> <p>Pros:  * Easy to implement  * Works well when data contention is small</p> <p>Cons:  * Similar to optimistic locking, performs poorly when data contention is high  * Database constraints cannot be easily version-controlled like application code  * Not all databases support constraints</p> <p>This is another good option for a hotel reservation system due to its ease of implementation.</p>"},{"location":"system-design-interview/chapter23/#scalability","title":"Scalability","text":"<p>Usually, the load of a hotel reservation system is not high. </p> <p>However, the interviewer might ask you how you'd handle a situation where the system gets adopted for a larger, popular travel site such as booking.com In that case, QPS can be 1000 times larger.</p> <p>When there is such a situation, it is important to understand where our bottlenecks are. All the services are stateless, so they can be easily scaled via replication.</p> <p>The database, however, is stateful and it's not as obvious how it can get scaled.</p> <p>One way to scale it is by implementing database sharding - we can split the data across multiple databases, where each of them contain a portion of the data.</p> <p>We can shard based on <code>hotel_id</code> as all queries filter based on it.  Assuming, QPS is 30,000, after sharding the database in 16 shards, each shard handles 1875 QPS, which is within a single MySQL cluster's load capacity. </p> <p>We can also utilize caching for room inventory and reservations via Redis. We can set TTL so that old data can expire for days which are past. </p> <p>The way we store an inventory is based on the <code>hotel_id</code>, <code>room_type_id</code> and <code>date</code>: <pre><code>key: hotelID_roomTypeID_{date}\nvalue: the number of available rooms for the given hotel ID, room type ID and date.\n</code></pre></p> <p>Data consistency happens async and is managed by using a CDC streaming mechanism - database changes are read and applied to a separate system. Debezium is a popular option for synchronizing database changes with Redis.</p> <p>Using such a mechanism, there is a possibility that the cache and database are inconsistent for some time. This is fine in our case because the database will prevent us from making an invalid reservation.</p> <p>This will cause some issue on the UI as a user would have to refresh the page to see that \"there are no more rooms left\",  but that is something which can happen regardless of this issue if eg a person hesitates a lot before making a reservation.</p> <p>Caching pros:  * Reduced database load  * High performance, as Redis manages data in-memory</p> <p>Caching cons:  * Maintaining data consistency between cache and DB is hard. We need to consider how the inconsistency impacts user experience.</p>"},{"location":"system-design-interview/chapter23/#data-consistency-among-services","title":"Data consistency among services","text":"<p>A monolithic application enables us to use a shared relational database for ensuring data consistency.</p> <p>In our microservice design, we chose a hybrid approach where some services are separate,  but the reservation and inventory APIs are handled by the same servicefor the reservation and inventory APIs.</p> <p>This is done because we want to leverage the relational database's ACID guarantees to ensure consistency.</p> <p>However, the interviewer might challenge this approach as it's not a pure microservice architecture, where each service has a dedicated database: </p> <p>This can lead to consistency issues. In a monolithic server, we can leverage a relational DBs transaction capabilities to implement atomic operations: </p> <p>It's more challenging, however, to guarantee this atomicity when the operation spans across multiple services: </p> <p>There are some well-known techniques to handle these data inconsistencies:  * Two-phase commit - a database protocol which guarantees atomic transaction commit across multiple nodes.     It's not performant, though, since a single node lag leads to all nodes blocking the operation.  * Saga - a sequence of local transactions, where compensating transactions are triggered if any of the steps in a workflow fail. This is an eventually consistent approach.</p> <p>It's worth noting that addressing data inconsistencies across microservices is a challenging problem, which raise the system complexity. It is good to consider whether the cost is worth it, given our more pragmatic approach of encapsulating dependent operations within the same relational database.</p>"},{"location":"system-design-interview/chapter23/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>We presented a design for a hotel reservation system.</p> <p>These are the steps we went through:  * Gathering requirements and doing back-of-the-envelope calculations to understand the system's scale  * We presented the API Design, Data Model and system architecture in the high-level design  * In the deep dive, we explored alternative database schema designs as requirements changed  * We discussed race conditions and proposed solutions - pessimistic/optimistic locking, database constraints  * Ways to scale the system via database sharding and caching  * Finally we addressed how to handle data consistency issues across multiple microservices</p>"},{"location":"system-design-interview/chapter24/","title":"Distributed Email Service","text":"<p>We'll design a distributed email service, similar to gmail in this chapter.</p> <p>In 2020, gmail had 1.8bil active users, while Outlook had 400mil users worldwide.</p>"},{"location":"system-design-interview/chapter24/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: How many users use the system?</li> <li>I: 1bil users</li> <li>C: I think following features are important - auth, send/receive email, fetch email, filter emails, search email, anti-spam protection.</li> <li>I: Good list. Don't worry about auth for now.</li> <li>C: How do users connect \\w email servers?</li> <li>I: Typically, email clients connect via SMTP, POP, IMAP, but we'll use HTTP for this problem.</li> <li>C: Can emails have attachments?</li> <li>I: Yes</li> </ul>"},{"location":"system-design-interview/chapter24/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Reliability - we shouldn't lose data</li> <li>Availability - We should use replication to prevent single points of failure. We should also tolerate partial system failures.</li> <li>Scalability - As userbase grows, our system should be able to handle them.</li> <li>Flexibility and extensibility - system should be flexible and easy to extend with new features. One of the reasons we chose HTTP over SMTP/other mail protocols.</li> </ul>"},{"location":"system-design-interview/chapter24/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>1bil users</li> <li>Assuming one person sends 10 emails per day -&gt; 100k emails per second.</li> <li>Assuming one person receives 40 emails per day and each email on average has 50kb metadata -&gt; 730pb storage per year.</li> <li>Assuming 20% of emails have storage attachments and average size is 500kb -&gt; 1,460pb per year.</li> </ul>"},{"location":"system-design-interview/chapter24/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"system-design-interview/chapter24/#email-knowledge-101","title":"Email knowledge 101","text":"<p>There are various protocols used for sending and receiving emails:  * SMTP - standard protocol for sending emails from one server to another.  * POP - standard protocol for receiving and downloading emails from a remote mail server to a local client. Once retrieved, emails are deleted from remote server.  * IMAP - similar to POP, it is used for receiving and downloading emails from a remote server, but it keeps the emails on the server-side.  * HTTPS - not technically an email protocol, but it can be used for web-based email clients.</p> <p>Apart from the mailing protocol, there are some DNS records we need to configure for our email server - the MX records: </p> <p>Email attachments are sent base64-encoded and there is usually a size limit of 25mb on most mail services. This is configurable and varies from individual to corporate accounts.</p>"},{"location":"system-design-interview/chapter24/#traditional-mail-servers","title":"Traditional mail servers","text":"<p>Traditional mail servers work well when there are a limited number of users, connected to a single server.   * Alice logs into her Outlook email and presses \"send\". Email is sent to Outlook mail server. Communication is via SMTP.  * Outlook server queries DNS to find MX record for gmail.com and transfers the email to their servers. Communication is via SMTP.  * Bob fetches emails from his gmail server via IMAP/POP.</p> <p>In traditional mail servers, emails were stored on the local file system. Every email was a separate file. </p> <p>As the scale grew, disk I/O became a bottleneck. Also, it doesn't satisfy our high availability and reliability requirements. Disks can be damaged and server can go down.</p>"},{"location":"system-design-interview/chapter24/#distributed-mail-servers","title":"Distributed mail servers","text":"<p>Distributed mail servers are designed to support modern use-cases and solve modern scalability issues.</p> <p>These servers can still support IMAP/POP for native email clients and SMTP for mail exchange across servers.</p> <p>But for rich web-based mail clients, a RESTful API over HTTP is typically used.</p> <p>Example APIs:  * <code>POST /v1/messages</code> - sends a message to recipients in To, Cc, Bcc headers.  * <code>GET /v1/folders</code> - returns all folders of an email account Example response: <pre><code>[{id: string        Unique folder identifier.\n  name: string      Name of the folder.\n                    According to RFC6154 [9], the default folders can be one of\n                    the following: All, Archive, Drafts, Flagged, Junk, Sent,\n                    and Trash.\n  user_id: string   Reference to the account owner\n}]\n</code></pre>  * <code>GET /v1/folders/{:folder_id}/messages</code> - returns all messages under a folder \\w pagination  * <code>GET /v1/messages/{:message_id}</code> - get all information about a particular message Example response: <pre><code>{\n  user_id: string                      // Reference to the account owner.\n  from: {name: string, email: string}  // &lt;name, email&gt; pair of the sender.\n  to: [{name: string, email: string}]  // A list of &lt;name, email&gt; paris\n  subject: string                      // Subject of an email\n  body: string                         //  Message body\n  is_read: boolean                     //  Indicate if a message is read or not.\n}\n</code></pre></p> <p>Here's the high-level design of the distributed mail server:   * Webmail - users use web browsers to send/receive emails  * Web servers - public-facing request/response services used to manage login, signup, user profile, etc.  * Real-time servers - Used for pushing new email updates to clients in real-time. We use websockets for real-time communication but fallback to long-polling for older browsers that don't support them.  * Metadata db - stores email metadata such as subject, body, from, to, etc.  * Attachment store - Object store (eg Amazon S3), suitable for storing large files.  * Distributed cache - We can cache recent emails in Redis to improve UX.  * Search store - distributed document store, used for supporting full-text searches.</p> <p>Here's what the email sending flow looks like:   * User writes an email and presses \"send\". Email is sent to load balancer.  * Load balancer rate limits excessive mail sends and routes to one of the web servers.  * Web servers do basic email validation (eg email size) and short-circuits outbound flow if domain is same as sender. But does spam check first.  * If basic validation passes, email is sent to message queue (attachment is referenced from object store)  * If basic validation fails, email is sent to error queue  * SMTP outgoing workers pull messages from outgoing queue, do spam/virus checks and route to destination mail server.  * Email is stored in the \"Sent Emails\" folder</p> <p>We need to also monitor size of outgoing message queue. Growing too large might indicate a problem:  * Recipient's mail server is unavailable. We can retry sending the email at a later time using exponential backoff.  * Not enough consumers to handle the load, we might have to scale the consumers.</p> <p>Here's the email receiving flow:   * Incoming emails arrive at the SMTP load balancer. Mails are distributed to SMTP servers, where mail acceptance policy is done (eg invalid emails are directly discarded).  * If attachment of email is too large, we can put it in object store (s3).  * Mail processing workers do preliminary checks, after which mails are forwarded to storage, cache, object store and real-time servers.  * Offline users get their new emails once they come back online via HTTP API.</p>"},{"location":"system-design-interview/chapter24/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's now go deeper into some of the components.</p>"},{"location":"system-design-interview/chapter24/#metadata-database","title":"Metadata database","text":"<p>Here are some of the characteristics of email metadata:  * headers are usually small and frequently accessed  * Body size ranges from small to big, but is typically read once  * Most mail operations are isolated to a single user - eg fetching email, marking as read, searching.  * Data recency impacts data usage. Users typically read only recent emails  * Data has high-reliability requirements. Data loss is unacceptable.</p> <p>At gmail/outlook scale, the database is typically custom made to reduce input/output operations per second (IOPS).</p> <p>Let's consider what database options we have:  * Relational database - we can build indexes for headers and body, but these DBs are typically optimized for small chunks of data.  * Distributed object store - this can be a good option for backup storage, but can't efficiently support searching/marking as read/etc.  * NoSQL - Google BigTable is used by gmail, but it's not open-sourced.</p> <p>Based on above analysis, very few existing solutions seems to fit our needs perfectly. In an interview setting, it's infeasible to design a new distributed database solution, but important to mention characteristics:  * Single column can be a single-digit MB  * Strong data consistency  * Designed to reduce disk I/O  * Highly available and fault tolerant  * Should be easy to create incremental backups</p> <p>In order to partition the data, we can use the <code>user_id</code> as a partition key, so that one user's data is stored on a single shard. This prohibits us from sharing an email with multiple users, but this is not a requirement for this interview.</p> <p>Let's define the tables:  * Primary key consists of partition key (data distribution) and clustering key (sorting data)  * Queries we need to support - get all folders for a user, display all emails for a folder, create/get/delete an email, fetch read/unread email, get conversation threads (bonus)</p> <p>Legend for tables to follow: </p> <p>Here is the folders table: </p> <p>emails table:   * email_id is timeuuid which allows sorting based on timestamp when email was created</p> <p>Attachments are stored in a separate table, identified by filename: </p> <p>Supporting fetchin read/unread emails is easy in a traditional relational database, but not in Cassandra, since filtering on non-partition/clustering key is prohibited. One workaround to fetch all emails in a folder and filter in-memory, but that doesn't work well for a big-enough application.</p> <p>What we can do is denormalize the emails table into read/unread emails tables: </p> <p>In order to support conversation threads, we can include some headers, which mail clients interpret and use to reconstruct a conversation thread: <pre><code>{\n  \"headers\" {\n     \"Message-Id\": \"&lt;7BA04B2A-430C-4D12-8B57-862103C34501@gmail.com&gt;\",\n     \"In-Reply-To\": \"&lt;CAEWTXuPfN=LzECjDJtgY9Vu03kgFvJnJUSHTt6TW@gmail.com&gt;\",\n     \"References\": [\"&lt;7BA04B2A-430C-4D12-8B57-862103C34501@gmail.com&gt;\"]\n  }\n}\n</code></pre></p> <p>Finally, we'll trade availability for consistency for our distributed database, since it is a hard requirement for this problem.</p> <p>Hence, in the event of a failover or network parititon, sync/update actions will be briefly unavailable to impacted users.</p>"},{"location":"system-design-interview/chapter24/#email-deliverability","title":"Email deliverability","text":"<p>It is easy to setup a server to send emails, but getting the email to a receiver's inbox is hard, due to spam-protection algorithms.</p> <p>If we just setup a new mail server and start sending mails through it, our emails will probably end up in the spam folder.</p> <p>Here's what we can do to prevent that:  * Dedicated IPs - use dedicated IPs for sending emails, otherwise, recipient servers will not trust you.  * Classify emails - avoid sending marketing emails from the same servers to prevent more important email to be classified as spam  * Warm up your IP address slowly to build a good reputation with big email providers. It takes 2 to 6 weeks to warm up a new IP  * Ban spammers quickly to not deteriorate your reputation  * Feedback processing - setup a feedback loop with ISPs to keep track of complaint rate and ban spam accounts quickly.  * Email authentication - use common techniques to combat phishing such as Sender Policy Framework, DomainKeys Identified Mail, etc.</p> <p>You don't need to remember all of this. Just know that building a good mail server requires a lot of domain knowledge.</p>"},{"location":"system-design-interview/chapter24/#search","title":"Search","text":"<p>Searching includes doing a full-text search based on email contents or more advanced queries based on from, to, subject, unread, etc filters.</p> <p>One characteristic of email search is that it is local to the user and it has more writes than reads, because we need to re-index it on each operation, but users rarely use the search tab.</p> <p>Let's compare google search with email search: |               | Scope                | Sorting                               | Accuracy                                          | |---------------|----------------------|---------------------------------------|---------------------------------------------------| | Google search | The whole internet   | Sort by relevance                     | Indexing takes some time, so not instant results. | | Email search  | User\u2019s own email box | Sort by attributes eg time, date, etc | Indexing should be quick and results accurate.    |</p> <p>To achieve this search functionality, one option is to use an Elasticsearch cluster. We can use <code>user_id</code> as the partition key to group data under the same node: </p> <p>Mutating operations are async via Kafka in order to decouple services from the reindexing flow. Actually searching for data happens synchronously.</p> <p>Elasticsearch is one of the most popular search-engine databases and supports full-text search for emails very well.</p> <p>Alternatively, we can attempt to develop our own custom search solution to meet our specific requirements.</p> <p>Designing such a system is out of scope. One of the core challenges when building it is to optimize it for write-heavy workloads.</p> <p>To achieve that, we can use Log-Structured Merge-Trees (LSM) to structure the index data on disk. Write path is optimized for sequential writes only. This technique is used in Cassandra, BigTable and RocksDB.</p> <p>Its core idea is to store data in-memory until a predefined threshold is reached, after which it is merged in the next layer (disk): </p> <p>Main trade-offs between the two approaches:  * Elasticsearch scales to some extent, whereas a custom search engine can be fine-tuned for the email use-case, allowing it to scale further.  * Elasticsearch is a separate service we need to maintain, alongside the metadata store. A custom solution can be the datastore itself.  * Elasticsearch is an off-the-shelf solution, whereas the custom search engine would require significant engineering effort to build.</p>"},{"location":"system-design-interview/chapter24/#scalability-and-availability","title":"Scalability and availability","text":"<p>Since individual user operations don't collide with other users, most components can be independently scaled.</p> <p>To ensure high availability, we can also use a multi-DC setup with leader-folower failover in case of failures: </p>"},{"location":"system-design-interview/chapter24/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Additional talking points:  * Fault tolerance - Many parts of the system could fail. It is worthwhile how we'd handle node failures.  * Compliance - PII needs to be stored in a reasonable way, given Europe's GDPR laws.  * Security - email encryption, phishing protection, safe browsing, etc.  * Optimizations - eg preventing duplication of the same attachments, sent multiple times by different users.</p>"},{"location":"system-design-interview/chapter25/","title":"S3-like Object Storage","text":"<p>In this chapter, we'll be designing an object storage service, similar to Amazon S3.</p> <p>Storage systems fall into three broad categories:  * Block storage  * File storage  * Object storage</p> <p>Block storage are devices, which came out in 1960s. HDDs and SSDs are such examples. These devices are typically physically attached to a server, although they can also be network-attached via high-speed network protocols. Servers can format the raw blocks and use them as a file system or it can hand control of them to servers directly.</p> <p>File storage is built on top of block storage. It provides a higher level of abstraction, making it easier to manage folders and files.</p> <p>Object storage sacrifices performance for high durability, vast scale and low cost. It targets \"cold\" data and is mainly used for archival and backup. There is no hierarchical directory structure, all data is stored as objects in a flat structure. It is relatively slow compared to other storage types. Most cloud providers have an object storage offering - Amazon S3, Google GCS, etc. </p> Block Storage File Storage Object Storage Mutable Content Y Y N (has object versioning\uff09 Cost High Medium to high Low Performance Medium to high, very high Medium to high Low to medium Consistency Strong consistency Strong consistency Strong consistency [5] Data access SAS/iSCSI/FC Standard file access, CIFS/SMB, and NFS RESTful API Scalability Medium scalability High scalability Vast scalability Good for Virtual machines (VM), databases General-purpose file system access Binary data, unstructured data <p>Some terminology, related to object storage:  * Bucket - logical container for objects. Name is globally unique.  * Object - An individual piece of data, stored in a bucket. Contains object data and metadata.  * Versioning - A feature keeping multiple variants of an object in the same bucket.  * Uniform Resource Identifier (URI) - each resource is uniquely identified by a URI.  * Service-level Agreement (SLA) - contract between service provider and client. </p> <p>Amazon S3 Standard-Infrequent Access storage class SLAs:  * Durability of 99.999999999% across multiple Availability Zones  * Data is resilient in the event of entire Availability Zone being destroyed  * Designed for 99.9% availability</p>"},{"location":"system-design-interview/chapter25/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: Which features should be included?</li> <li>I: Bucket creation, Object upload/download, versioning, Listing objects in a bucket</li> <li>C: What is the typical data size?</li> <li>I: We need to store both massive objects and small objects efficiently</li> <li>C: How much data do we store in a year?</li> <li>I: 100 petabytes</li> <li>C: Can we assume 6 nines of data durbility (99.9999%) and service availability of 4 nines (99.99%)?</li> <li>I: Yes, sounds reasonable</li> </ul>"},{"location":"system-design-interview/chapter25/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>100 PB of data</li> <li>6 nines of data durability</li> <li>4 nines of service availability</li> <li>Storage efficiency. Reduce storage cost while maintaining high reliability and performance</li> </ul>"},{"location":"system-design-interview/chapter25/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>Object storage is likely to have bottlenecks in disk capacity or IO per second (IOPS).</p> <p>Assumptions:  * we have 20% small (less than 1mb), 60% mid-size (1-64mb) and 20% large objects (greater than 64mb),  * One hard disk (SATA, 7200rpm) is capable of doing 100-150 random seeks per second (100-150 IOPS)</p> <p>Given the assumptions, we can estimate the total number of objects the system can persist.  * Let's use median size per object type to simplify calculation - 0.5mb for small, 32mb for medium, 200mb for large.  * Given 100PB of storage (10^11 MB) and 40% of storage usage results in 0.68bil objects  * If we assume metadata is 1kb, then we need 0.68tb space to store metadata info</p>"},{"location":"system-design-interview/chapter25/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>Let's explore some interesting properties of object storage before diving into the design:  * Object immutability - objects in object storage are immutable (not the case in other storage systems). We may delete them or replace them, but no update.  * Key-value store - an object URI is its key and we can get its contents by making an HTTP call  * Write once, read many times - data access pattern is writing once and reading many times. According to some Linkedin research, 95% of operations are reads  * Support both small and large objects</p> <p>Design philosophy of object storage is similar to UNIX - when we save a file, it creates the filename in a data structure, called inode and file data is stored in different disk locations. The inode contains a list of file block pointers, which point to different locations on disk. </p> <p>When accessing a file, we first fetch its metadata from the inode, prior to fetching the file contents.</p> <p>Object storage works similarly - metadata store is used for file information, but contents are stored on disk: </p> <p>By separating metadata from file contents, we can scale the different stores independently: </p>"},{"location":"system-design-interview/chapter25/#high-level-design","title":"High-level design","text":"<p>  * Load balancer - distributes API requests across service replicas  * API service - Stateless server, orchestrating calls to metadata and object store, as well as IAM service.  * Identity and access management (IAM) - central place for auth, authz, access control.  * Data store - stores and retrieves actual data. Operations are based on object ID (UUID).  * Metadata store - stores object metadata</p>"},{"location":"system-design-interview/chapter25/#uploading-an-object","title":"Uploading an object","text":"<p>  * Create a bucket named \"bucket-to-share\" via HTTP PUT request  * API service calls IAM to ensure user is authorized and has write permissions  * API service calls metadata store to create a bucket entry. Once created, success response is returned.  * After bucket is created, HTTP PUT is sent to create an object named \"script.txt\"  * API service verifies user identity and ensures user has write permissions  * Once validation passes, object payload is sent via HTTP PUT to the data store. Data store persists it and returns a UUID.  * API service calls metadata store to create a new entry with object_id, bucket_id and bucket_name, among other metadata.</p> <p>Example object upload request: <pre><code>PUT /bucket-to-share/script.txt HTTP/1.1\nHost: foo.s3example.org\nDate: Sun, 12 Sept 2021 17:51:00 GMT\nAuthorization: authorization string\nContent-Type: text/plain\nContent-Length: 4567\nx-amz-meta-author: Alex\n\n[4567 bytes of object data]\n</code></pre></p>"},{"location":"system-design-interview/chapter25/#downloading-an-object","title":"Downloading an object","text":"<p>Buckets have no directory hierarchy, buy we can create a logical hierarchy by concatenating bucket name and object name to simulate a folder structure.</p> <p>Example GET request for fetching an object: <pre><code>GET /bucket-to-share/script.txt HTTP/1.1\nHost: foo.s3example.org\nDate: Sun, 12 Sept 2021 18:30:01 GMT\nAuthorization: authorization string\n</code></pre></p> <p>  * Client sends an HTTP GET request to the load balancer, ie <code>GET /bucket-to-share/script.txt</code>  * API service queries IAM to verify the user has correct permissions to read the bucket  * Once validated, UUID of object is retrieved from metadata store  * Object payload is retrieved from data store based on UUID and returned to the client</p> <p>// sprint 1</p>"},{"location":"system-design-interview/chapter25/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":""},{"location":"system-design-interview/chapter25/#data-store","title":"Data store","text":"<p>Here's how the API service interacts with the data store: </p> <p>The data store's main components: </p> <p>The data routing service provides a RESTful or gRPC API to access the data node cluster. It is a stateless service, which scales by adding more servers.</p> <p>It's main responsibilities are:  * querying the placement service to get the best data node to store data  * reading data from data nodes and returning it to the API service  * Writing data to data nodes</p> <p>The placement service determines which data nodes should store an object. It maintains a virtual cluster map, which determines the physical topology of a cluster. </p> <p>The service also sends heartbeats to all data nodes to determine if they should be removed from the virtual cluster.</p> <p>Since this is a critical service, it is recommended to maintain a cluster of 5 or 7 replicas, synchronized via Paxos or Raft consensus algorithms. Eg a 7 node cluster can tolerate 3 nodes failing.</p> <p>Data nodes store the actual object data. Reliability and durability is ensured by replicating data to multiple data nodes.</p> <p>Each data node has a daemon running, which sends heartbeats to the placement service.</p> <p>The heartbeat includes:  * How many disk drives (HDD or SSD) does the data node manage?  * How much data is stored on each drive?</p>"},{"location":"system-design-interview/chapter25/#data-persistence-flow","title":"Data persistence flow","text":"<p>  * API service forwards the object data to data store  * Data routing service sends the data to the primary data node  * Primary data node saves the data locally and replicates it to two secondary data nodes. Response is sent after successful replication.  * The UUID of the object is returned to the API service.</p> <p>Caveats:  * Given an object UUID, it's replication group is deterministically chosen by using consistent hashing  * In step 4, the primary data node replicates the object data before returning a response. This favors strong consistency over higher latency. </p>"},{"location":"system-design-interview/chapter25/#how-data-is-organized","title":"How data is organized","text":"<p>One simple approach to managing data is to store each object in a separate file. </p> <p>This works, but is not performant with many small files in a file system:  * Data blocks on HDD are wasted, because every file uses the whole block size. Typical block size is 4kb.  * Many files means many inodes. Operating systems don't deal well with too many inodes and there is also a max inode limit.</p> <p>These issues can be addressed by merging many small files into bigger ones via a write-ahead log (WAL). Once the file reaches its capacity (typically a few GB), a new file is created: </p> <p>The downside of this approach is that write access to the file needs to be serialized. Multiple cores accessing the same file must wait for each other. To fix this, we can confine files to specific cores to avoid lock contention.</p>"},{"location":"system-design-interview/chapter25/#object-lookup","title":"Object lookup","text":"<p>To support storing multiple objects in the same file, we need to maintain a table, which tells the data node:  * <code>object_id</code>  * <code>filename</code> where object is stored  * <code>file_offset</code> where object starts  * <code>object_size</code></p> <p>We can deploy this table in a file-based db like RocksDB or a traditional relational database. Since the access pattern is low write+high read, a relational database works better.</p> <p>How should we deploy it? We could deploy the db and scale it separately in a cluster, accessed by all data nodes.</p> <p>Downsides:  * we'd need to aggressively scale the cluster to serve all requests  * there's additional network latency between data node and db cluster</p> <p>An alternative is to take advantage of the fact that data nodes are only interested to data related to them,  so we can deploy the relational db within the data node itself. </p> <p>SQLite is a good option as it's a lightweight file-based relational database.</p>"},{"location":"system-design-interview/chapter25/#updated-data-persistence-flow","title":"Updated data persistence flow","text":"<p>  * API Service sends a request to save a new object  * Data node service appends the new object at the end of a file, named \"/data/c\"  * A new record for the object is inserted into the object mapping table</p>"},{"location":"system-design-interview/chapter25/#durability","title":"Durability","text":"<p>Data durability is an important requirement in our design. In order to achieve 6 nines of durability, every failure case needs to be properly examined.</p> <p>First problem to address is hardware failures. We can achieve that by replicating data nodes to minimize probability of failure. But in addition to that, we also ought to replicate across different failure domains (cross-rack, cross-dc, separate networks, etc).  A critical event can cause multiple hardware failures within the same domain: </p> <p>Assuming annual failure rate of a typical HDD is 0.81%, making three copies gives us 6 nines of durability.</p> <p>Replicating the data nodes like that grants us the durability we want, but we could also leverage erasure coding to reduce storage costs.</p> <p>Erasure coding enables us to use parity bits, which allow us to reconstruct lost bits in the event of a failure: </p> <p>Imagine those bits are data nodes. If two of them go down, they can be recovered using the remaining four ones.</p> <p>There are different erasure coding schemes. In our case, we could use 8+4 erasure coding, split across different failure domains to maximize reliability: </p> <p>Erasure coding enables us to achieve a much lower storage cost (50% improvement) at the expense of access speed due to the data routing service having to collect data from multiple locations: </p> <p>Other caveats:  * Replication requires 200% storage overhead (in case of 3 replicas) vs. 50% via erasure coding  * Erasure coding gives us 11 nines of durability vs 6 nines via replication  * Erasure coding requires more computation to calculate and store parities</p> <p>In sum, replication is more useful for latency-sensitive applications, whereas erasure coding is attractive for storage cost efficiency and durability. Erasure coding is also much harder to implement.</p>"},{"location":"system-design-interview/chapter25/#correctness-verification","title":"Correctness verification","text":"<p>If a disk fails entirely, then the failure is easy to detect. This is less straightforward in the event part of the disk memory gets corrupted.</p> <p>To detect this, we can use checksums - a hash of the file contents, which can be used to verify the file's integrity.</p> <p>In our case, we'll store checksums for each file and each object: </p> <p>In the case of erasure coding (8+4), we'll need to fetch each of the 8 pieces of data separately and verify each of their checksums.</p> <p>// sprint 2</p>"},{"location":"system-design-interview/chapter25/#metadata-data-model","title":"Metadata data model","text":"<p>Table schemas: </p> <p>Queries we need to support:  * Find an object ID by name  * Insert/delete object based on name  * List objects in a bucket sharing the same prefix</p> <p>There is usually a limit on the number of buckets a user can create, hence, the size of the buckets table is small and can fit into a single db server. But we still need to scale the server for read throughput.</p> <p>The object table will probably not fit into a single database server, though. Hence, we can scale the table via sharding:  * Sharding by bucket_id will lead to hotspot issues as a bucket can have billions of objects  * Sharding by bucket_id makes the load more evenly distributed, but our queries will be slow  * We choose sharding by <code>hash(bucket_name, object_name)</code> since most queries are based on the object/bucket name.</p> <p>Even with this sharding scheme, though, listing objects in a bucket will be slow.</p>"},{"location":"system-design-interview/chapter25/#listing-objects-in-a-bucket","title":"Listing objects in a bucket","text":"<p>In a single database, listing an object based on its prefix (looks like a directory) works like this: <pre><code>SELECT * FROM object WHERE bucket_id = \"123\" AND object_name LIKE `abc/%`\n</code></pre></p> <p>This is challenging to fulfill when the database is sharded. To achieve it, we can run the query on every shard and aggregate the results in-memory. This makes pagination challenging though, since different shards contain a different result size and we need to maintain separate limit/offset for each.</p> <p>We can leverage the fact that typically object stores are not optimized for listing objects, so we can sacrifice listing performance. We can also create a denormalized table for listing objects, sharded by bucket ID.  That would make our listing query sufficiently fast as it's isolated to a single database instance.</p>"},{"location":"system-design-interview/chapter25/#object-versioning","title":"Object versioning","text":"<p>Versioning works by having another <code>object_version</code> column which is of type TIMEUUID, enabling us to sort records based on it.</p> <p>Each new version produces a new <code>object_id</code>: </p> <p>Deleting an object creates a new version with a special <code>object_id</code> indicating that the object was deleted. Queries for it return 404: </p>"},{"location":"system-design-interview/chapter25/#optimizing-uploads-of-large-files","title":"Optimizing uploads of large files","text":"<p>Uploading large files can be optimized by using multipart uploads - splitting a big file into several chunks, uploaded independently:   * Client calls service to initiate a multipart upload  * Data store returns an upload ID which uniquely identifies the upload  * Client splits the large file into several chunks, uploaded independently using the upload id  * When a chunk is uploaded, the data store returns an etag, which is a md5 checksum, identifying that upload chunk  * After all parts are uploaded, client sends a complete multipart upload request, which includes upload_id, part numbers and all etags  * Data store reassembles the object from its parts. The process can take a few minutes. After that, success response is returned to the client.</p> <p>Old parts, which are no longer useful can be removed at this point. We can introduce a garbage collector to deal with it.</p>"},{"location":"system-design-interview/chapter25/#garbage-collection","title":"Garbage collection","text":"<p>Garbage collection is the process of reclaiming storage space, which is no longer used. There are a few ways data becomes garbage:  * lazy object deletion - object is marked as deleted without actually getting deleted  * orphan data - eg an upload failed mid-flight and old parts need to be deleted  * corrupted data - data which failed checksum verification</p> <p>The garbage collector is also responsible for reclaiming unused space in replicas.  With replication, data is deleted from both primaries and replicas. With erasure coding (8+4), data is deleted from all 12 nodes.</p> <p>To facilitate the deletion, we'll use a process called compaction:  * Garbage collector copies objects which are not deleted from \"data/b\" to \"data/d\"  * <code>object_mapping</code> table is updated once copying is complete using a database transaction  * To avoid making too many small files, compaction is done on files which grow beyond a certain threshold </p>"},{"location":"system-design-interview/chapter25/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Things we covered:  * Designing an S3-like object storage  * Comparing differences between object, block and file storages  * Covered uploading, downloading, listing, versioning of objects in a bucket  * Deep dived in the design - data store and metadata store, replication and erasure coding, multipart uploads, sharding</p>"},{"location":"system-design-interview/chapter26/","title":"Real-time Gaming Leaderboard","text":"<p>We are going to design a leaderboard for an online mobile game: </p>"},{"location":"system-design-interview/chapter26/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: How is the score calculated for the leaderboard?</li> <li>I: User gets a point whenever they win a match.</li> <li>C: Are all players included in the leaderboard?</li> <li>I: Yes</li> <li>C: Is there a time segment, associated with the leaderboard?</li> <li>I: Each month, a new tournament starts which starts a new leaderboard.</li> <li>C: Can we assume we only care about top 10 users?</li> <li>I: We want to display top 10 users, along with position of specific user. If time permits, we can discuss showing users around particular user in the leaderboard.</li> <li>C: How many users are in a tournament?</li> <li>I: 5mil DAU and 25mil MAU</li> <li>C: How many matches are played on average during a tournament?</li> <li>I: Each player plays 10 matches per day on average</li> <li>C: How do we determine the rank if two players have the same score?</li> <li>I: Their rank is the same in that case. If time permits, we can discuss breaking ties.</li> <li>C: Does the leaderboard need to be real-time?</li> <li>I: Yes, we want to present real-time results or as close as possible to real-time. It is not okay to present batched result history.</li> </ul>"},{"location":"system-design-interview/chapter26/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Display top 10 players on leaderboard</li> <li>Show a user's specific rank</li> <li>Display users which are four places above and below given user (bonus)</li> </ul>"},{"location":"system-design-interview/chapter26/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Real-time updates on scores</li> <li>Score update is reflected on the leaderboard in real-time</li> <li>General scalability, availability, reliability</li> </ul>"},{"location":"system-design-interview/chapter26/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>With 50mil DAU, if the game has an even distribution of players during a 24h period, we'd have an average of 50 users per second. However, since distribution is typically uneven, we can estimate that the peak online users would be 250 users per second.</p> <p>QPS for users scoring a point - given 10 games per day on average, 50 users/s * 10 = 500 QPS. Peak QPS = 2500.</p> <p>QPS for fetching the top 10 leaderboard - assuming users open that once a day on average, QPS is 50.</p>"},{"location":"system-design-interview/chapter26/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"system-design-interview/chapter26/#api-design","title":"API Design","text":"<p>The first API we need is one to update a user's score: <pre><code>POST /v1/scores\n</code></pre></p> <p>This API takes two params - <code>user_id</code> and <code>points</code> scored for winning a game.</p> <p>This API should only be accessible to game servers, not end clients.</p> <p>Next one is for getting the top 10 players of the leaderboard: <pre><code>GET /v1/scores\n</code></pre></p> <p>Example response: <pre><code>{\n  \"data\": [\n    {\n      \"user_id\": \"user_id1\",\n      \"user_name\": \"alice\",\n      \"rank\": 1,\n      \"score\": 12543\n    },\n    {\n      \"user_id\": \"user_id2\",\n      \"user_name\": \"bob\",\n      \"rank\": 2,\n      \"score\": 11500\n    }\n  ],\n  ...\n  \"total\": 10\n}\n</code></pre></p> <p>You can also get the score of a particular user: <pre><code>GET /v1/scores/{:user_id}\n</code></pre></p> <p>Example response: <pre><code>{\n    \"user_info\": {\n        \"user_id\": \"user5\",\n        \"score\": 1000,\n        \"rank\": 6,\n    }\n}\n</code></pre></p>"},{"location":"system-design-interview/chapter26/#high-level-architecture","title":"High-level architecture","text":"<p>  * When a player wins a game, client sends a request to the game service  * Game service validates if win is valid and calls the leaderboard service to update the player's score  * Leaderboard service updates the user's score in the leaderboard store  * Player makes a call to leaderboard service to fetch leaderboard data, eg top 10 players and given player's rank</p> <p>An alternative design which was considered is the client updating their score directly within the leaderboard service: </p> <p>This option is not secure as it's susceptible to man-in-the-middle attacks. Players can put a proxy and change their score as they please.</p> <p>One additional caveat is that for games, where the game logic is managed by the server, cliets don't need to call the server explicitly to record their win.  Servers do it automatically for them based on the game logic.</p> <p>One additional consideration is whether we should put a message queue between the game server and the leaderboard service. This would be useful if other services are interested in game results, but that is not an explicit requirement in the interview so far, hence it's not included in the design: </p>"},{"location":"system-design-interview/chapter26/#data-models","title":"Data models","text":"<p>Let's discuss the options we have for storing leaderboard data - relational DBs, Redis, NoSQL.</p> <p>The NoSQL solution is discussed in the deep dive section.</p>"},{"location":"system-design-interview/chapter26/#relational-database-solution","title":"Relational database solution","text":"<p>If the scale doesn't matter and we don't have that many users, a relational DB serves our quite well.</p> <p>We can start from a simple leaderboard table, one for each month (personal note - this doesn't make sense. You can just add a <code>month</code> column and avoid the headache of maintaining new tables each month):  </p> <p>There is additional data to include in there, but that is irrelevant to the queries we'd run, so it's omitted.</p> <p>What happens when a user wins a point? </p> <p>If a user doesn't exist in the table yet, we need to insert them first: <pre><code>INSERT INTO leaderboard (user_id, score) VALUES ('mary1934', 1);\n</code></pre></p> <p>On subsequent calls, we'd just update their score: <pre><code>UPDATE leaderboard set score=score + 1 where user_id='mary1934';\n</code></pre></p> <p>How do we find the top players of a leaderboard? </p> <p>We can run the following query: <pre><code>SELECT (@rownum := @rownum + 1) AS rank, user_id, score\nFROM leaderboard\nORDER BY score DESC;\n</code></pre></p> <p>This is not performant though as it makes a table scan to order all records in the database table.</p> <p>We can optimize it by adding an index on <code>score</code> and using the <code>LIMIT</code> operation to avoid scanning everything: <pre><code>SELECT (@rownum := @rownum + 1) AS rank, user_id, score\nFROM leaderboard\nORDER BY score DESC\nLIMIT 10;\n</code></pre></p> <p>This approach, however, doesn't scale well if the user is not at the top of the leaderboard and you'd want to locate their rank.</p>"},{"location":"system-design-interview/chapter26/#redis-solution","title":"Redis solution","text":"<p>We want to find a solution, which works well even for millions of players without having to fallback on complex database queries.</p> <p>Redis is an in-memory data store, which is fast as it works in-memory and has a suitable data structure to serve our needs - sorted set.</p> <p>A sorted set is a data structure similar to sets in programming languages, which allows you to keep a data structure sorted by a given criteria. Internally, it is implemented using a hash-map to maintain mapping between key (user_id) and value (score) and a skip list which maps scores to users in sorted order: </p> <p>How does a skip list work?  * It is a linked list which allows for fast search  * It consists of a sorted linked list and multi-level indexes </p> <p>This structure enables us to quickly search for specific values when the data set is large enough. In the example below (64 nodes), it requires traversing 62 nodes in a base linked list to find the given value and 11 nodes in the skip-list case: </p> <p>Sorted sets are more performant than relational databases as the data is kept sorted at all times at the price of O(logN) add and find operation.</p> <p>In contract, here's an example nested query we need to run to find the rank of a given user in a relational DB: <pre><code>SELECT *,(SELECT COUNT(*) FROM leaderboard lb2\nWHERE lb2.score &gt;= lb1.score) RANK\nFROM leaderboard lb1\nWHERE lb1.user_id = {:user_id};\n</code></pre></p> <p>What operations do we need to operate our leaderboard in Redis?  * <code>ZADD</code> - insert the user into the set if they don't exist. Otherwise, update the score. O(logN) time complexity.  * <code>ZINCRBY</code> - increment the score of a user by given amount. If user doesn't exist, score starts at zero. O(logN) time complexity.  * <code>ZRANGE/ZREVRANGE</code> - fetch a range of users, sorted by their score. We can specify order (ASC/DESC), offset and result size. O(logN+M) time complexity where M is result size.  * <code>ZRANK/ZREVRANK</code> - Fetch the position (rank) of given user in ASC/DESC order. O(logN) time complexity.</p> <p>What happens when a user scores a point? <pre><code>ZINCRBY leaderboard_feb_2021 1 'mary1934'\n</code></pre></p> <p>There's a new leaderboard created every month while old ones are moved to historical storage.</p> <p>What happens when a user fetches top 10 players? <pre><code>ZREVRANGE leaderboard_feb_2021 0 9 WITHSCORES\n</code></pre></p> <p>Example result: <pre><code>[(user2,score2),(user1,score1),(user5,score5)...]\n</code></pre></p> <p>What about user fetching their leaderboard position? </p> <p>This can be easily achieved by the following query, given that we know a user's leaderboard position: <pre><code>ZREVRANGE leaderboard_feb_2021 357 365\n</code></pre></p> <p>A user's position can be fetched using <code>ZREVRANK &lt;user-id&gt;</code>.</p> <p>Let's explore what our storage requirements are:  * Assuming worst-case scenario of all 25mil MAU participating in the game for a given month  * ID is 24-character string and score is 16-bit integer, we need 26 bytes * 25mil = ~650MB of storage  * Even if we double the storage cost due to the overhead of the skip list, this would still easily fit in a modern redis cluster</p> <p>Another non-functional requirement to consider is supporting 2500 updates per second. This is well within a single Redis server's capabilities.</p> <p>Additional caveats:  * We can spin up a Redis replica to avoid losing data when a redis server crashes  * We can still leverage Redis persistence to not lose data in the event of a crash  * We'll need two supporting tables in MySQL to fetch user details such as username, display name, etc as well as store when eg a user won a game  * The second table in MySQL can be used to reconstruct leaderboard when there is an infrastructure failure  * As a small performance optimization, we could cache the user details of top 10 players as they'd be frequently accessed</p>"},{"location":"system-design-interview/chapter26/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":""},{"location":"system-design-interview/chapter26/#to-use-a-cloud-provider-or-not","title":"To use a cloud provider or not","text":"<p>We can either choose to deploy and manage our own services or use a cloud provider to manage them for us.</p> <p>If we choose to manage the services our selves, we'll use redis for leaderboard data, mysql for user profile and potentially a cache for user profile if we want to scale the database: </p> <p>Alternatively, we could use cloud offerings to manage a lot of the services for us. For example, we can use AWS API Gateway to route API calls to AWS Lambda functions: </p> <p>AWS Lambda enables us to run code without managing or provisioning servers ourselves. It runs only when needed and scales automatically.</p> <p>Exmaple user scoring a point: </p> <p>Example user retrieving leaderboard: </p> <p>Lambdas are an implementation of a serverless architecture. We don't need to manage scaling and environment setup.</p> <p>Author recommends going with this approach if we build the game from the ground up.</p>"},{"location":"system-design-interview/chapter26/#scaling-redis","title":"Scaling Redis","text":"<p>With 5mil DAU, we can get away with a single Redis instance from both a storage and QPS perspective.</p> <p>However, if we imagine userbase grows 10x to 500mil DAU, then we'd need 65gb for storage and QPS goes to 250k.</p> <p>Such scale would require sharding.</p> <p>One way to achieve it is by range-partitioning the data: </p> <p>In this example, we'll shard based on user's score. We'll maintain the mapping between user_id and shard in application code. We can do that either via MySQL or another cache for the mapping itself.</p> <p>To fetch the top 10 players, we'd query the shard with the highest scores (<code>[900-1000]</code>).</p> <p>To fetch a user's rank, we'll need to calculate the rank within the user's shard and add up all users with higher scores in other shards. The latter is a O(1) operation as total records per shard can quickly be accessed via the info keyspace command.</p> <p>Alternatively, we can use hash partitioning via Redis Cluster. It is a proxy which distributes data across redis nodes based on partitioning similar to consistent hashing, but not exactly the same: </p> <p>Calculating the top 10 players is challenging with this setup. We'll need to get the top 10 players of each shard and merge the results in the application: </p> <p>There are some limitations with the hash partitioning:  * If we need to fetch top K users, where K is high, latency can increase as we'll need to fetch a lot of data from all the shards  * Latency increases as the number of partitions grows  * There is no straightforward approach to determine a user's rank</p> <p>Due to all this, the author leans towards using fixed partitions for this problem.</p> <p>Other caveats:  * A best practice is to allocate twice as much memory as required for write-heavy redis nodes to accommodate snapshots if required  * We can use a tool called Redis-benchmark to track the performance of a redis setup and make data-driven decisions</p>"},{"location":"system-design-interview/chapter26/#alternative-solution-nosql","title":"Alternative solution: NoSQL","text":"<p>An alternative solution to consider is using an appropriate NoSQL database optimized for:  * heavy writes  * effectively sorting items within the same partition by score</p> <p>DynamoDB, Cassandra or MongoDB are all good fits.</p> <p>In this chapter, the author has decided to use DynamoDB. It is a fully-managed NoSQL database, which offers reliable performance and great scalability. It also enables usage of global secondary indexes when we need to query fields not part of the primary key. </p> <p>Let's start from a table for storing a leaderboard for a chess game: </p> <p>This works well, but doesn't scale well if we need to query anything by score. Hence, we can put the score as a sort key: </p> <p>Another problem with this design is that we're partitioning by month. This leads to a hotspot partition as the latest month will be unevenly accessed compared to the others.</p> <p>We could use a technique called write sharding, where we append a partition number for each key, calculated via <code>user_id % num_partitions</code>: </p> <p>An important trade-off to consider is how many partitions we should use:  * The more partitions there are, the higher the write scalability  * However, read scalability suffers as we need to query more partitions to collect aggregate results</p> <p>Using this approach requires that we use the \"scatter-gather\" technique we saw earlier, which grows in time complexity as we add more partitions: </p> <p>To make a good evaluation on the number of partitions, we'd need to do some benchmarking.</p> <p>This NoSQL approach still has one major downside - it is hard to calculate the specific rank of a user.</p> <p>If we have sufficient scale to require us to shard, we could then perhaps tell users what \"percentile\" of scores they're in.</p> <p>A cron job can periodically run to analyze score distributions, based on which a user's percentile is determined, eg: <pre><code>10th percentile = score &lt; 100\n20th percentile = score &lt; 500\n...\n90th percentile = score &lt; 6500\n</code></pre></p>"},{"location":"system-design-interview/chapter26/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Other things to discuss if time permits:  * Faster retrieval - We can cache the user object via a Redis hash with mapping <code>user_id -&gt; user object</code>. This enables faster retrieval vs. querying the database.  * Breaking ties - When two players have the same score, we can break the tie by sorting them based on last played game.  * System failure recovery - In the event of a large-scale Redis outage, we can recreate the leaderboard by going through the MySQL WAL entries and recreate it via an ad-hoc script</p>"},{"location":"system-design-interview/chapter27/","title":"Payment System","text":"<p>We'll design a payment system in this chapter, which underpins all of modern e-commerce.</p> <p>A payment system is used to settle financial transactions, transferring monetary value.</p>"},{"location":"system-design-interview/chapter27/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: What kind of payment system are we building?</li> <li>I: A payment backend for an e-commerce system, similar to Amazon.com. It handles everything related to money movement.</li> <li>C: What payment options are supported - Credit cards, PayPal, bank cards, etc?</li> <li>I: The system should support all these options in real life. For the purposes of the interview, we can use credit card payments.</li> <li>C: Do we handle credit card processing ourselves?</li> <li>I: No, we use a third-party provider like Stripe, Braintree, Square, etc.</li> <li>C: Do we store credit card data in our system?</li> <li>I: Due to compliance reasons, we do not store credit card data directly in our systems. We rely on third-party payment processors.</li> <li>C: Is the application global? Do we need to support different currencies and international payments?</li> <li>I: The application is global, but we assume only one currency is used for the purposes of the interview.</li> <li>C: How many payment transactions per day do we support?</li> <li>I: 1mil transactions per day.</li> <li>C: Do we need to support the payout flow to eg payout to payers each month?</li> <li>I: Yes, we need to support that</li> <li>C: Is there anything else I should pay attention to?</li> <li>I: We need to support reconciliations to fix any inconsistencies in communicating with internal and external systems.</li> </ul>"},{"location":"system-design-interview/chapter27/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Pay-in flow - payment system receives money from customers on behalf of merchants</li> <li>Pay-out flow - payment system sends money to sellers around the world</li> </ul>"},{"location":"system-design-interview/chapter27/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Reliability and fault-tolerance. Failed payments need to be carefully handled</li> <li>A reconciliation between internal and external systems needs to be setup.</li> </ul>"},{"location":"system-design-interview/chapter27/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>The system needs to process 1mil transactions per day, which is 10 transactions per second.</p> <p>This is not a high throughput for any database system, so it's not the focus of this interview.</p>"},{"location":"system-design-interview/chapter27/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>At a high-level, we have three actors, participating in money movement: </p>"},{"location":"system-design-interview/chapter27/#pay-in-flow","title":"Pay-in flow","text":"<p>Here's the high-level overview of the pay-in flow:   * Payment service - accepts payment events and coordinates the payment process. It typically also does a risk check using a third-party provider for AML violations or criminal activity.  * Payment executor - executes a single payment order via the Payment Service Provider (PSP). Payment events may contain several payment orders.  * Payment service provider (PSP) - moves money from one account to another, eg from buyer's credit card account to e-commerce site's bank account.  * Card schemes - organizations that process credit card operations, eg Visa MasterCard, etc.  * Ledger - keeps financial record of all payment transactions.  * Wallet - keeps the account balance for all merchants.</p> <p>Here's an example pay-in flow:  * user clicks \"place order\" and a payment event is sent to the payment service  * payment service stores the event in its database  * payment service calls the payment executor for all payment orders, part of that payment event  * payment executor stores the payment order in its database  * payment executor calls external PSP to process the credit card payment  * After the payment executor processes the payment, the payment service updates the wallet to record how much money the seller has  * wallet service stores updated balance information in its database  * payment service calls the ledger to record all money movements</p>"},{"location":"system-design-interview/chapter27/#apis-for-payment-service","title":"APIs for payment service","text":"<pre><code>POST /v1/payments\n{\n  \"buyer_info\": {...},\n  \"checkout_id\": \"some_id\",\n  \"credit_card_info\": {...},\n  \"payment_orders\": [{...}, {...}, {...}]\n}\n</code></pre> <p>Example <code>payment_order</code>: <pre><code>{\n  \"seller_account\": \"SELLER_IBAN\",\n  \"amount\": \"3.15\",\n  \"currency\": \"USD\",\n  \"payment_order_id\": \"globally_unique_payment_id\"\n}\n</code></pre></p> <p>Caveats:  * The <code>payment_order_id</code> is forwarded to the PSP to deduplicate payments, ie it is the idempotency key.  * The amount field is <code>string</code> as <code>double</code> is not appropriate for representing monetary values.</p> <pre><code>GET /v1/payments/{:id}\n</code></pre> <p>This endpoint returns the execution status of a single payment, based on the <code>payment_order_id</code>.</p>"},{"location":"system-design-interview/chapter27/#payment-service-data-model","title":"Payment service data model","text":"<p>We need to maintain two tables - <code>payment_events</code> and <code>payment_orders</code>.</p> <p>For payments, performance is typically not an important factor. Strong consistency, however, is.</p> <p>Other considerations for choosing the database:  * Strong market of DBAs to hire to administer the databaseS  * Proven track-record where the database has been used by other big financial institutions  * Richness of supporting tools  * Traditional SQL over NoSQL/NewSQL for its ACID guarantees</p> <p>Here's what the <code>payment_events</code> table contains:  * <code>checkout_id</code> - string, primary key  * <code>buyer_info</code> - string (personal note - prob a foreign key to another table is more appropriate)  * <code>seller_info</code> - string (personal note - same remark as above)  * <code>credit_card_info</code> - depends on card provider  * <code>is_payment_done</code> - boolean</p> <p>Here's what the <code>payment_orders</code> table contains:  * <code>payment_order_id</code> - string, primary key  * <code>buyer_account</code> - string  * <code>amount</code> - string  * <code>currency</code> - string  * <code>checkout_id</code> - string, foreign key  * <code>payment_order_status</code> - enum (<code>NOT_STARTED</code>, <code>EXECUTING</code>, <code>SUCCESS</code>, <code>FAILED</code>)  * <code>ledger_updated</code> - boolean  * <code>wallet_updated</code> - boolean</p> <p>Caveats:  * there are many payment orders, linked to a given payment event  * we don't need the <code>seller_info</code> for the pay-in flow. That's required on pay-out only  * <code>ledger_updated</code> and <code>wallet_updated</code> are updated when the respective service is called to record the result of a payment  * payment transitions are managed by a background job, which checks updates of in-flight payments and triggers an alert if a payment is not processed in a reasonable timeframe</p>"},{"location":"system-design-interview/chapter27/#double-entry-ledger-system","title":"Double-entry ledger system","text":"<p>The double-entry accounting mechanism is key to any payment system. It is a mechanism of tracking money movements by always applying money operations to two accounts, where one's account balance increases (credit) and the other decreases (debit): | Account | Debit | Credit | |---------|-------|--------| | buyer   | $1    |        | | seller  |       | $1     |</p> <p>Sum of all transaction entries is always zero. This mechanism provides end-to-end traceability of all money movements within the system.</p>"},{"location":"system-design-interview/chapter27/#hosted-payment-page","title":"Hosted payment page","text":"<p>To avoid storing credit card information and having to comply with various heavy regulations, most companies prefer utilizing a widget, provided by PSPs, which store and handle credit card payments for you: </p>"},{"location":"system-design-interview/chapter27/#pay-out-flow","title":"Pay-out flow","text":"<p>The components of the pay-out flow are very similar to the pay-in flow.</p> <p>Main differences:  * money is moved from e-commerce site's bank account to merchant's bank account  * we can utilize a third-party account payable provider such as Tipalti  * There's a lot of bookkeeping and regulatory requirements to handle with regards to pay-outs as well</p>"},{"location":"system-design-interview/chapter27/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>This section focuses on making the system faster, more robust and secure.</p>"},{"location":"system-design-interview/chapter27/#psp-integration","title":"PSP Integration","text":"<p>If our system can directly connect to banks or card schemes, payment can be made without a PSP. These kinds of connections are very rare and uncommon, typically done at large companies which can justify the investment.</p> <p>If we go down the traditional route, a PSP can be integrated in one of two ways:  * Through API, if our payment system can collect payment information  * Through a hosted payment page to avoid dealing with payment information regulations</p> <p>Here's how the hosted payment page workflow works:   * User clicks \"checkout\" button in the browser  * Client calls the payment service with the payment order information  * After receiving payment order information, the payment service sends a payment registration request to the PSP.  * The PSP receives payment info such as currency, amount, expiration, etc, as well as a UUID for idempotency purposes. Typically the UUID of the payment order.  * The PSP returns a token back which uniquely identifies the payment registration. The token is stored in the payment service database.  * Once token is stored, the user is served with a PSP-hosted payment page. It is initialized using the token as well as a redirect URL for success/failure.   * User fills in payment details on the PSP page, PSP processes payment and returns the payment status  * User is now redirected back to the redirectURL. Example redirect url - <code>https://your-company.com/?tokenID=JIOUIQ123NSF&amp;payResult=X324FSa</code>  * Asynchronously, the PSP calls our payment service via a webhook to inform our backend of the payment result  * Payment service records the payment result based on the webhook received</p>"},{"location":"system-design-interview/chapter27/#reconciliation","title":"Reconciliation","text":"<p>The previous section explains the happy path of a payment. Unhappy paths are detected and reconciled using a background reconciliation process.</p> <p>Every night, the PSP sends a settlement file which our system uses to compare the external system's state against our internal system's state. </p> <p>This process can also be used to detect internal inconsistencies between eg the ledger and the wallet services.</p> <p>Mismatches are handled manually by the finance team. Mismatches are handled as:  * classifiable, hence, it is a known mismatch which can be adjusted using a standard procedure  * classifiable, but can't be automated. Manually adjusted by the finance team  * unclassifiable. Manually investigated and adjusted by the finance team</p>"},{"location":"system-design-interview/chapter27/#handling-payment-processing-delays","title":"Handling payment processing delays","text":"<p>There are cases, where a payment can take hours to complete, although it typically takes seconds.</p> <p>This can happen due to:  * a payment being flagged as high-risk and someone has to manually review it  * credit card requires extra protection, eg 3D Secure Authentication, which requires extra details from card holder to complete</p> <p>These situations are handled by:  * waiting for the PSP to send us a webhook when a payment is complete or polling its API if the PSP doesn't provide webhooks  * showing a \"pending\" status to the user and giving them a page, where they can check-in for payment updates. We could also send them an email once their payment is complete</p>"},{"location":"system-design-interview/chapter27/#communication-among-internal-services","title":"Communication among internal services","text":"<p>There are two types of communication patterns services use to communicate with one another - synchronous and asynchronous.</p> <p>Synchronous communication (ie HTTP) works well for small-scale systems, but suffers as scale increases:  * low performance - request-response cycle is long as more services get involved in the call chain  * poor failure isolation - if PSPs or any other service fails, user will not receive a response  * tight coupling - sender needs to know the receiver  * hard to scale - not easy to support sudden increase in traffic due to not having a buffer</p> <p>Asynchronous communication can be divided into two categories.</p> <p>Single receiver - multiple receivers subscribe to the same topic and messages are processed only once: </p> <p>Multiple receivers - multiple receivers subscribe to the same topic, but messages are forwarded to all of them: </p> <p>Latter model works well for our payment system as a payment can trigger multiple side effects, handled by different services.</p> <p>In a nutshell, synchronous communication is simpler but doesn't allow services to be autonomous.  Async communication trades simplicity and consistency for scalability and resilience.</p>"},{"location":"system-design-interview/chapter27/#handling-failed-payments","title":"Handling failed payments","text":"<p>Every payment system needs to address failed payments. Here are some of the mechanism we'll use to achieve that:  * Tracking payment state - whenever a payment fails, we can determine whether to retry/refund based on the payment state.  * Retry queue - payments which we'll retry are published to a retry queue  * Dead-letter queue - payments which have terminally failed are pushed to a dead-letter queue, where the failed payment can be debugged and inspected. </p>"},{"location":"system-design-interview/chapter27/#exactly-once-delivery","title":"Exactly-once delivery","text":"<p>We need to ensure a payment gets processed exactly-once to avoid double-charging a customer.</p> <p>An operation is executed exactly-once if it is executed at-least-once and at-most-once at the same time.</p> <p>To achieve the at-least-once guarantee, we'll use a retry mechanism: </p> <p>Here are some common strategies on deciding the retry intervals:  * immediate retry - client immediately sends another request after failure  * fixed intervals - wait a fixed amount of time before retrying a payment  * incremental intervals - incrementally increase retry interval between each retry  * exponential back-off - double retry interval between subsequent retries  * cancel - client cancels the request. This happens when the error is terminal or retry threshold is reached</p> <p>As a rule of thumb, default to an exponential back-off retry strategy. A good practice is for the server to specify a retry interval using a <code>Retry-After</code> header.</p> <p>An issue with retries is that the server can potentially process a payment twice:  * client clicks the \"pay button\" twice, hence, they are charged twice  * payment is successfully processed by PSP, but not by downstream services (ledger, wallet). Retry causes the payment to be processed by the PSP again</p> <p>To address the double payment problem, we need to use an idempotency mechanism - a property that an operation applied multiple times is processed only once.</p> <p>From an API perspective, clients can make multiple calls which produce the same result.  Idempotency is managed by a special header in the request (eg <code>idempotency-key</code>), which is typically a UUID. </p> <p>Idempotency can be achieved using the database's mechanism of adding unique key constraints:  * server attempts to insert a new row in the database  * the insertion fails due to a unique key constraint violation  * server detects that error and instead returns the existing object back to the client</p> <p>Idempotency is also applied at the PSP side, using the nonce, which was previously discussed. PSPs will take care to not process payments with the same nonce twice.</p>"},{"location":"system-design-interview/chapter27/#consistency","title":"Consistency","text":"<p>There are several stateful services called throughout a payment's lifecycle - PSP, ledger, wallet, payment service.</p> <p>Communication between any two services can fail.  We can ensure eventual data consistency between all services by implementing exactly-once processing and reconciliation.</p> <p>If we use replication, we'll have to deal with replication lag, which can lead to users observing inconsistent data between primary and replica databases.</p> <p>To mitigate that, we can serve all reads and writes from the primary database and only utilize replicas for redundancy and fail-over. Alternatively, we can ensure replicas are always in-sync by utilizing a consensus algorithm such as Paxos or Raft. We could also use a consensus-based distributed database such as YugabyteDB or CockroachDB.</p>"},{"location":"system-design-interview/chapter27/#payment-security","title":"Payment security","text":"<p>Here are some mechanisms we can use to ensure payment security:  * Request/response eavesdropping - we can use HTTPS to secure all communication  * Data tampering - enforce encryption and integrity monitoring  * Man-in-the-middle attacks - use SSL \\w certificate pinning  * Data loss - replicate data across multiple regions and take data snapshots  * DDoS attack - implement rate limiting and firewall  * Card theft - use tokens instead of storing real card information in our system  * PCI compliance - a security standard for organizations which handle branded credit cards  * Fraud - address verification, card verification value (CVV), user behavior analysis, etc</p>"},{"location":"system-design-interview/chapter27/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Other talking points:  * Monitoring and alerting  * Debugging tools - we need tools which make it easy to understand why a payment has failed  * Currency exchange - important when designing a payment system for international use  * Geography - different regions might have different payment methods  * Cash payment - very common in places like India and Brazil  * Google/Apple Pay integration</p>"},{"location":"system-design-interview/chapter28/","title":"Digital Wallet","text":"<p>Payment platforms usually have a wallet service, where they allow clients to store funds within the application, which they can withdraw later.</p> <p>You can also use it to pay for goods &amp; services or transfer money to other users, who use the digital wallet service. That can be faster and cheaper than doing it via normal payment rails. </p>"},{"location":"system-design-interview/chapter28/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: Should we only focus on transfers between digital wallets? Should we support any other operations?</li> <li>I: Let's focus on transfers between digital wallets for now.</li> <li>C: How many transactions per second does the system need to support?</li> <li>I: Let's assume 1mil TPS</li> <li>C: A digital wallet has strict correctness requirements. Can we assume transactional guarantees are sufficient?</li> <li>I: Sounds good</li> <li>C: Do we need to prove correctness?</li> <li>I: We can do that via reconciliation, but that only detects discrepancies vs. showing us the root cause for them. Instead, we want to be able to replay data from the beginning to reconstruct the history.</li> <li>C: Can we assume availability requirement is 99.99%?</li> <li>I: Yes</li> <li>C: Do we need to take foreign exchange into consideration?</li> <li>I: No, it's out of scope</li> </ul> <p>Here's what we have to support in summary:  * Support balance transfers between two accounts  * Support 1mil TPS  * Reliability is 99.99%  * Support transactions  * Support reproducibility</p>"},{"location":"system-design-interview/chapter28/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>A traditional relational database, provisioned in the cloud can support ~1000 TPS.</p> <p>In order to reach 1mil TPS, we'd need 1000 database nodes. But if each transfer has two legs, then we actually need to support 2mil TPS.</p> <p>One of our design goals would be to increase the TPS a single node can handle so that we can have less database nodes. | Per-node TPS | Node Number | |--------------|-------------| | 100          | 20,000      | | 1,000        | 2,000       | | 10,000       | 200         |</p>"},{"location":"system-design-interview/chapter28/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"system-design-interview/chapter28/#api-design","title":"API Design","text":"<p>We only need to support one endpoint for this interview: <pre><code>POST /v1/wallet/balance_transfer - transfers balance from one wallet to another\n</code></pre></p> <p>Request parameters - from_account, to_account, amount (string to not lose precision), currency, transaction_id (idempotency key).</p> <p>Sample response: <pre><code>{\n    \"status\": \"success\"\n    \"transaction_id\": \"01589980-2664-11ec-9621-0242ac130002\"\n}\n</code></pre></p>"},{"location":"system-design-interview/chapter28/#in-memory-sharding-solution","title":"In-memory sharding solution","text":"<p>Our wallet application maintains account balances for every user account.</p> <p>One good data structure to represent this is a <code>map&lt;user_id, balance&gt;</code>, which can be implemented using an in-memory Redis store.</p> <p>Since one redis node cannot withstand 1mil TPS, we need to partition our redis cluster into multiple nodes.</p> <p>Example partitioning algorithm: <pre><code>String accountID = \"A\";\nInt partitionNumber = 7;\nInt myPartition = accountID.hashCode() % partitionNumber;\n</code></pre></p> <p>Zookeeper can be used to store the number of partitions and addresses of redis nodes as it's a highly-available configuration storage. </p> <p>Finally, a wallet service is a stateless service responsible for carrying out transfer operations. It can easily scale horizontally: </p> <p>Although this solution addresses scalability concerns, it doesn't allow us to execute balance transfers atomically.</p>"},{"location":"system-design-interview/chapter28/#distributed-transactions","title":"Distributed transactions","text":"<p>One approach for handling transactions is to use the two-phase commit protocol on top of standard, sharded relational databases: </p> <p>Here's how the two-phase commit (2PC) protocol works:   * Coordinator (wallet service) performs read and write operations on multiple databases as normal  * When application is ready to commit the transaction, coordinator asks all databases to prepare it  * If all databases replied with a \"yes\", then the coordinator asks the databases to commit the transaction.  * Otherwise, all databases are asked to abort the transaction</p> <p>Downsides to the 2PC approach:  * Not performant due to lock contention  * The coordinator is a single point of failure</p>"},{"location":"system-design-interview/chapter28/#distributed-transaction-using-try-confirmcancel-tcc","title":"Distributed transaction using Try-Confirm/Cancel (TC/C)","text":"<p>TC/C is a variation of the 2PC protocol, which works with compensating transactions:  * Coordinator asks all databases to reserve resources for the transaction  * Coordinator collects replies from DBs - if yes, DBs are asked to try-confirm. If no, DBs are asked to try-cancel.</p> <p>One important difference between TC/C and 2PC is that 2PC performs a single transaction, whereas in TC/C, there are two independent transactions.</p> <p>Here's how TC/C works in phases: | Phase | Operation | A                   | C                   | |-------|-----------|---------------------|---------------------| | 1     | Try       | Balance change: -$1 | Do nothing          | | 2     | Confirm   | Do nothing          | Balance change: +$1 | |       | Cancel    | Balance change: +$1 | Do Nothing          |</p> <p>Phase 1 - try:   * coordinator starts local transaction in A's DB to reduce A's balance by 1$  * C's DB is given a NOP instruction, which does nothing</p> <p>Phase 2a - confirm:   * if both DBs replied with \"yes\", confirm phase starts.  * A's DB receives NOP, whereas C's DB is instructed to increase C's balance by 1$ (local transaction)</p> <p>Phase 2b - cancel:   * If any of the operations in phase 1 fails, the cancel phase starts.  * A's DB is instructed to increase A's balance by 1$, C's DB receives NOP</p> <p>Here's a comparison between 2PC and TC/C: |      | First Phase                                            | Second Phase: success              | Second Phase: fail                        | |------|--------------------------------------------------------|------------------------------------|-------------------------------------------| | 2PC  | transactions are not done yet                          | Commit/Cancel all transactions     | Cancel all transactions                   | | TC/C | All transactions are completed - committed or canceled | Execute new transactions if needed | Reverse the already committed transaction |</p> <p>TC/C is also referred to as a distributed transaction by compensation. High-level operation is handled in the business logic.</p> <p>Other properties of TC/C:  * database agnostic, as long as database supports transactions  * Details and complexity of distributed transactions need to be handled in the business logic</p>"},{"location":"system-design-interview/chapter28/#tcc-failure-modes","title":"TC/C Failure modes","text":"<p>If the coordinator dies mid-flight, it needs to recover its intermediary state.  That can be done by maintaining phase status tables, atomically updated within the database shards: </p> <p>What does that table contain:  * ID and content of distributed transaction  * status of try phase - not sent, has been sent, response received  * second phase name - confirm or cancel  * status of second phase  * out-of-order flag (explained later)</p> <p>One caveat when using TC/C is that there is a brief moment where the account states are inconsistent with each other while a distributed transaction is in-flight: </p> <p>This is fine as long as we always recover from this state and that users cannot use the intermediary state to eg spend it.  This is guaranteed by always executing deductions prior to additions. | Try phase choices  | Account A | Account C | |--------------------|-----------|-----------| | Choice 1           | -$1       | NOP       | | Choice 2 (invalid) | NOP       | +$1       | | Choice 3 (invalid) | -$1       | +$1       |</p> <p>Note that choice 3 from table above is invalid because we cannot guarantee atomic execution of transactions across different databases without relying on 2PC.</p> <p>One edge-case to address is out of order execution: </p> <p>It is possible that a database receives a cancel operation, before receiving a try. This edge case can be handled by adding an out of order flag in our phase status table. When we receive a try operation, we first check if the out of order flag is set and if so, a failure is returned.</p>"},{"location":"system-design-interview/chapter28/#distributed-transaction-using-saga","title":"Distributed transaction using Saga","text":"<p>Another popular approach is using Sagas - a standard for implementing distributed transactions with microservice architectures.</p> <p>Here's how it works:  * all operations are ordered in a sequence. All operations are independent in their own databases.  * operations are executed from first to last  * when an operation fails, the entire process starts to roll back until the beginning with compensating operations </p> <p>How do we coordinate the workflow? There are two approaches we can take:  * Choreography - all services involved in a saga subscribe to the related events and do their part in the saga  * Orchestration - a single coordinator instructs all services to do their jobs in the correct order</p> <p>The challenge of using choreography is that business logic is split across multiple service, which communicate asynchronously. The orchestration approach handles complexity well, so it is typically the preferred approach in a digital wallet system.</p> <p>Here's a comparison between TC/C and Saga: |                                           | TC/C            | Saga                     | |-------------------------------------------|-----------------|--------------------------| | Compensating action                       | In Cancel phase | In rollback phase        | | Central coordination                      | Yes             | Yes (orchestration mode) | | Operation execution order                 | any             | linear                   | | Parallel execution possibility            | Yes             | No (linear execution)    | | Could see the partial inconsistent status | Yes             | Yes                      | | Application or database logic             | Application     | Application              |</p> <p>The main difference is that TC/C is parallelizable, so our decision is based on the latency requirement - if we need to achieve low latency, we should go for the TC/C approach.</p> <p>Regardless of the approach we take, we still need to support auditing and replaying history to recover from failed states.</p>"},{"location":"system-design-interview/chapter28/#event-sourcing","title":"Event sourcing","text":"<p>In real-life, a digital wallet application might be audited and we have to answer certain questions:  * Do we know the account balance at any given time?  * How do we know the historical and current balances are correct?  * How do we prove the system logic is correct after a code change?</p> <p>Event sourcing is a technique which helps us answer these questions.</p> <p>It consists of four concepts:  * command - intended action from the real world, eg transfer 1$ from account A to B. Need to have a global order, due to which they're put into a FIFO queue.    * commands, unlike events, can fail and have some randomness due to eg IO or invalid state.    * commands can produce zero or more events    * event generation can contain randomness such as external IO. This will be revisited later  * event - historical facts about events which occured in the system, eg \"transferred 1$ from A to B\".    * unlike commands, events are facts that have happened within our system    * similar to commands, they need to be ordered, hence, they're enqueued in a FIFO queue  * state - what has changed as a result of an event. Eg a key-value store between account and their balances.  * state machine - drives the event sourcing process. It mainly validates commands and applies events to update the system state.    * the state machine should be deterministic, hence, it shouldn't read external IO or rely on randomness.  </p> <p>Here's a dynamic view of event sourcing: </p> <p>For our wallet service, the commands are balance transfer requests. We can put them in a FIFO queue, such as Kafka: </p> <p>Here's the full picture:   * state machine reads commands from the command queue  * balance state is read from the database  * command is validated. If valid, two events for each of the accounts is generated  * next event is read and applied by updating the balance (state) in the database</p> <p>The main advantage of using event sourcing is its reproducibility. In this design, all state update operations are saved as immutable history of all balance changes.</p> <p>Historical balances can always be reconstructed by replaying events from the beginning.  Because the event list is immutable and the state machine is deterministic, we are guaranteed to succeed in replaying any of the intermediary states. </p> <p>All audit-related questions asked in the beginning of the section can be addressed by relying on event sourcing:  * Do we know the account balance at any given time? - events can be replayed from the start until the point which we are interested in  * How do we know the historical and current balances are correct? - correctness can be verified by recalculating all events from the start  * How do we prove the system logic is correct after a code change? - we can run different versions of the code against the events and verify their results are identical</p> <p>Answering client queries about their balance can be addressed using the CQRS architecture - there can be multiple read-only state machines which are responsible for querying the historical state, based on the immutable events list: </p>"},{"location":"system-design-interview/chapter28/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>In this section we'll explore some performance optimizations as we're still required to scale to 1mil TPS.</p>"},{"location":"system-design-interview/chapter28/#high-performance-event-sourcing","title":"High-performance event sourcing","text":"<p>The first optimization we'll explore is to save commands and events into local disk store instead of an external store such as Kafka.</p> <p>This avoids the network latency and also, since we're only doing appends, that operation is generally fast for HDDs.</p> <p>The next optimization is to cache recent commands and events in-memory in order to save the time of loading them back from disk.</p> <p>At a low-level, we can achieve the aforementioned optimizations by leveraging a command called mmap, which stores data in local disk as well as cache it in-memory: </p> <p>The next optimization we can do is also store state in the local file system using SQLite - a file-based local relational database. RocksDB is also another good option.</p> <p>For our purposes, we'll choose RocksDB because it uses a log-structured merge-tree (LSM), which is optimized for write operations. Read performance is optimized via caching. </p> <p>To optimize the reproducibility, we can periodically save snapshots to disk so that we don't have to reproduce a given state from the very beginning every time. We could store snapshots as large binary files in distributed file storage, eg HDFS: </p>"},{"location":"system-design-interview/chapter28/#reliable-high-performance-event-sourcing","title":"Reliable high-performance event sourcing","text":"<p>All the optimizations done so far are great, but they make our service stateful. We need to introduce some form of replication for reliability purposes.</p> <p>Before we do that, we should analyze what kind of data needs high reliability in our system:  * state and snapshot can always be regenerated by reproducing them from the events list. Hence, we only need to guarantee the event list reliability.  * one might think we can always regenerate the events list from the command list, but that is not true, since commands are non-deterministic.  * conclusion is that we need to ensure high reliability for the events list only</p> <p>In order to achieve high reliability for events, we need to replicate the list across multiple nodes. We need to guarantee:  * that there is no data loss  * the relative order of data within a log file remains the same across replicas</p> <p>To achieve this, we can employ a consensus algorithm, such as Raft.</p> <p>With Raft, there is a leader who is active and there are followers who are passive. If a leader dies, one of the followers picks up.  As long as more than half of the nodes are up, the system continues running. </p> <p>With this approach, all nodes update the state, based on the events list. Raft ensures leader and followers have the same events list.</p>"},{"location":"system-design-interview/chapter28/#distributed-event-sourcing","title":"Distributed event sourcing","text":"<p>So far, we've managed to design a system which has high single-node performance and is reliable.</p> <p>Some limitations we have to tackle:  * The capacity of a single raft group is limited. At some point, we need to shard the data and implement distributed transactions  * In the CQRS architecture, the request/response flow is slow. A client would need to periodically poll the system to learn when their wallet has been updated</p> <p>Polling is not real-time, hence, it can take a while for a user to learn about an update in their balance. Also, it can overload the query services if the polling frequency is too high: </p> <p>To mitigate the system load, we can introduce a reverse proxy, which sends commands on behalf of the user and polls for response on their behalf: </p> <p>This alleviates the system load as we could fetch data for multiple users using a single request, but it still doesn't solve the real-time receipt requirement.</p> <p>One final change we could do is make the read-only state machines push responses back to the reverse proxy once it's available. This can give the user the sense that updates happen real-time: </p> <p>Finally, to scale the system even further, we can shard the system into multiple raft groups, where we implement distributed transactions on top of them using an orchestrator either via TC/C or Sagas: </p> <p>Here's an example lifecycle of a balance transfer request in our final system:  * User A sends a distributed transaction to the Saga coordinator with two operations - <code>A-1</code> and <code>C+1</code>.  * Saga coordinator creates a record in the phase status table to trace the status of the transaction  * Coordinator determines which partitions it needs to send commands to.  * Partition 1's raft leader receives the <code>A-1</code> command, validates it, converts it to an event and replicates it across other nodes in the raft group  * Event result is synchronized to the read state machine, which pushes a response back to the coordinator  * Coordinator creates a record indicating that the operation was successful and proceeds with the next operation - <code>C+1</code>  * Next operation is executed similarly to the first one - partition is determined, command is sent, executed, read state machine pushes back a response  * Coordinator creates a record indicating operation 2 was also successful and finally informs the client of the result</p>"},{"location":"system-design-interview/chapter28/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Here's the evolution of our design:  * We started from a solution using an in-memory Redis. The problem with this approach is that it is not durable storage.  * We moved on to using relational databases, on top of which we execute distributed transactions using 2PC, TC/C or distributed saga.  * Next, we introduced event sourcing in order to make all the operations auditable  * We started by storing the data into external storage using external database and queue, but that's not performant  * We proceeded to store data in local file storage, leveraging the performance of append-only operations. We also used caching to optimize the read path  * The previous approach, although performant, wasn't durable. Hence, we introduced Raft consensus with replication to avoid single points of failure  * We also adopted CQRS with a reverse proxy to manage a transaction's lifecycle on behalf of our users  * Finally, we partitioned our data across multiple raft groups, which are orchestrated using a distributed transaction mechanism - TC/C or distributed saga</p>"},{"location":"system-design-interview/chapter29/","title":"Stock Exchange","text":"<p>We'll design an electronic stock exchange in this chapter.</p> <p>Its basic function is to efficiently match buyers and sellers.</p> <p>Major stock exchanges are NYSE, NASDAQ, among others. </p>"},{"location":"system-design-interview/chapter29/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design scope","text":"<ul> <li>C: Which securities are we going to trade? Stocks, options or futures?</li> <li>I: Only stocks for simplicity</li> <li>C: Which order types are supported - place, cancel, replace? What about limit, market, conditional orders?</li> <li>I: We need to support placing and canceling an order. We need to only consider limit orders for the order type.</li> <li>C: Does the system need to support after hours trading?</li> <li>I: No, just normal trading hours</li> <li>C: Could you describe the exchange's basic functions?</li> <li>I: Clients can place or cancel limit orders and receive matched trades in real-time. They should be able to see the order book in real time.</li> <li>C: What's the scale of the exchange?</li> <li>I: Tens of thousands of users trading at the same time and ~100 symbols. Billions of orders per day. We need to also support risk checks for compliance.</li> <li>C: What kind of risk checks?</li> <li>I: Let's do simple risk checks - eg limiting a user to trade only 1mil apple stocks in a day</li> <li>C: How about user wallet engagement?</li> <li>I: We need to ensure clients have sufficient funds before placing orders. Funds meant for pending orders need to be withheld until order is finalized.</li> </ul>"},{"location":"system-design-interview/chapter29/#non-functional-requirements","title":"Non-functional requirements","text":"<p>The scale mentioned by the interviewer hints that we are to design a small to medium scale exchange. We need to also ensure flexibility to support more symbols and users in the future.</p> <p>Other non-functional requirements:  * Availability - At least 99.99%. Downtime can harm reputation  * Fault tolerance - fault tolerance and a fast recovery mechanism are needed to limit the impact of a production incident  * Latency - Round-trip latency should be in the ms level with focus on 99th percentile. Persistently high 99p latency causes bad experience for a handful or users.  * Security - We should have an account management system. For legal compliance, we need to support KYC to verify user identity. We should also protect against DDoS for public resources.</p>"},{"location":"system-design-interview/chapter29/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>100 symbols, 1bil orders per day</li> <li>Normal trading hours are from 09:30 to 16:00 (6.5h)</li> <li>QPS = 1bil / 6.5 / 3600 = 43000</li> <li>Peak QPS = 5*QPS = 215000</li> <li>Trading volume is significantly higher when the market opens</li> </ul>"},{"location":"system-design-interview/chapter29/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"system-design-interview/chapter29/#business-knowledge-101","title":"Business Knowledge 101","text":"<p>Let's discuss some basic concepts, related to an exchange.</p> <p>A broker mediates interactions between an exchange and end users - Robinhood, Fidelity, etc.</p> <p>Institutional clients trade in large quantities using specialized trading software. They need specialized treatment. Eg order splitting when trading in large volumes to avoid impacting the market.</p> <p>Types of orders:  * Limit - buy or sell at a fixed price. It might not find a match immediately or it might be partially matched.  * Market - doesn't specify a price. Executed at the current market price immediately.</p> <p>Prices:  * Bid - highest price a buyer is willing to buy a stock  * Ask - lowest price a seller is willing to sell a stock</p> <p>The US market has three tiers of price quotes - L1, L2, L3.</p> <p>L1 market data contains best bid/ask prices and quantities: </p> <p>L2 includes more price levels: </p> <p>L3 shows levels and queued quantity at each level: </p> <p>A candlestick shows the market open and close price, as well as the highest and lowest prices in the given interval: </p> <p>FIX is a protocol for exchanging securities transaction information, used by most vendors. Example securities transaction: <pre><code>8=FIX.4.2 | 9=176 | 35=8 | 49=PHLX | 56=PERS | 52=20071123-05:30:00.000 | 11=ATOMNOCCC9990900 | 20=3 | 150=E | 39=E | 55=MSFT | 167=CS | 54=1 | 38=15 | 40=2 | 44=15 | 58=PHLX EQUITY TESTING | 59=0 | 47=C | 32=0 | 31=0 | 151=15 | 14=0 | 6=0 | 10=128 |\n</code></pre></p>"},{"location":"system-design-interview/chapter29/#high-level-design","title":"High-level design","text":"<p>Trade flow:  * Client places order via trading interface  * Broker sends the order to the exchange  * Order enters exchange through client gateway, which validates, rate limits, authenticates, etc. Order is forwarded to order manager.  * Order manager performs risk checks based on rules set by the risk manager  * After passing risk checks, order manager verifies there are sufficient funds in the wallet for the order  * Order is sent to matching engine. When match is found, matching engine emits two executions (called fills) for buy and sell. Both orders are sequenced so that they're deterministic.  * Executions are returned to the client.</p> <p>Market data flow (M1-M3):  * matching engine generates a stream of executions, sent to the market data publisher  * Market data publisher constructs the candlestick charts and sends them to the data service  * Market data is stored in specialized storage for real-time analytics. Brokers connect to the data service for timely market data.</p> <p>Reporter flow (R1-R2):  * reporter collects all necessary reporting fields from orders and executions and writes them to DB  * reporting fields - client_id, price, quantity, order_type, filled_quantity, remaining_quantity</p> <p>Trading flow is on the critical path, whereas the rest of the flows are not, hence, latency requirements differ between them.</p>"},{"location":"system-design-interview/chapter29/#trading-flow","title":"Trading flow","text":"<p>The trading flow is on the critical path, hence, it should be highly optimized for low latency.</p> <p>The matching engine is at its heart, also called the cross engine. Primary responsibilities:  * Maintain the order book for each symbol - a list of buy/sell orders for a symbol.  * Match buy and sell orders - a match results in two executions (fills), with one each for the buy and sell sides. This function must be fast and accurate  * Distribute the execution stream as market data  * Matches must be produced in a deterministic order. Foundational for high availability</p> <p>Next is the sequencer - it is the key component making the matching engine deterministic by stamping each inbound order and outbound fill with a sequence ID. </p> <p>We stamp inbound orders and outbound fills for several reasons:  * timeliness and fairness  * fast recovery/replay  * exactly-once guarantee</p> <p>Conceptually, we could use Kafka as our sequencer since it's effectively an inbound and outbound message queue. However, we're going to implement it ourselves in order to achieve lower latency.</p> <p>The order manager manages the orders state. It also interacts with the matching engine - sending orders and receiving fills.</p> <p>The order manager's responsibilities:  * Sends orders for risk checks - eg verifying user's trade volume is less than 1mil  * Checks the order against the user wallet and verifies there are sufficient funds to execute it  * It sends the order to the sequencer and on to the matching engine. To reduce bandwidth, only necessary order information is passed to the matching engine  * Executions (fills) are received back from the sequencer, where they are then send to the brokers via the client gateway</p> <p>The main challenge with implementing the order manager is the state transition management. Event sourcing is one viable solution (discussed in deep dive).</p> <p>Finally, the client gateway receives orders from users and sends them to the order manager. Its responsibilities: </p> <p>Since the client gateway is on the critical path, it should stay lightweight.</p> <p>There can be multiple client gateways for different clients. Eg a colo engine is a trading engine server, rented by the broker in the exchange's data center: </p>"},{"location":"system-design-interview/chapter29/#market-data-flow","title":"Market data flow","text":"<p>The market data publisher receives executions from the matching engine and builds the order book/candlestick charts from the execution stream.</p> <p>That data is sent to the data service, which is responsible for showing the aggregated data to subscribers: </p>"},{"location":"system-design-interview/chapter29/#reporting-flow","title":"Reporting flow","text":"<p>The reporter is not on the critical path, but it is an important component nevertheless. </p> <p>It is responsible for trading history, tax reporting, compliance reporting, settlements, etc. Latency is not a critical requirement for the reporting flow. Accuracy and compliance are more important.</p>"},{"location":"system-design-interview/chapter29/#api-design","title":"API Design","text":"<p>Clients interact with the stock exchange via the brokers to place orders, view executions, market data, download historical data for analysis, etc.</p> <p>We use a RESTful API for communication between the client gateway and the brokers.</p> <p>For institutional clients, a proprietary protocol is used to satisfy their low-latency requirements.</p> <p>Create order: <pre><code>POST /v1/order\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * side - buy or sell. String  * price - the price of the limit order. Long  * orderType - limit or market (we only support limit orders in our design). String  * quantity - the quantity of the order. Long</p> <p>Response:  * id - the ID of the order. Long  * creationTime - the system creation time of the order. Long  * filledQuantity - the quantity that has been successfully executed. Long  * remainingQuantity - the quantity still to be executed. Long  * status - new/canceled/filled. String  * rest of the attributes are the same as the input parameters</p> <p>Get execution: <pre><code>GET /execution?symbol={:symbol}&amp;orderId={:orderId}&amp;startTime={:startTime}&amp;endTime={:endTime}\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * orderId - the ID of the order. Optional. String  * startTime - query start time in epoch [11]. Long  * endTime - query end time in epoch. Long</p> <p>Response:  * executions - array with each execution in scope (see attributes below). Array  * id - the ID of the execution. Long  * orderId - the ID of the order. Long  * symbol - the stock symbol. String  * side - buy or sell. String  * price - the price of the execution. Long  * orderType - limit or market. String  * quantity - the filled quantity. Long</p> <p>Get order book: <pre><code>GET /marketdata/orderBook/L2?symbol={:symbol}&amp;depth={:depth}\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * depth - order book depth per side. Int</p> <p>Response:  * bids - array with price and size. Array  * asks - array with price and size. Array</p> <p>get candlesticks: <pre><code>GET /marketdata/candles?symbol={:symbol}&amp;resolution={:resolution}&amp;startTime={:startTime}&amp;endTime={:endTime}\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * resolution - window length of the candlestick chart in seconds. Long  * startTime - start time of the window in epoch. Long  * endTime - end time of the window in epoch. Long</p> <p>Response:  * candles - array with each candlestick data (attributes listed below). Array  * open - open price of each candlestick. Double  * close - close price of each candlestick. Double  * high - high price of each candlestick. Double  * low - low price of each candlestick. Double</p>"},{"location":"system-design-interview/chapter29/#data-models","title":"Data models","text":"<p>There are three main types of data in our exchange:  * Product, order, execution  * order book  * candlestick chart</p>"},{"location":"system-design-interview/chapter29/#product-order-execution","title":"Product, order, execution","text":"<p>Products describe the attributes of a traded symbol - product type, trading symbol, UI display symbol, etc.</p> <p>This data doesn't change frequently, it is primarily used for rendering in a UI.</p> <p>An order represents an instruction for a buy/sell order. Executions are outbound matched result.</p> <p>Here's the data model: </p> <p>We encounter orders and executions in all of our three flows:  * in the critical path, they are processed in-memory for high performance. They are stored and recovered from the sequencer.  * The reporter writes orders and executions to the database for reporting use-cases  * Executions are forwarded to market data to reconstruct the order book and candlestick chart</p>"},{"location":"system-design-interview/chapter29/#order-book","title":"Order book","text":"<p>The order book is a list of buy/sell orders for an instrument, organized by price level.</p> <p>An efficient data structure for this model, needs to satisfy:  * constant lookup time - getting volume at price level or between price levels  * fast add/execute/cancel operations  * query best bid/ask price  * iterate through price levels</p> <p>Example order book execution: </p> <p>After fulfilling this large order, the price increases as the bid/ask spread widens.</p> <p>Example order book implementation in pseudo code: <pre><code>class PriceLevel{\n    private Price limitPrice;\n    private long totalVolume;\n    private List&lt;Order&gt; orders;\n}\n\nclass Book&lt;Side&gt; {\n    private Side side;\n    private Map&lt;Price, PriceLevel&gt; limitMap;\n}\n\nclass OrderBook {\n    private Book&lt;Buy&gt; buyBook;\n    private Book&lt;Sell&gt; sellBook;\n    private PriceLevel bestBid;\n    private PriceLevel bestOffer;\n    private Map&lt;OrderID, Order&gt; orderMap;\n}\n</code></pre></p> <p>For a more efficient implementation, we can use a doubly-linked list instead of a standard list:  * Placing a new order is O(1), because we're adding an order to the tail of the list.  * Matching an order is O(1), because we are deleting an order from the head  * Canceling an order means deleting an order from the order book. We utilize <code>orderMap</code> for O(1) lookup and O(1) delete (due to the <code>Order</code> having a reference to the previous element in the list). </p> <p>This data structure is also used in the market data services to reconstruct the order book.</p>"},{"location":"system-design-interview/chapter29/#candlestick-chart","title":"Candlestick chart","text":"<p>The candlestick data is calcualated within the market data services based on processing orders in a time interval: <pre><code>class Candlestick {\n    private long openPrice;\n    private long closePrice;\n    private long highPrice;\n    private long lowPrice;\n    private long volume;\n    private long timestamp;\n    private int interval;\n}\n\nclass CandlestickChart {\n    private LinkedList&lt;Candlestick&gt; sticks;\n}\n</code></pre></p> <p>Some optimizations to avoid consuming too much memory:  * Use pre-allocated ring buffers to hold sticks to reduce the allocation number  * Limit the number of sticks in memory and persist the rest to disk</p> <p>We'll use an in-memory columnar database (eg KDB) for real-time analytics. After market close, data is persisted in historical database.</p>"},{"location":"system-design-interview/chapter29/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>One interesting thing to be aware of about modern exchanges is that unlike most other software, they typically run everything on one gigantic server.</p> <p>Let's explore the details.</p>"},{"location":"system-design-interview/chapter29/#performance","title":"Performance","text":"<p>For an exchange, it is very important to have good overall latency for all percentiles.</p> <p>How can we reduce latency?  * Reduce the number of tasks on the critical path  * Shorten the time spent on each task by reducing network/disk usage and/or reducing task execution time</p> <p>To achieve the first goal, we're stripped the critical path from all extraneous responsibility, even logging is removed to achieve optimal latency.</p> <p>If we follow the original design, there are several bottlenecks - network latency between services and disk usage of the sequencer.</p> <p>With such a design we can achieve tens of milliseconds end to end latency. We want to achieve tens of microseconds instead.</p> <p>Hence, we'll put everything on one server and processes are going to communicate via mmap as an event store: </p> <p>Another optimization is using an application loop (while loop executing mission-critical tasks), pinned to the same CPU to avoid context switching: </p> <p>Another side effect of using an application loop is that there is no lock contention - multiple threads fighting for the same resource.</p> <p>Let's now explore how mmap works - it is a UNIX syscall, which maps a file on disk to an application's memory.</p> <p>One trick we can use is creating the file in <code>/dev/shm</code>, which stands for \"shared memory\". Hence, we have no disk access at all.</p>"},{"location":"system-design-interview/chapter29/#event-sourcing","title":"Event sourcing","text":"<p>Event sourcing is discussed in-depth in the digital wallet chapter. Reference it for all the details.</p> <p>In a nutshell, instead of storing current states, we store immutable state transitions:   * On the left - traditional schema  * On the right - event source schema</p> <p>Here's how our design looks like thus far:   * external domain interacts with our client gateway using the FIX protocol  * Order manager receives the new order event, validates it and adds it to its internal state. Order is then sent to matching core  * If order is matched, the <code>OrderFilledEvent</code> is generated and sent over mmap  * Other components subscribe to the event store and do their part of the processing</p> <p>One additional optimizations - all components hold a copy of the order manager, which is packaged as a library to avoid extra calls for managing orders</p> <p>The sequencer in this design, changes to not be an event store, but be a single writer, sequencing events before forwarding them to the event store: </p>"},{"location":"system-design-interview/chapter29/#high-availability","title":"High availability","text":"<p>We aim for 99.99% availability - only 8.64s of downtime per day.</p> <p>To achieve that, we have to identify single-point-of-failures in the exchange architecture:  * setup backup instances of critical services (eg matching engine) which are on stand-by  * aggressively automate failure detection and failover to the backup instance</p> <p>Stateless services such as the client gateway can easily be horizontally scaled by adding more servers.</p> <p>For stateful components, we can process inbound events, but not publish outbound events if we're not the leader: </p> <p>To detect the primary replica being down, we can send heartbeats to detect that its non-functional.</p> <p>This mechanism only works within the boundary of a single server.  If we want to extend it, we can setup an entire server as hot/warm replica and failover in case of failure.</p> <p>To replicate the event store across the replicas, we can use reliable UDP for faster communication.</p>"},{"location":"system-design-interview/chapter29/#fault-tolerance","title":"Fault tolerance","text":"<p>What if even the warm instances go down? It is a low probability event but we should be ready for it.</p> <p>Large tech companies tackle this problem by replicating core data to data centers in multiple cities to mitigate eg natural disasters.</p> <p>Questions to consider:  * If the primary instance is down, how and when do we failover to the backup instance?  * How do we choose the leader among the backup instances?  * What is the recovery time needed (RTO - recovery time objective)?  * What functionalities need to be recovered? Can our system operate under degraded conditions?</p> <p>How to address these:  * System can be down due to a bug (affecting primary and replicas), we can use chaos engineering to surface edge-cases and disastrous outcomes like these  * Initially though, we could perform failovers manually until we gather sufficient knowledge about the system's failure modes  * leader-election can be used (eg Raft) to determine which replica becomes the leader in the event of the primary going down</p> <p>Example of how replication works across different servers: </p> <p>Example leader-election terms: </p> <p>For details on how Raft works, check this out</p> <p>Finally, we need to also consider loss tolerance - how much data can we lose before things get critical? This will determine how often we backup our data.</p> <p>For a stock exchange, data loss is unacceptable, so we have to backup data often and rely on raft's replication to reduce probability of data loss.</p>"},{"location":"system-design-interview/chapter29/#matching-algorithms","title":"Matching algorithms","text":"<p>Slight detour on how matching works via pseudo code: <pre><code>Context handleOrder(OrderBook orderBook, OrderEvent orderEvent) {\n    if (orderEvent.getSequenceId() != nextSequence) {\n        return Error(OUT_OF_ORDER, nextSequence);\n    }\n\n    if (!validateOrder(symbol, price, quantity)) {\n        return ERROR(INVALID_ORDER, orderEvent);\n    }\n\n    Order order = createOrderFromEvent(orderEvent);\n    switch (msgType):\n        case NEW:\n            return handleNew(orderBook, order);\n        case CANCEL:\n            return handleCancel(orderBook, order);\n        default:\n            return ERROR(INVALID_MSG_TYPE, msgType);\n\n}\n\nContext handleNew(OrderBook orderBook, Order order) {\n    if (BUY.equals(order.side)) {\n        return match(orderBook.sellBook, order);\n    } else {\n        return match(orderBook.buyBook, order);\n    }\n}\n\nContext handleCancel(OrderBook orderBook, Order order) {\n    if (!orderBook.orderMap.contains(order.orderId)) {\n        return ERROR(CANNOT_CANCEL_ALREADY_MATCHED, order);\n    }\n\n    removeOrder(order);\n    setOrderStatus(order, CANCELED);\n    return SUCCESS(CANCEL_SUCCESS, order);\n}\n\nContext match(OrderBook book, Order order) {\n    Quantity leavesQuantity = order.quantity - order.matchedQuantity;\n    Iterator&lt;Order&gt; limitIter = book.limitMap.get(order.price).orders;\n    while (limitIter.hasNext() &amp;&amp; leavesQuantity &gt; 0) {\n        Quantity matched = min(limitIter.next.quantity, order.quantity);\n        order.matchedQuantity += matched;\n        leavesQuantity = order.quantity - order.matchedQuantity;\n        remove(limitIter.next);\n        generateMatchedFill();\n    }\n    return SUCCESS(MATCH_SUCCESS, order);\n}\n</code></pre></p> <p>This matching algorithm uses the FIFO algorithm for determining which orders at a price level to match.</p>"},{"location":"system-design-interview/chapter29/#determinism","title":"Determinism","text":"<p>Functional determinism is guaranteed via the sequencer technique we used.</p> <p>The actual time when the event happens doesn't matter: </p> <p>Latency determinism is something we have to track. We can calculate it based on monitoring 99 or 99.99 percentile latency.</p> <p>Things which can cause latency spikes are garbage collector events in eg Java.</p>"},{"location":"system-design-interview/chapter29/#market-data-publisher-optimizations","title":"Market data publisher optimizations","text":"<p>The market data publisher receives matched results from the matching engine and rebuilds the order book and candlestick charts based on them.</p> <p>We only keep part of the candlesticks as we don't have infinite memory. Clients can choose how much granular info they want. More granular info might require a higher price: </p> <p>A ring buffer (aka circular buffer) is a fixed-size queue with the head connected to the tail. The space is preallocated to avoid allocations. The data structure is also lock-free.</p> <p>Another technique to optimize the ring buffer is padding, which ensures the sequence number is never in a cache line with anything else.</p>"},{"location":"system-design-interview/chapter29/#distribution-fairness-of-market-data-and-multicast","title":"Distribution fairness of market data and multicast","text":"<p>We need to ensure subscribers receive the data at the same time since if one receives data before another, that gives them crucial market insight, which they can use to manipulate the market.</p> <p>To achieve this, we can use multicast using reliable UDP when publishing data to subscribers.</p> <p>Data can be transported via the internet in three ways:  * Unicast - one source, one destination  * Broadcast - one source to entire subnetwork  * Multicast - one source to a set of hosts on different subnetworks</p> <p>In theory, by using multicast, all subscribers should receive the data at the same time.</p> <p>UDP, however, is unreliable and the data might not reach everyone. It can be enhanced with retransmissions, however.</p>"},{"location":"system-design-interview/chapter29/#colocation","title":"Colocation","text":"<p>Exchanges offer brokers the ability to colocate their servers in the same data center as the exchange.</p> <p>This reduces the latency drastically and can be considered a VIP service.</p>"},{"location":"system-design-interview/chapter29/#network-security","title":"Network Security","text":"<p>DDoS is a challenge for exchanges as there are some internet-facing services. Here's our options:  * Isolate public services and data from private services, so DDoS attacks don't impact the most important clients  * Use a caching layer to store data which is infrequently updated  * Harden URLs against DDoS, eg prefer <code>https://my.website.com/data/recent</code> vs. <code>https://my.website.com/data?from=123&amp;to=456</code>, because the former is more cacheable  * Effective allowlist/blocklist mechanism is needed.  * Rate limiting can be used to mitigate DDoS</p>"},{"location":"system-design-interview/chapter29/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Other interesting notes:  * not all exchanges rely on putting everything on one big server, but some still do  * modern exchanges rely more on cloud infrastructure and also on automatic market makers (AMM) to avoid maintaining an order book</p>"}]}