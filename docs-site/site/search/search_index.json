{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Notes","text":"<p>This is a collection of my personal notes, rendered as a website.</p>"},{"location":"booknotes/system-design/","title":"System Design","text":"<p>Notes for resources related to system design.</p>"},{"location":"booknotes/system-design/#books","title":"Books","text":"<ul> <li>Understanding Distributed Systems</li> <li>System Design Interview - An Insider's Guide (vol 1 &amp; 2)</li> </ul>"},{"location":"booknotes/system-design/understanding-distributed-systems/","title":"Index","text":""},{"location":"booknotes/system-design/understanding-distributed-systems/#understanding-distributed-systems","title":"Understanding Distributed Systems","text":"<p>Book notes taken during my read-through of UDS. Check out my review of the book.</p> <ul> <li>Part 0 - Introduction</li> <li>Part 1 - Communication</li> <li>Part 2 - Coordination</li> <li>Part 3 - Scalability</li> <li>Part 4 - Resiliency</li> <li>Part 5 - Maintainability</li> </ul>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/","title":"Introduction","text":"<ul> <li>Preface</li> <li>Communication</li> <li>Coordination</li> <li>Scalability</li> <li>Resiliency</li> <li>Maintainability</li> <li>Anatomy of a distributed system</li> </ul>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/#preface","title":"Preface","text":"<p>Most materials on distributed systems are either too theoretical or too practical. This book aims at the middle. If you're already an expert, this book is not for you. It focuses on breadth rather than depth.</p> <p>Distributed system == group of nodes, communicating over some channel to accomplish a task. Nodes can be a computer, phone, browser, etc.</p> <p>Why bother building such a system?  * Some apps are inherently distributed - eg the web.  * To achieve high availability - if one node fails, the system doesn't crash.  * To tackle data-intensive workloads which can't fit on a single machine.  * Performance requirements - eg Netflix streams HQ tv thanks to serving you from a datacenter close to you.</p> <p>The book focuses on tackling the fundamental problems one faces when designing a distributed system.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/#communication","title":"Communication","text":"<p>First challenge is for nodes to communicate with each other over the network:  * How are request/response messages represented on the wire?  * What happens when there is a temporary network outage?  * How to avoid man in the middle attacks?</p> <p>In practice, a nice network library can abstract away those details, but oftentimes abstractions leak and you need to know how the networks work under the hood.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/#coordination","title":"Coordination","text":"<p>Some form of coordination is required to make two nodes work together.</p> <p>Good metaphor - the two generals problem.</p> <p>Two generals need to agree on a time to make a joint attack on a city. All messages between them can be intercepted &amp; dropped. How to ensure they attack the city at the same time, despite the faulty communication medium?</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/#scalability","title":"Scalability","text":"<p>How efficiently can the application handle load. Ways to measure it depends on the app's use-case - number of users, number of reads/writes, RPS, etc.</p> <p>For the type of applications covered in the book, load is measured in:  * Throughput - number of requests per second (RPS) the application can process.  * Response time - time elapsed in seconds between sending a request and receiving a response.</p> <p>When the application reaches a certain threshold in terms of load, throughput starts degrading: </p> <p>The capacity depends on a system's architecture, implementation, physical limitations (eg CPU, Memory, etc).</p> <p>An easy way to fix this is to buy more expensive hardware - scaling up. This scaling mechanism has a limit, though, since you can't scale hardware without bounds.</p> <p>The alternative is to scale horizontally by adding more machines which work together.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/#resiliency","title":"Resiliency","text":"<p>A distributed system is resilient when it doesn't crash even in the face of failures.</p> <p>Failures which are left unchecked can impact the system's availability - the % of time the system is available for use. <pre><code>availability = uptime / total time.\n</code></pre></p> <p>Availability is often expressed in terms of nines: </p> <p>Three nines is considered acceptable for users, anything above four nines is considered highly available.</p> <p>There are various techniques one can use to increase availability. All of them try to mitigate failures when they inevitably come up - fault isolation, self-healing mechanisms, redundancy, etc.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/#maintainability","title":"Maintainability","text":"<p>Most of the cost of software is spent maintaining it - fixing bugs, adding new features, operating it.</p> <p>We should aspire to make our systems easy to maintain - easy to extend, modify and operate.</p> <p>One critical aspect is for a system to be well-tested - unit, integration &amp; e2e tests. Another one is for operators to have the necessary tools to monitor a system's health &amp; making quick changes - feature flags, rollback mechanisms, etc.</p> <p>Historically, developers, testers &amp; operators were different teams, but nowadays, it tends to be the same team.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part00/#anatomy-of-a-distributed-system","title":"Anatomy of a distributed system","text":"<p>Main focus of the book is on backend distributed systems that run on commodity machines.</p> <p>From a hardware perspective, a distributed system is a set of machines communicating over network links. From a runtime perspective, it's a set of processes which communicate via inter-process communication (IPC), eg HTTP. From an implementation perspective, it's a set of loosely-coupled components which communicate via APIs.</p> <p>All are valid points of view on the system &amp; the book will switch between them based on context.</p> <p>A service implements one specific part of the overall system's capabilities. At its core sit the business logic.</p> <p>At the sides, there are interfaces which define functionalities provided by external systems (eg database, another service in the system, etc). The interfaces are implemented by adapters which implement the technical details of connecting to the external systems.</p> <p>This architectural pattern is known as ports and adapters</p> <p>This architectural style accomplishes a setup in which the business logic don't depend on the technical details. Instead, technical details depend on business logic - dependency inversion: </p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/","title":"Communication","text":"<ul> <li>Introduction</li> <li>Reliable links</li> <li>Secure links</li> <li>Discovery</li> <li>APIs</li> </ul>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#introduction","title":"Introduction","text":"<p>In order for processes to communicate, they need to agree on a set of rules which determine how data is transmitted and processed. Network protocols define these rules.</p> <p>Network protocols are arranged in a stack - every protocol piggybacks on the abstraction provided by the protocol underneath. </p> <p>Taking a closer look:  * The link layer provides an interface for operating on the underlying network hardware via eg the Ethernet protocol.  * The internet layer provides an interface for sending data from one machine to another with a best-effort guarantee - ie data can be lost, corrupted, etc.  * The transport layer enables transmitting data between processes (different from machines, because you might have N processes on 1 machine). Most important protocol at this layer is TCP - it adds reliability to the IP protocol.  * The application layer is a high-level protocol, targetted by applications - eg HTTP, DNS, etc.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#reliable-links","title":"Reliable links","text":"<p>Communication between nodes happens by transmitting packages between them. This requires 1) addressing nodes and 2) a mechanism for routing packets across routers.</p> <p>Addressing is handled by the IP protocol. Routing is handled within the routers' routing tables. Those create a mapping between destination address and the next router along the path. The responsibility of building &amp; communicating the routing tables is handled by the Border Gateway Protocol (BGP).</p> <p>IP, however, doesn't guarantee that data sent over the internet will arrive at the destination. TCP, which lies in the transport layer, handles this. It provides a reliable communication channel on top of an unreliable one (IP). A stream of bytes arrives at the destination without gaps, duplication or corruption. This protocol also has backoff mechanisms in-place to avoid congesting the transportation network, making it a healthy protocol for the internet.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#reliability","title":"Reliability","text":"<p>Achieved by:  * splitting a stream of bytes into discrete segments, which have sequence numbers and a checksum.  * Due to this, the receiver can detect holes, duplicates and corrupt segments (due to the checksum).  * Every segment needs to be acknowledged by the receiver, otherwise, they are re-transmitted.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#connection-lifecycle","title":"Connection lifecycle","text":"<p>With TCP, a connection must be established first. The OS manages the connection state on both sides via sockets.</p> <p>To establish a connection, TCP uses a three-way-handshake: </p> <p>The numbers exchanged are the starting sequence numbers for upcoming data packets on both sides of the connection.</p> <p>This connection setup step is an overhead in starting the communication. It is mainly driven by the round-trip time. To make it fast, put servers as close as possible to clients. There is also some tear down time spent when closing a connection.</p> <p>Finally, once a connection is closed, the socket is not released immediately. It has some teardown time as well. Due to this, constantly opening and closing connections can quickly drain your available sockets.</p> <p>This is typically mitigated by:  * Leveraging connection pools  * Not closing a TCP connection between subsequent request/response pairs</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#flow-control","title":"Flow Control","text":"<p>Flow control is a back-off mechanism which TCP implements to prevent the sender from overflowing the receiver.</p> <p>Received segments are stored in a buffer, while waiting for the process to read them: </p> <p>The receive buffer size is also communicated back to the sender, who uses it to determine when to back-off: </p> <p>This mechanism is similar to rate limiting, but at the connection level.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#congestion-control","title":"Congestion control","text":"<p>TCP protects not only the receiver, but the underlying network as well.</p> <p>An option called <code>congestion window</code> is used, which specifies the total number of packets which can be in-flight (sent but not ack).</p> <p>The smaller the congestion window, the less bandwidth is utilized.</p> <p>When a connection starts, the congestion window is first set to a system default and it slowly adjusts based on the feedback from the underlying network: </p> <p>Timeouts &amp; missed packets adjusts the congestion window down. Successful transmissions adjust it up.</p> <p>Effectively, slower round trip times yield larger bandwidths. Hence, favor placing servers close to clients.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#custom-protocols","title":"Custom Protocols","text":"<p>TCP delivers reliability and stability at the expense of extra latency and reduced bandwidth.</p> <p>UDP is an alternative protocol, which doesn't provide TCP's reliability mechanisms. It is used as a canvas for custom protocols to be built on-top which have some of TCP's functionalities but not all.</p> <p>Games are one example where using TCP is an overhead.  If a client misses a single game frame sent from the server, TCP would attempt to retransmit it. </p> <p>However, for games that is unnecessary because the game state would have already progressed once the packet gets retransmitted.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#secure-links","title":"Secure links","text":"<p>We can now transmit bytes over the network, but they're transmitted in plain-text. We can use TLS to secure the communication.</p> <p>TLS provides encryption, authentication and integrity for application-layer protocols (eg HTTP).</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#encryption","title":"Encryption","text":"<p>Encryption guarantees that transmitted data can only be read by sender and receiver. To everyone else, the data is obfuscated.</p> <p>When a TLS connection is first opened, the sender and receiver negotiate a shared encryption secret using asymmetric encryption.</p> <p>The shared secret, which was exchanged, is then used for secure communication between the client and server using symmetric encryption. Periodically, the shared secret key is renegotiated to mitigate the risk of someone decrypting the shared secret during the on-going exchange.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#authentication","title":"Authentication","text":"<p>So far, we've managed to encrypt the data exchange, but we haven't established a way for clients to authenticate who the server is and vice versa.</p> <p>This is achieved via digital signatures and certificates:  * Digital signature - a server signs some data \\w their private key. Client can verify data is indeed created by server by taking the signature and the server's public key.  * Certificate - a document, which include details about the server (eg name, business, address, etc.).</p> <p>The client uses the certificate, which the server provides to verify that the server is who they say they are. However, how can the client verify that the public key they received indeed belongs to eg google.com?</p> <p>This is done by having the client cross-check a server's certificate with a certificate authority.</p> <p>Certificates include a server's info, certificate authority info, public key and it is signed by a certificate authority's private key.</p> <p>If a user has the CA's public key, they use it to check that the certificate is valid.  Otherwise, they get the CA's certificate, which is signed by another, higher level, certificate authority. This process repeats until the client finds a certificate signed by a CA they know (ie have their public key).  By default, browsers have a list of well-known and trusted certificate authorities.</p> <p>The chain of certificates always ends with a root CA, who self-signs their certificate and is well-known and trusted. </p> <p>Note: servers don't just send their certificate back to the client. They send the whole certificate chain to avoid additional network calls.</p> <p>One of the most common mistakes made when using TLS is to let a certificate expire. That prevents clients from connecting to your server via TLS. This is why, automation is important to prevent this from happening.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#integrity","title":"Integrity","text":"<p>Encryption prevents middleman from reading transmitted data, while authentication enables us to verify the identity of the server.</p> <p>However, what's stopping a middleman from tampering the transmitted data?</p> <p>Eg, one could intercept a message, switch a few bytes and the server/client will decode an entirely different message. How can they know if this is the actual message sent by the other side or if there is someone who's tampered it?</p> <p>That's where secure hash functions and in particular HMACs come into play.</p> <p>Every message is hashed \\w this function and the hash is included in the message's payload.  The client/server can then verify that the message has not been tampered via the formula hash(message.payload) == message.hash.</p> <p>In addition to tampering, this process also allows us to detect data corruption. </p> <p>Typically, we can rely on TCP to prevent data corruptions via its checksum, but that mechanism has a flaw where once every ~16GB-1TB, a data corruption issue is not detected.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#handshake","title":"Handshake","text":"<p>When a new TLS connection is opened, a handshake occurs which exchanges the variables used throughout the connection for achieving any of the purposes mentioned so far - encryption, authentication, integrity.</p> <p>During this handshake, the following events occur:  * Parties agree on a cipher suite - what encryption algorithms are they going to use:     * key-exchange algorithm - used to generate the shared secret     * signature algorithm - used to sign/verify digital signatures     * symmetric encryption algorithm - used to encrypt communication     * HMAC algorithm - used to verify message integrity  * Parties use the key-exchange algorithm to generate the shared secret, used afterwards for symmetric encryption  * Client verifies the certificate provided by the server. Optionally, the server can also verify the client's certificate if one is present (and server is configured to verify it).     * Client certificates come into play when eg your server has a fixed, well-known set of clients which are allowed to interact with it</p> <p>The operations don't necessarily happen in this order as there are some optimizations which come into play in recent TLS versions.</p> <p>The bottom line, though, is that instantiating a TLS connection is not free. Hence, put servers close to clients &amp; reuse connections when possible.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#discovery","title":"Discovery","text":"<p>How do we find out what's the IP address of the server we want to connect to?</p> <p>DNS to the rescue - distributed, hierarchical, eventually consistent key-value store of hostname -&gt; ip address</p> <p>How does it work?  * You enter www.example.com into your browser  * If the hostname you're looking for is in the browser's cache, it returns it  * Otherwise, the lookup request is routed to the DNS resolver - a server, maintained by your internet service provider (ISP).  * The DNS resolver checks its cache &amp; returns the address if found. If not, lookup request is routed to the root name server (root NS).  * Root NS maps top-level domain (TLD), eg <code>.com</code>, to the NS server responsible for it.  * DNS resolver sends request to TLD NS server for <code>example.com</code>. NS Server returns the authoritative NS server for <code>example.com</code>  * DNS resolver sends request to the <code>example.com</code> NS server which returns the IP address for <code>www.example.com</code> </p> <p>In reality, a lot of caching is involved at each step, enabling us to avoid all the round trips, as domain names rarely change. Caches refresh the domain names based on a time-to-live value (TTL) which every domain name has.</p> <p>Specifying the TTL is a trade-off:  * if it's too big &amp; you make a domain name change, some clients will take longer to update.   * Make it too small &amp; average request time will increase.</p> <p>The original DNS protocol sent plain-text messages over UDP for efficiency, but nowadays, DNS over TLS (using TCP) is used for security purposes.</p> <p>One interesting observation is that a DNS name server can be a single point of failure &amp; lead to outages.  A smart mitigation is to let resolvers serve stale entries vs. returning an error in case the name server is down.</p> <p>The property of a system to continue to function even when a dependency is down is referred to as \"static stability\".</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#apis","title":"APIs","text":"<p>When a client makes a call to a server via their hostname + port, that calls goes through an adapter which maps the request to interface calls within the server, invoking its business logic.</p> <p>Communication can be direct or indirect:  * Direct - both processes are up and running &amp; communicate in real-time.  * Indirect - communication is achieved via a broker. One process might be down &amp; receive the message at a later time, asynchronously.</p> <p>This chapter focuses on direct communication. Indirect comms is covered later in the book. In particular, the chapter focuses on the request-response communication style.</p> <p>In this style, clients send messages to the server &amp; the server responds back, similar to function calls in a codebase, but across process &amp; network boundaries.</p> <p>The data format is language agnostic. The format specifies the (de)serialization speed, whether it's human readable &amp; how hard it is to evolve over time.</p> <p>Popular data formats:  * JSON - textual, human-readable format at the expense of larger data packets &amp; parsing overhead.  * Protocol buffers - binary, efficient format, which has smaller payloads than JSON, but is not human-readable</p> <p>When a client sends a request to the server, it can either block &amp; wait for response or run the request in a thread &amp; invoke a callback when the response is received. The former approach is synchronous, latter is asynchronous. For most use-cases, favor asynchronous communication to avoid blocking the UI.</p> <p>Common IPC technologies for request-response interactions:  * HTTP - slower but more adopted  * gRPC - faster, but not supported by all browsers</p> <p>Typically, server to server communication is implemented via gRPC, client to server is via HTTP.</p> <p>A popular set of design principles for designing elegant HTTP APIs is REST.</p> <p>These principles include:  * requests are stateless - each request contains all the necessary information required to process it.  * responses are implicitly or explicitly labeled as cacheable, allowing clients to cache them for subsequent requests.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#http","title":"HTTP","text":"<p>HTTP is a request-response protocol for client to server communication. </p> <p>In HTTP 1.1, a message is text-based \\w three parts - start line, headers, optional body:  * Start line - In requests, indicates what the request is for, in responses, indicates if request were successful or not  * Headers - key-value pairs with metadata about the message  * Body - container for data</p> <p>HTTP is a stateless protocol, hence, all client info needs to be specified within the requests.  Servers treat requests as independent, unless there is extra data, indicating what client they're associated with.</p> <p>HTTP uses TCP for the reliability guarantees and TLS for the security ones. When using TLS, it is referred to as HTTPS.</p> <p>HTTP 1.1 keeps a connection to a server open between requests to avoid the TCP overhead of re-opening connections. However, it has a limitation that subsequent requests need to wait before the server sends a response back. </p> <p>This is troublesome when eg loading a dozen of images on a page - something which can easily be done in parallel.</p> <p>Browsers typically work around this by making multiple connections to the server for fetching independent resources. The price for this workaround is using more memory &amp; sockets.</p> <p>HTTP 2 was designed to address the limitations of HTTP 1.1 - it uses a binary protocol vs. a textual one.  This allows multiplexing multiple concurrent request-response transactions on the same connection.</p> <p>In early 2020, half of the most visited websites used HTTP 2.</p> <p>HTTP 3 is the latest iteration, which is based on UDP and implements its own reliability mechanisms to address some of TCP's shortcomings. For example, losing a packet with HTTP 2 blocks all subsequent packages, including ones on unrelated streams, due to how TCP works. </p> <p>With HTTP 3, a package loss only blocks the affected stream, not all of them.</p> <p>Examples in the book use HTTP 1.1 due to its human-readability.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#resources","title":"Resources","text":"<p>Example server application we're building - product catalog management for an e-commerce site. Customers can browse the catalog, administrators can create, update or delete products.</p> <p>We'll build this via an HTTP API. HTTP APIs host resources, which can be physical (eg image) or abstract (eg product) entities on which we can execute create/read/update/delete (CRUD) operations.</p> <p>A URL identifies a resource by describing its location on the server, eg <code>https://www.example.com/products?sort=price</code> :  * <code>http</code> is the protocol  * <code>www.example.com</code> is the hostname  * <code>products</code> is the name of the resource  * <code>?sort=price</code> is the query string, which contains additional parameters about how to fetch the results.</p> <p>A URL can also model relationships between resources, eg:  * <code>/products/42</code> - the product with ID 42  * <code>/products/42/reviews</code> - the reviews of product with ID 42</p> <p>We also need to deal with how the resource data is serialized.  That is specified by the client via the <code>Content-type</code> header, most popular one being <code>application/json</code>.</p> <p>Example JSON-serialized product: <pre><code>{\n   \"id\": 42,\n   \"category\": \"Laptop\",\n   \"price\": 999\n}\n</code></pre></p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#request-methods","title":"Request methods","text":"<p>URLs designate what resource you're targeting. HTTP methods define what operation you're performing on the resource.</p> <p>Most commonly used operations \\w example <code>product</code> entity:  * <code>POST /products</code> - Create a new product  * <code>GET /products</code>  - List all products. Usually includes query parameters to apply filters on the result set.  * <code>GET /products/42</code> - Get a particular product  * <code>PUT /products/42</code> - Update a product  * <code>DELETE /products/42</code> - Delete a product</p> <p>Request methods are categorized based on whether they are safe and idempotent:  * Safe - don't have visible side-effects, hence, can safely be cached.  * Idempotent - can be executed multiple times and the end result should be the same as if it was executed once. This property is crucial for APIs (covered later).</p> <p>Table of methods &amp; safe/idempotent properties:  * POST - not safe &amp; not idempotent  * GET - safe &amp; idempotent  * PUT - not safe &amp; idempotent  * DELETE - not safe &amp; idempotent</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#response-status-codes","title":"Response status codes","text":"<p>Responses contain a status code, which indicates whether the request is successful or not:  * 200-299 - indicate success, eg 200 OK  * 300-399 - used for redirection, eg 301 Moved Permanently indicates the resource is moved to a different URL. The new URL is specified in the <code>Location</code> header.  * 400-499 - client errors    * 400 Bad Request - when eg input format is wrong    * 401 Unauthorized - when eg password on login is wrong)    * 403 Forbidden - when eg you don't have access to resource    * 404 Not Found - when eg the specified resource doesn't exist  * 500-599 - server errors, when eg the database is down &amp; you can't process the request. These requests can typically be retried.    * 500 Internal Server Error - server encountered unexpected error, due to which request can't be handled    * 502 Bad Gateway - server, while acting as proxy, received a downstream server error    * 503 Service Unavailable - server is unavailable due to eg a temporary heavy load on it.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#openapi","title":"OpenAPI","text":"<p>The server's API can be defined via an Interface Definition Language (IDL) - a language-agnostic format, which specifies what the API contains.</p> <p>This can be used for generating the server's adapter &amp; the client's library for interacting with the server.</p> <p>The OpenAPI specification evolved from the Swagger project and is the most popular IDL for RESTful APIs. It can formally describe an API via YAML format including available endpoints, supported request methods, status codes, schema of the entities.</p> <p>Example OpenAPI definition: <pre><code>openapi: 3.0.0\ninfo:\n   version: \"1.0.0\"\n   title: Catalog Service API\n\npaths:\n   /products:\n      get:\n         summary: List products\n         parameters:\n            - in: query\n              name: sort\n              required: false\n              schema:\n                 type: string\n         responses:\n            \"200\":\n               description: list of products in catalog\n               content:\n                  application/json:\n                     schema:\n                        type: array\n                        items:\n                           $ref: \"#/components/schemas/ProductItem\"\n            \"400\":\n               description: bad input\n\ncomponents:\n   schemas:\n      ProductItem:\n         type: object\n         required:\n            - id\n            - name\n            - category\n         properties:\n            id:\n               type: number\n            name:\n               type: string\n            category:\n               type: string\n</code></pre></p> <p>This can be then used to generate an API's documentation, boilerplate adapters and client SDKs.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#evolution","title":"Evolution","text":"<p>An API starts out as a well-designed interface but it would eventually need to change due to change in requirements.</p> <p>The last thing we need is to change the API &amp; introduce a breaking change.  This means that existing client software will stop working properly with the API after the change. Hence, we'll need to modify every single client of the API. Some of them, we might not have access to.</p> <p>Two types of breaking changes:  * at the endpoint level - eg changing <code>/products</code> to <code>/new-products</code> or making an optional query parameter required.  * at the schema level - eg changing the type of the <code>category</code> property from <code>string</code> to <code>int</code></p> <p>To tackle this, REST APIs should be versioned (eg <code>/v1/products</code>). As a general rule of thumb, though, REST APIs should evolve in a backwards compatible manner.</p> <p>Backwards compatible APIs are usually not elegant, but they're practical.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part01/#idempotency","title":"Idempotency","text":"<p>If an API request times out without knowing the result, a sensible way to tackle this on the client side is to retry the request (one or more times).</p> <p>Idempotent HTTP requests are susceptible to this technique as executing multiple identical requests yields the same result as executing it once.</p> <p>Non-idempotent requests, on the other hand, can't be retried so easily, because they can result in a bad state such as eg creating the same product twice.</p> <p>To mitigate this, some effort can be done on the server-side to eg make POST requests idempotent. </p> <p>A technique to achieve this is to associate a request with an idempotency key. A key used to detect if this request has already been executed. This can be achieved by eg having an <code>Idempotency-Key</code> header (similar to Stripe's API), which is a UUID, generated by the client.</p> <p>When a server received such a key, it first checks if there is such a key in the DB. If not, execute the request. If there is one, return the already executed response.</p> <p>These keys can be cleaned up from the database after a while. In other words, they can have a TTL.</p> <p>One consideration when executing this is to make the act of 1) storing the idempotency key and 2) executing the request an atomic operation. Otherwise, the server can crash right after storing the idempotency key. Hence, after restart, server thinks that product is already created, but it is not.</p> <p>This is easy to achieve if we're using an ACID database, which has transaction guarantees. It is a lot more challenging if we need to also change the state of an external system during the transaction.</p> <p>An interesting edge-case:  * Client A attempts to create a resource, request fails but resource is created  * Client B deletes the resource  * Client A attempts to create the resource again</p> <p>What should the response be? - less surprising behavior is to return success.</p> <p>In summary, idempotent APIs enable more robust clients, since they can always retry failures without worrying about consequences.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/","title":"Coordination","text":"<ul> <li>System Models</li> <li>Failure Detection</li> <li>Time</li> <li>Leader Election</li> <li>Replication</li> <li>Coordination avoidance</li> <li>Transactions</li> <li>Asynchronous transactions</li> <li>Summary</li> </ul> <p>So far, focus is on making nodes communicate. Focus now is to make nodes coordinate as if they're a single coherent system.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#system-models","title":"System Models","text":"<p>To reason about distributed systems, we must strictly define what can and cannot happen.</p> <p>A system model is a set of assumptions about the behavior of processes, communication links and timing while abstracting away the complexities of the technology being used.</p> <p>Some common models for communication links:  * fair-loss link - messages may be lost and duplicated, but if sender keeps retransmitting messages, they will eventually be delivered.  * reliable link - messages are delivered exactly once without loss or duplication. Reliable links can be implemented on top of fair-loss links by deduplicating messages at receiving end.  * authenticated reliable link - same as reliable link but also assumes the receiver can authenticate the sender.</p> <p>Although these models are abstractions of real communication links, they are useful to verify the correctness of algorithms.</p> <p>We can also model process behavior based on the types of failures we can expect:  * arbitrary-fault model - aka \"Byzantine\" model, the process can deviate from algorithm in unexpected ways, leading to crashes due to bugs or malicious activity.  * crash-recovery model - process doesn't deviate from algorithm, but can crash and restart at any time, losing its in-memory state.  * crash-stop model - process doesn't deviate from algorithm, it can crash and when it does, it doesn't come back online. Used to model unrecoverable hardware faults.</p> <p>The arbitrary-fault model is used to describe systems which are safety-criticla or not in full control of all the processes - airplanes, nuclear power plants, the bitcoin digital currency.</p> <p>This model is out of scope for the book. Algorithms presented mainly focus on the crash-recovery model.</p> <p>Finally, we can model timing assumptions:  * synchronous model - sending a message or executing an operation never takes more than a certain amount of time. Not realistic for the kinds of systems we're interested in.     * Sending messages over the network can take an indefinite amount of time. Processes can also be slowed down by garbage collection cycles.  * asynchronous model - sending a message or executing an operation can take an indefinite amount of time.  * partially synchronous model - system behaves synchronously most of the time. Typically representative enough of real-world systems.</p> <p>Throughout the book, we'll assume a system model with fair-loss links, crash-recovery processes and partial synchrony.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#failure-detection","title":"Failure Detection","text":"<p>When a client sends a request to a server &amp; doesn't get a reply back, what's wrong with the server?</p> <p>One can't tell, it might be:  * server is very slow  * it crashed  * there's a network issue</p> <p>Worst case, client will wait forever until they get a response back. Possible mitigation is to setup timeout, back-off and retry. </p> <p>However, a client need not wait until they have to send a message to figure out that the server is down. They can maintain a list of available processes which they ping or heartbeat to figure out if they're alive:  * ping - periodic request from client to server to check if process is available.  * heartbeat - periodic request from server to client to indicate that server is still alive.</p> <p>Pings and heartbeats are used in situations where processes communicate with each other frequently, eg within a microservice deployment.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#time","title":"Time","text":"<p>Time is critical in distributed systems:  * On the network layer for DNS TTL records  * For failure detection via timeouts  * For determining ordering of events in a distributed system</p> <p>The last use-case is one we're to tackle in the chapter. </p> <p>The challenge with that is that in distributed systems, there is no single global clock all services agree on unlike a single-threaded application where events happen sequentially.</p> <p>We'll be exploring a family of clocks which work out the order of operations in a distributed system.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#physical-clocks","title":"Physical Clocks","text":"<p>Processes have access to a physical wall-time clock. Every machine has one. The most common one is based on a vibrating quartz crystal, which is cheap but not insanely accurate.</p> <p>Because quartz clocks are not accurate, they need to be occasionally synced with servers with high-accuracy clocks.</p> <p>These kinds of servers are equipped with atomic clocks, which are based on quantum mechanics. They are accurate up to 1s in 3 mil years.</p> <p>Most common protocol for time synchronization is the Network Time Protocol (NTP).</p> <p>Clients synchronize their clock via this protocol, which also factors in the network latency. The problem with this approach is that the clock can go back in time on the origin machine.</p> <p>This can lead to a situation where operation A which happens after operation B has a smaller timestamp.</p> <p>An alternative is to use a clock, provided by most OS-es - monotonic clock. It's a forward-only moving clock which measures time elapsed since a given event (eg boot up).</p> <p>This is useful for measuring time elapsed between timestamps on the same machine, but not for timestamps across multiple ones.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#logical-clocks","title":"Logical Clocks","text":"<p>Logical clocks measure passing of time in terms of logical operations, not wall-clock time.</p> <p>Example - counter, incremented on each operation. This works fine on single machines, but what if there are multiple machines.</p> <p>A Lamport clock is an enhanced version where each message sent across machines includes the logical clock's counter. Receiving machines take into consideration the counter of the message they receive.</p> <p>Subsequent operations are timestamped with a counter bigger than the one they received. </p> <p>With this approach, dependent operations will have different timestamps, but unrelated ones can have identical ones.</p> <p>To break ties, the process ID can be included as a second ordering factor.</p> <p>Regardless of this, logical clocks don't imply a causal relationship. It is possible for event A to happen before B even if B's timestamp is greater.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#vector-clocks","title":"Vector Clocks","text":"<p>Vector clocks are logical clocks with the additional property of guaranteeing that event A happened before event B if A's timestamp is smaller than B.</p> <p>Vector clocks have an array of counters, one for each process in the system:  * Counters start from 0  * When an operation occurs, the process increments its process' counter by 1  * Sending a message increments the process' counter by 1. The array is included in the message  * Receiving process merges the received array with its local one by taking the max for each element.    * Finally, its process counter is incremented by 1 </p> <p>This guarantees that event A happened before B if:  * Every counter in A is <code>less than or equal</code> every counter in B  * At least one counter in A is <code>less than</code> corresponding counter in B </p> <p>If neither A happened before B nor B happened before A, the operations are considered concurrent </p> <p>In the above example:  * B happened before C  * E and C are concurrent</p> <p>A downside of vector clocks is that storage requirements grow with every new process. Other types of logical clocks, like dotted version clocks solve this issue.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#summary","title":"Summary","text":"<p>Using physical clocks for timestamp is good enough for some records such as logs.</p> <p>However, when you need to derive the order of events across different processes, you'll need vector clocks.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#leader-election","title":"Leader Election","text":"<p>There are use-cases where 1 among N processes needs to gain exclusive rights to accessing a shared resource or to assign work to others.</p> <p>To achieve this, one needs to implement a leader-election algorithm - to elect a single process to be a leader among a group of equally valid candidates.</p> <p>There are two properties this algorithm needs to sustain:  * Safety - there will always be one leader elected at a time.  * Liveness - the process will work correctly even in the presence of failures.</p> <p>This chapter explores a particular leader-election algorithm - Raft, and how it guarantees these properties.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#raft-leader-election","title":"Raft leader election","text":"<p>Every process is a state machine with one of three states:  * follower state - process recognizes another process as a leader  * candidate state - process starts a new election, proposing itself as a leader  * leader state - process is the leader</p> <p>Time is divided into election terms, numbered with consecutive integers (logical timestamp). A term begins with a new election - one or more candidates attempt to become the leader.</p> <p>When system starts up, processes begin their journey as followers.  As a follower, the process expects a periodic heartbeat from the leader. If one does not arrive, a timeout begins ticking after which the process starts a new election &amp; transitions into the candidate state.</p> <p>In the candidate state, the process sends a request to all other notes, asking them to vote for them as the new leader.</p> <p>State transition happens when:  * Candidate wins the election - majority of processes vote for process, hence it wins the election.  * Another process wins - when a leader heartbeat is received \\w election term index <code>greater than or equal</code> to current one.  * No one wins election - due to split vote. Then, after a random timeout (to avoid consecutive split votes), a new election starts. </p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#practical-considerations","title":"Practical considerations","text":"<p>There are also other leader election algorithms but Raft is simple &amp; widely used.</p> <p>In practice, you would rarely implement leader election from scratch, unless you're aiming to avoid external dependencies.</p> <p>Instead, you can use any fault-tolerant key-value store \\w support for TTL and linearizable CAS (compare-and-swap) operations. This means, in a nutshell, that operations are atomic &amp; there is no possibility for race conditions.</p> <p>However, there is a possibility for race conditions after you acquire the lease.</p> <p>It is possible that there is some network issue during which you lose the lease but you still think you're the leader. This has lead to big outages in the past</p> <p>As a rule of thumb, leaders should do as little work as possible &amp; we should be prepared to occasionally have more than one leaders.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#replication","title":"Replication","text":"<p>Data replication is fundamental for distributed systems.</p> <p>One reason for doing that is to increase availability. If data is stored on a single process and it goes down, the data is no longer available.</p> <p>Another reason is to scale - the more replicas there are, the more clients can be supported concurrently.</p> <p>The challenge of replication is to keep them consistent with one another in the event of failure. The chapter will explore Raft's replication algorithm which provides the strongest consistency guarantee - to clients, the data appears to be stored on a single process.</p> <p>Paxos is a more popular protocol but we're exploring Raft as it's simpler to understand.</p> <p>Raft is based on state machine replication - leader emits events for state changes &amp; all followers update their state when the event is received.</p> <p>As long as the followers execute the same sequence of operations as the leader, they will end up with the same state.</p> <p>Unfortunately, just broadcasting the event is not sufficient in the event of failures, eg network outages.</p> <p>Example state-machine replication with a distributed key-value store:  * KV store accepts operations put(k, v) and get(k)  * State is stored in a dictionary</p> <p>If every node executes the same sequence of puts, the state is consistent across all of them.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#state-machine-replication","title":"State machine replication","text":"<p>When system starts up, leader is elected using Raft's leader election algorithm, which was already discussed.</p> <p>State changes are persisted in an event log which is replicated to followers. </p> <p>Note: The number in the boxes above the operation is the leader election term.</p> <p>The replication algorithm works as follows:  * Only the leader can accept state updates  * When it accepts an update, it is persisted in the local log, but the state is not changed  * The leader then sends an <code>AppendEntries</code> request to all followers \\w the new entry. This is also periodically sent for the purposes of heartbeat.  * When a follower receives the event, it appends it to its local log, but not its state. It then sends an acknowledge message back to leader.  * Once leader receives enough confirmations to form a quorum (&gt;50% acks from nodes), the state change is persisted to local state.  * Followers also persist the state change on a subsequent <code>AppendEntries</code> request which indicates that the state is changed.</p> <p>Such a system can tolerate F failures where total nodes = 2F + 1.</p> <p>What if a failure happens?</p> <p>If the leader dies, an election occurs. However, what if a follower with less up-to-date log becomes a leader? To avoid this, a node can't vote for another candidate if the candidate's log is less up-to-date.</p> <p>What if a follower dies &amp; comes back alive afterwards?  * They will update their log after receiving an <code>AppendEntries</code> message from leader. That message also contains the index of the second to last log.  * If the record is already present in the log, the follower ignores the message.  * If the record is not present but the second to last log entry is, the new entry is appended.  * If neither are present, the message is rejected.  * When the message is rejected, the leader sends the last two messages. If those are also rejected, the last three are sent and so forth.  * Eventually, the follower will receive all log entries they've missed and make their state up-to-date.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#consensus","title":"Consensus","text":"<p>Solving state machine replication lead us to discover a solution to an important distributed systems problem - achieving consensus among a group of nodes.</p> <p>Consensus definition:  * every non-faulty process eventually agrees on a value  * the final decision of non-faulty processes is the same everywhere  * the value that has been agreed on has been proposed by a process</p> <p>Consensus use-case example - agreeing on which process in a group gets to acquire a lease. This is useful in a leader-election scenario.</p> <p>Realistically, you won't implement consensus yourself. You'll use an off-the-shelf solution. That, however, needs to be fault-tolerant.</p> <p>Two widely used options are etcd and zookeeper. These data stores replicate their state for fault-tolerance.</p> <p>To implement acquiring a lease, clients can attempt to create a key \\w a specific TTL in the key-value store.  Clients can also subscribe to state changes, allowing them to reacquire the lease if the leader dies.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#consistency-models","title":"Consistency models","text":"<p>Let's take a closer look at what happens when a client sends a request to a replicated data store.</p> <p>Ideally, writes are instant: </p> <p>In reality, writes take some time: </p> <p>In a replicated data store, writes need to go through the leader, but what about reads?  * If reads are only served by the leader, then clients will always get the latest state, but throughput is limited  * If reads are served by leaders and followers, throughput if higher, but two clients can get different views of the system, in case a follower is lagging behind the leader.</p> <p>Consistency models help define the trade-off between consistency and performance.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#strong-consistency","title":"Strong Consistency","text":"<p>If all reads and writes go through the leader, this can guarantee that all observers will see the write after it's persisted. </p> <p>This is a <code>strong consistency</code> model.</p> <p>One caveat is that a leader can't instantly confirm a write is complete after receiving it. It must first confirm that they're still the leader by sending a request to all followers. This adds up to the time required to serve a read.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#sequential-consistency","title":"Sequential consistency","text":"<p>Serializing all reads through the leader guarantees strong consistency but limits throughput.</p> <p>An alternative is to enable followers to serve reads and <code>attach clients to particular followers</code>.  This will lead to an issue where different clients can see a different state, but operations will always occur in the same order. </p> <p>This is a <code>sequential consistency</code> model.</p> <p>Example implementation is a producer/consumer system synchronized with a queue. Producers and consumers always see the events in the same order, but at different times.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#eventual-consistency","title":"Eventual consistency","text":"<p>In the sequential consistency model, we had to attach clients to particular clients to guarantee in-order state changes. This will be an issue, however, when the follower a client is pinned to is down. The client will have to wait until the follower is back up.</p> <p>An alternative is to allow clients to use any follower.</p> <p>This comes at a consistency price. If follower B is lagging behind follower A and a client first queries follower A and then follower B, they will receive an earlier version of the state.</p> <p>The only guarantee a client has is that it will eventually see the latest state. This consistency model is referred to as <code>eventual consistency</code>.</p> <p>Using an eventually consistent system can lead to subtle bugs, which are hard to debug. However, not all applications need a strongly consistent system. If you're eg tracking the number of views on a youtube video, it doesn't matter if you display it wrong by a few users more or less.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#the-cap-theorem","title":"The CAP Theorem","text":"<p>When a network partition occurs - ie parts of the system become disconnected, the system has two choices:  * Remain highly available by allowing clients to query followers &amp; sacrifice strong consistency.  * Guarantee strong consistency by declining reads that can't reach the leader.</p> <p>This dilemma is summarized by the CAP theorem - \"strong consistency, availability and fault-tolerance. You can only pick two.\"</p> <p>In reality, the choice is only among strong consistency and availability as network faults will always happen. Additionally, consistency and availability are not binary choices. They're a spectrum. </p> <p>Also, even though network partitions can happen, they're usually rare.  But even without them, there is a trade-off between consistency and latency - The stronger the consistency is, the higher the latency is.</p> <p>This relationship is expressed by the PACELC theorem:  * In case of partition (P), one has to choose between availability (A) and consistency (C).  * Else (E), one has to choose between latency (L) and consistency (C).</p> <p>Similar to CAP, the choices are a spectrum, not binary.</p> <p>Due to these considerations, there are data stores which come with counter-intuitive consistency guarantees in order to provide strong availability and performance. Others allow you to configure how consistent you want them to be - eg Amazon's Cosmos DB and Cassandra.</p> <p>Taken from another angle, PACELC implies that there is a trade-off between the required coordination and performance. One way to design around this is to move coordination away from the critical path.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#chain-replication","title":"Chain Replication","text":"<p>Chain replication is a different kettle of fish, compared to leader-based replication protocols such as Raft.  * With this scheme, processes are arranged in a chain. Leftmost process is the head, rightmost one is the tail.  * Clients send writes to the head, it makes the state change and forwards to the next node  * Next node persists &amp; forwards it to the next one. This continues until the tail is reached.  * After the state update reaches the tail, it acknowledges the receipt back to the previous node. It then acknowledges receipt to previous one and so forth.  * Finally, the head receives the confirmation and informs the client that the write is successful.</p> <p>Writes go exclusively through the head. Reads, on the other hand, are served exclusively by the tail. </p> <p>In the absence of failures, this is a strongly-consistent model. What happens in the event of a failure?</p> <p>Fault-tolerance is dedicated to a separate component, called the control plane. It is replicated (to enable strong consistency) and can eg use Raft for consensus.</p> <p>The control plane monitors the system's health and reacts when a node dies:  * If head fails, a new head is appointed and clients are notified of the change.    * If the head persisted a write without forwarding, no harm is done because client never received acknowledgment of successful write.  * If the tail fails, its predecessor becomes the new tail.    * All updates received by the tail must necessarily be acknowledged by the predecessors, so consistency is sustained.  * If an intermediate node dies, its predecessor is linked to its successor.    * To sustain consistency, the failing node's successor needs to communicate the last received change to the control plane, which is forwarded to the predecessor.</p> <p>Chain replication can tolerate up to N-a failures, but the control plane can tolerate only up to C/2 failures.</p> <p>This model is more suited for strong consistency than Raft, because read/write responsibilities are split between head and tail and the tail need not contact any other node before answering a read response.</p> <p>The trade-off, though, is that writes need to go through all nodes. A single slow node can slow down all the writes. In contrast, Raft doesn't require all nodes to acknowledge a write. In addition to that, in case of failure, writes are delayed until the control plane detects the failure &amp; mitigates it.</p> <p>There are a few optimizations one can do with chain replication:  * write requests can be pipelined - ie, multiple write requests can be in-flight, even if they are dependent on one another.  * read throughput can be done by other replicas as well while guaranteeing strong consistency    * To achieve this, replicas mark an object as dirty if a write request for it passed through them without a subsequent acknowledgment.    * If someone attempts to read a dirty object, the replica first consults the tail to verify if the write is committed. </p> <p>As previously discussed, leader-based replication presents a scalability issue. But if the consensus coordination is delegated away from the critical path, you can achieve higher throughput and consistency for data-related operations.</p> <p>Chain replication delegates coordination &amp; failure recovery concerns to the control plane.</p> <p>But can we go further? Can we achieve replication without the need for consensus? </p> <p>That's the next chapter's topic.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#coordination-avoidance","title":"Coordination avoidance","text":"<p>There are two main ingredients to state-machine replication:  * A broadcast protocol which guarantees every replica eventually receives all updates in the same order  * A deterministic update function for each replica</p> <p>Implementing fault-tolerant total order requires consensus. It is also a scalability bottleneck as it requires one process to handle updates at a time.</p> <p>We'll explore a form of replication which doesn't require total order but still has useful guarantees.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#broadcast-protocols","title":"Broadcast protocols","text":"<p>Network communication over wide area networks like the internet offer unicast protocols only (eg TCP).</p> <p>To broadcast a message to a group of receivers, a multicast protocol is needed. Hence, we need to implement a multicast protocol over a unicast one.</p> <p>The challenge is supporting multiple senders and receivers that can crash at any point.</p> <p>Best-effort broadcast guarantees that a message will be deliver to all non-faulty receivers in a group. One way to implement it is by sending a message to all senders one by one over reliable links. If, however, the sender fails midway, the receiver will never get the message. </p> <p>Reliable broadcast guarantees that the message is eventually delivered to all non-faulty processes even if the sender crashes before the message is fully delivered. One way to implement it is to make every sender that receives a message retransmit it to the rest of the group.  This is known as eager reliable broadcast and is costly as it takes sending the message N^2 times. </p> <p>Gossip broadcast optimizes this by sending the message to a subset of nodes. This approach is probabilistic. It doesn't guarantee all nodes get the message, but you can minimize the probability of non-reception by tuning the config params. This approach is particularly useful when there are large number of nodes as deterministic protocols can't scale to such amounts. </p> <p>Reliable broadcast protocols guarantee messages are delivered to non-faulty processes, but they don't make ordering guarantees. Total order broadcast guarantees both reception &amp; order of messages, but it requires consensus, which is a scalability bottleneck.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#conflict-free-replicated-data-types","title":"Conflict-free replicated data types","text":"<p>If we implement replication without guaranteeing total order, we don't need to serialize all writes through a single node.</p> <p>However, since replicas can receive messages in different order, their states can diverge. Hence, for this to work, divergence can only be temporary &amp; the replica states need to converge back to the same state.</p> <p>This is the main mechanism behind eventual consistency: * Eventual delivery guarantee - every update applied to a replica is eventually applied to all replicas * Convergence guarantee - replicas that have applied the same updates eventually reach the same state</p> <p>Using a broadcast protocol that doesn't guarantee message order can lead to divergence. You'll have to reconcile conflicting writes via consensus that all replicas need to agree to. </p> <p>This approach has better availability and performance than total order broadcast. Consensus is only required to reconcile conflicts &amp; can be moved off the critical path.</p> <p>However, there is a way to resolve conflicts without using consensus at all - eg the write with the latest timestamp wins. If we do this, conflicts will be resolved by design &amp; there is no need for consensus.</p> <p>This replication strategy offers stronger guarantees than eventual consistency:  * Eventual delivery - same as before  * Strong convergence - replicas that have executed the same updates have the same state</p> <p>This is referred to as strong eventual consistency.</p> <p>There are certain conditions to guarantee that replicas strongly converge.</p> <p>Consider a replicated data store which supports query and update operations and behaves in the following way:  * When a query is received, a replica immediately replies with its local state  * When an update is received, it is first applied to local state and then broadcast to all replicas  * When a broadcast message is received, the received state is merged with its own</p> <p>If two replicas converge, one way to resolve the conflicts is to have an arbiter (consensus) which determines who wins.</p> <p>Alternatively, convergence can be achieved without an arbiter if these two conditions are met:  * The object's possible states form a semilattice (set whose elements can be partially ordered)  * The merge operation returns the least upper bound between two object states. Hence, it needs to be idempotent, commutative and associative.</p> <p>Note: The next couple of lines are personal explanation which might not be 100% correct according to mathy folks, but suffices for my own understanding.</p> <p>What this means is that if you visualize a graph of an object's ordered states, if you take two of them (c and d), you might not immediately determine which one is greater, but there is eventually some state which is \"greater\" than both of the non-comparable states (e). </p> <p>Put into practice, if you have two replicas, whose states have diverged (c and d), eventually, they will converge back to the same value (e).</p> <p>The second part of the requirement mandates that the operation for merging two states needs to be:  * Idempotent: x + x = x  * Commutative: (x + y) + z = x + (y + z)  * Associative: x + y = y + x</p> <p>This is because you're not guaranteed that you'll always get to merge two states in the same order (associative &amp; commutative) and you might get a merge request for the same states twice (idempotent).</p> <p>A data type which has this property is referred to as a convergent replicated data type which is part of the family of conflict-free replicated data types (CRDT).</p> <p>Example - integers can be (partially) ordered and the merge operation is taking the maximum of two ints (least upper bound).</p> <p>Replicas will always converge to the global maximum even if requests are delivered out of order and/or multiple times. With this scheme, replicas can also use an unreliable broadcast protocol as long as they periodically broadcast their states &amp; merge them. (anti-entropy mechanism).</p> <p>Some data types which converge when replicated - registers, counters, sets, dictionaries, graphs.</p> <p>Example - registers are a memory cell, containing a byte array.  They can be made convergent by defining partial order and merge operation.</p> <p>Two common implementations are last-writer-wins registers (LWW) and multi-value registers (MV).</p> <p>LWW registers associate a timestamp with every update, making them order-able. The timestamp can be a Lamport timestamp to preserve happened-before relationships with a replica identifier to break ties.</p> <p>A lamport clock is sufficient because it guarantees happens-before relationships between dependent events &amp; the updates we have are dependent. </p> <p>The issue with LWW registers is that when there are concurrent updates, one of them randomly wins.</p> <p>An alternative is to store both states in the register &amp; let applications determine which update to take. That's what a MV register does. It uses a vector clock and the merge operation returns the union of all concurrent updates. </p> <p>CRDTs can be composed - eg, you can have a dictionary of LWW or MV registers. Dynamo-style data stores leverage this.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#dynamo-style-data-stores","title":"Dynamo-style data stores","text":"<p>Dynamo is the most popular eventually consistent and highly available key-value store.</p> <p>Others are inspired by it - Cassandra &amp; Riak KV.</p> <p>How it works:  * Every replica accepts read and write requests  * When a client wants to write, it sends the write request to all N replicas, but waits for acknowledgement from only W replicas (write quorum).  * Reads work similarly to writes - sent to all replicas (N), sufficient to get acknowledgement by a subset of them (R quorum).</p> <p>To resolve conflicts, entries behave like LWW or MV registers, depending on the implementation you choose.</p> <p>W and R are configurable based on your needs:  * Stronger consistency - When <code>W + R &gt; N</code>, at least one read will always return the latest version.*   * This is not guaranteed on its own. Writes sent to N replicas might not make it to all of them. To make this work, writes need to be bundled in an atomic transaction.  * Max performance \\w consistency cost - <code>W + R &lt; N</code>.</p> <p>You can fine-tune read/write performance while preserving strong consistency:  * When R is small, reads are fast at the expense of slower writes and vice versa, assuming you preserve <code>W + R &gt; N</code>. </p> <p>This mechanism doesn't guarantee that all replicas will converge on its own. If a replica never receives a write request, it will never converge.</p> <p>To solve this issue, there are two possible anti-entropy mechanisms you can use:  * Read repair - if a client detects that a replica doesn't have the latest version of a key, it sends a write request to that replica.  * Replica synchronization - Replicas periodically exchange state information to notify each other of their latest states. To minimize amount of data transmitted, replicas exchange merkle tree hashes instead of the key-value pairs directly.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#the-calm-theorem","title":"The CALM theorem","text":"<p>When does an application need coordination (ie consensus) and when can it use an eventually consistent data store?</p> <p>The CALM theorem states that a program can be consistent and not use coordination if it is monotonic.</p> <p>An application is monotonic if new inputs further refine the output vs. taking it back to a previous state.  * Example monotonic program - a counter, which supports increment(n) operations, eg inc(1), inc(2) = 3 == inc(2), inc(1) = 3  * Example non-monotonic program - arbitrary variable assignment, eg set(5), set(3) = 3 != set(3), set(5) = 5    * Bear in mind, though, that if you transform your vanilla register to eg a LWW or MV register, you can make it monotonic. </p> <p>A monotonic program can be consistent, available and partition tolerant all at once.</p> <p>However, consistent in the context of CALM is different from consistent in the context of CAP.</p> <p>CAP refers to consistency in terms of reads and writes. CALM refers to consistency in terms of program output.</p> <p>It is possible to build applications which are consistent at the application-level but inconsistent on the storage level.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#causal-consistency","title":"Causal consistency","text":"<p>Eventual consistency can be used to write consistent, highly available and partition-tolerant applications as long as they're monotonic.</p> <p>For many applications, though, eventual consistency guarantees are insufficient. Eventual consistency doesn't guarantee that an operation which happened-before another is observed in the same order.</p> <p>Eg uploading an image and referencing it in a gallery. In an eventually consistent system, you might get an empty image placeholder for some time.</p> <p>Strong consistency helps as it guarantees that if operation B happens, operation A is guaranteed to be observed.</p> <p>There is an alternative which is not strongly consistent but good enough to guarantee happens-before relationships - causal consistency. Causal consistency imposes a partial order on operations, while strong consistency imposes global order.</p> <p>Causal consistency is the strongest consistency model which enables building highly available and partition tolerant systems.</p> <p>The essence of this model is that you as a client only care about happens-before relationships of the operations concerning you, rather than all operations within the system.</p> <p>Example:  * Client A writes a value (operation A)  * Client B reads the same value (operation B)  * Client B writes another value (operation C)  * Client C writes an unrelated value (operation D)</p> <p>With a causally consistent system, the system guarantees that operation C happens-before A, hence any client which queries the system must receive operations A and C in this order.</p> <p>However, it doesn't guarantee the order of operation D in relation to the other operations. Hence, some clients might read operation A and D in this order, others in reverse order.</p> <p>COPS is a causally-consistent key-value store.  * clients can make read/write requests to any replica.  * When a client receives a read request, it keeps track of the returned value's version in a local key-version dictionary to keep track of dependencies  * Writes are accompanied by copies of the locally-stored key-version dictionary.   * Replica assigns a version to the write and sends back the new version to the client  * The write is propagated to the other replicas  * When a replica receives a write request, it doesn't commit it immediately.   * It first checks if all the write's dependencies are committed locally. Write is only committed when all dependencies are resolved. </p> <p>One caveat is that there is a possibility of data loss if a replica commits a write locally but fails before broadcasting it to the rest of the nodes.</p> <p>This is considered acceptable in COPS case to avoid paying the price of waiting for long-distance requests before acknowledging a write.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#practical-considerations_1","title":"Practical considerations","text":"<p>In summary, replication implies that we have to choose between consistency and availability.</p> <p>In other words, we must minimize coordination to build a scalable system.</p> <p>This limitation is present in any large-scale system and there are data stores which allow you to control it - eg Cosmos DB enables developers to choose among 5 consistency models, ranging from eventual consistency to strong consistency, where weaker consistency models have higher throughput.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#transactions","title":"Transactions","text":"<p>Transactions allow us to execute multiple operations atomically - either all of them pass or none of them do.</p> <p>If the application only deals with data within a SQL database, bundling queries into a transaction is straightforward. However, if operations are performed on multiple data stores, you'll have to execute a distributed transaction which is a lot harder.</p> <p>In microservice environments, however, this scenario is common.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#acid","title":"ACID","text":"<p>If you eg have to execute a money transfer from one bank to another and the deposit on the oher end fails, then you'd like your money to get deposited back into your account.</p> <p>In a traditional database, transactions are ACID:  * Atomicity - partial failures aren't possible. Either all operations succeed or all fail.  * Consistency - transactions can only transition a database from one valid state to another. The application cannot be left in an invalid state.  * Isolation - concurrent execution of transactions doesn't cause race conditions. It appears as if transactions execute sequentially when they actually don't.  * Durability - if a transaction is committed, its changes are persisted &amp; there is no way for the database to lose those changes.</p> <p>Let's explore how transactions are implemented in centralized, non-distributed databases before going into distributed transactions.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#isolation","title":"Isolation","text":"<p>One way to achieve isolation is for transactions to acquire a global lock so that only one transaction executes at a time. This will work but is not efficient.</p> <p>Instead, databases do let transactions run concurrently, but this can lead to all sorts of race conditions:  * dirty write - transaction overwrites the value written by another transaction which is not committed yet.  * dirty read - transaction reads a value written by a transaction which is not committed yet.  * fuzzy read - transaction reads a value twice &amp; sees different values because another transaction modified the value in-between.  * phantom read - transactions read a group matching a given criteria, whilst another transaction changes objects due to which the original matched group changes.</p> <p>To prevent this, transactions need to be isolated from one another. This is fine-tuned by the transaction's isolation level.</p> <p>The stronger the isolation level, the stronger the protection against any of the above issues is. However, transactions are less performant as a result. </p> <p>Serializability is the strongest isolation level - it guarantees that executing a group of transactions has the same effect as executing them sequentially. It guarantees strong consistency, but it involves coordination, which increases contention &amp; hence, transactions are less performant.</p> <p>Our goal as developers is to maximize concurrency while preserving the appearance of serial execution.</p> <p>The concurrency strategy is determined by a concurrency control protocol and there are two kinds - pessimistic and optimistic.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#concurrency-control","title":"Concurrency control","text":"<p>Pessimistic protocols use locks to block other transactions from accessing a shared object.</p> <p>The most common implementation is two-phase locking (2PL):  * Read locks can be acquired by multiple transactions on an object. It is released once all transactions holding the lock commit.  * Read locks block transactions which want to acquire a write lock.  * Write locks are held by one transaction only. It prevents read locks from getting acquired on the locked object. It is released once the transaction commits.</p> <p>With 2PL locking, it is possible to create a deadlock - tx A locks object 1 and wants to read object 2, while tx B locks object 2 and wants to read object 1.</p> <p>A common approach to handling deadlocks is to detect them, after which a \"victim\" transaction is chosen &amp; aborted.</p> <p>Optimistic protocols, on the other hand, works on the principle of (optimistically) attempting to execute transactions concurrently without blocking and if it turns out a value used by a transaction was mutated, that transaction is restarted.</p> <p>This relies on the fact that transactions are usually short-lived and collisions rarely happen.</p> <p>Optimistic concurrency control (OCC) is the most popular optimistic protocol. In OCC, transactions make changes to a local data store. Once it attempts to commit, it checks if the transaction's workspace collides with another transaction's workspace and if so, the transaction is restarted.</p> <p>Optimistic protocols are usually more performant than pessimistic protocols and are well suited for workflows with a lot of reads and hardly any writes. Pessimistic protocols are more appropriate for conflict-heavy workflows as they avoid wasting time doing work which might get discarded.</p> <p>Both protocols, however, are not optimal for read-only transactions:  * With pessimistic protocols, a read-only tx might have to wait for a long time to acquire a read lock.  * With optimistic protocols, a read-only tx might get aborted because the value it reads gets written in the middle of it.</p> <p>Multi-version concurrency control (MVCC) aims to address this issue:  * Version tags are maintained for each record.  * When a write happens, a new version tag is created  * When a read happens, the newest version since the transaction started is read  * For write operations, the protocol falls back to standard pessimistic or optimistic concurrency.</p> <p>This allows read-only transactions to not get blocked or aborted as they don't conflict with write transactions and is a major performance improvement. This is why MVCC is the most widely used concurrency control scheme nowadays. The trade-off is that your data store becomes bigger as it needs to tore version tags for each record.</p> <p>A limited form of OCC is used in distributed applications:  * Each object has a version number.  * A transaction assigns a new version number to the object &amp; proceeds with the operation.  * Once it is ready to commit, the object is only updated if the original version tag hasn't changed.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#atomicity","title":"Atomicity","text":"<p>Either all operations within a transaction pass or all of them fail.</p> <p>To guarantee this, data stores record changes in a write-ahead log (WAL) persisted on disk before applying the operations. Each log entry contains the transaction id, the id of the modified object and both the old and new values.</p> <p>In most circumstances, that log is not read. But if the data store crashes before a transaction finishes persisting the changes, the database recovers the latest state by reading the WAL. Alternatively, if a transaction is rollbacked, the log is used to undo the persisted changes.</p> <p>This WAL-based recovery mechanism guarantees atomicity only within a single data store. If eg you have two transactions which span different data store (money transfer from one bank to the other), this mechanism is insufficient.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#two-phase-commit","title":"Two-phase commit","text":"<p>Two-phase commit (2PC) is a protocol used to implement atomic transactions across multiple processes.</p> <p>One process is a coordinator which orchestrates the actions of all other processes - the participants.</p> <p>Eg, the client initiating a transaction can act as a coordinator.</p> <p>How it works:  * Coordinator sends prepare request to participants - asking them if they are ready to commit.  * If all of them are, coordinator then sends commit request to execute the commits.  * If not all processes reply or aren't prepared for some reason, coordinator sends an abort request. </p> <p>There are two critical points in the above flow:  * If a participants says it's prepared, it can't move forward without receiving a commit or abort request. This means that a faulty coordinator can make the participant stuck.  * Once a coordinator decides to commit or abort a transaction, it has to see it through no matter what. If a participant is temporarily down, the coordinator has to retry.</p> <p>2PC has a mixed reputation:  * It's slow due to multiple round trips for the same transaction  * If a process crashes, all other processes are blocked.</p> <p>One way to mitigate 2PC's shortcomings is to make it resilient to failures - eg replicate coordinator via Raft or participants via primary-secondary replication.</p> <p>Atomically committing a transaction is a type of consensus called uniform consensus - all processes have to agree on a value, even faulty ones. In contrast, standard consensus only guarantees that non-faulty processes agree on a value, meaning that uniform consensus is harder to implement.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#newsql","title":"NewSQL","text":"<p>Originally, we had SQL databases, which offered ACID guarantees but were hard to scale. In late 2000s, NoSQL databases emerged which ditched ACID guarantees in favor of scalability.</p> <p>So for a long time, one had to choose between scale &amp; strong consistency. Nowadays, a new type of databases emerged which offer both strong consistency guarantees and high scalability. They're referred to as NewSQL.</p> <p>One of the most successful implementations is Google's Spanner:  * It breaks key-value pairs into partitions in order to scale.  * Each partition is replicated among a group of nodes using state machine replication (Paxos).  * In each such group (replication group) there is a leader.  * The leader applies client write requests for that partition by replicating it among the majority of nodes &amp; then applying it.  * The leader is also a lock manager, which implements 2PL to isolate transactions on the same partition from one another.  * For multiple partition transactions, Spanner implements 2PC. The coordinator is one of the involved partitions' leader.  * The transaction is logged into a WAL. If the coordinator crashes, one of the other nodes in the replication group picks up from where they left of using the WAL.  * Spanner uses MVCC for read-only transactions and 2PL for write transactions to achieve maximum concurrency.  * MVCC is based on timestamping records using physical clocks. This is easy to do on a single machine, but not as easy in a distributed system.  * To solve this, Spanner calculates the possible timestamp offset &amp; transactions wait for the longest possible offset to avoid future transactions to be logged with earlier timestamps.  * This mechanism means that the timestamp offset needs to be as low as possible to have fast transactions. Therefore, Google has deployed very accurate GPS and atomic clocks in every data center. </p> <p>Another system inspired by Spanner is CockroachDB which works in a similar way, but uses hybrid-logical clocks to avoid having to provision highly costly atomic clocks.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#asynchronous-transactions","title":"Asynchronous transactions","text":"<p>2PC is synchronous &amp; blocking. It is usually combined with 2PL to provide isolation.</p> <p>If any of the participants is not available, the transaction can't make progress.</p> <p>The underlying assumption of 2PC is that transactions are short-lived and participants are highly available. We can control availability, but some transactions are inherently slow.</p> <p>In addition to that, if the transaction spans several organizations, they might be unwilling to grant you the power to block their systems.</p> <p>Solution from real world - fund transfer:  * Asynchronous &amp; atomic  * Check is sent from one bank to another and it can't be lost or deposited more than once.  * While check is in transfer, bank accounts are in an inconsistent state (lack of isolation).</p> <p>Checks are an example of persistent messages - in other words, they are processed exactly once.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#outbox-pattern","title":"Outbox Pattern","text":"<p>Common issue - persist data in multiple data stores, eg Database + Elasticsearch.</p> <p>The problem is the lack of consistency between the non-related services.</p> <p>Solution - using the outbox pattern:  * When saving the data in DB, save it to the designated table + a special \"outbox\" table.  * A process periodically starts which queries the outbox table &amp; sends pending messages to the second data store.  * A message channel such as Kafka can be used to achieve idempotency &amp; guaranteed delivery.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#sagas","title":"Sagas","text":"<p>Problem - we're a travel booking service which coordinates booking travels + hotels via separate third-party services.</p> <p>Bookings must happen atomically &amp; both the travel &amp; hotel booking services can fail.</p> <p>Booking a trip requires several steps to complete, some of which are required on failure.</p> <p>The saga pattern provides a solution:  * A distributed transaction is composed of several local transactions, each of which has a compensating transaction which reverses it.  * The saga guarantees that either all transactions succeed or in case of failure, the compensating transactions reverse the partial result.</p> <p>A saga can be implemented via an orchestrator, who manages execution of local transactions across the involved processes.</p> <p>In our example, the travel booking saga can be implemented as follows:  * T1 is executed - flight is booked via third-party service.  * T2 is executed - hotel is booked via third-party service.  * If there are any failures, execute compensating transactions C2 and C1:    * C1 - cancel flight via third-party service API.    * C2 - cancel hotel booking via third-party service API.</p> <p>The orchestrator can communicate via message channels (ie Kafka) to tolerate temporary failures. The requests need to be idempotent to tolerate temporary failures. It also has to checkpoint a transaction's intermediary state to persistent storage in case the orchestrator crashes. </p> <p>In practice, one doesn't need to implement workflow engines from scratch. One can leverage workflow engines such as Temporal which already do this for you.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#isolation_1","title":"Isolation","text":"<p>A sacrifice we endured when we used distributed transactions is the lack of isolation - distributed transactions operate on the same data &amp; there is no isolation between them.</p> <p>One way to work around this is to use \"semantic locks\" - the data used by a transaction is marked as \"dirty\" and is unavailable for use by other transactions.</p> <p>Other transactions reliant on the dirty data can either fail &amp; rollback or wait until the flag is cleared.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part02/#summary_1","title":"Summary","text":"<p>Takeaways:  * Failures are unavoidable - systems would be so much simpler if they needn't be fault tolerant.  * Coordination is expensive - keep it off the critical path when possible.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/","title":"Scalability","text":"<ul> <li>HTTP Caching</li> <li>Content delivery networks</li> <li>Partitioning</li> <li>File storage</li> <li>Network load balancing</li> <li>Data storage</li> <li>Caching</li> <li>Microservices</li> <li>Control planes and data planes</li> <li>Messaging</li> <li>Summary</li> </ul> <p>Over the last decades, the number of people with internet access has risen. This has lead to the need for businesses to handle millions of concurrent users.</p> <p>To scale, an application must run without performance degradation. The only long-term solution is to scale horizontally.</p> <p>This part focuses on scaling a small CRUD application:  * REST API  * SPA JavaScript front-end  * Images are stored locally on the server  * Both database &amp; application server are hosted on the same machine - AWS EC2  * Public IP Address is managed by a DNS service - AWS Route53 </p> <p>Problems:  * Not scalable  * Not fault-tolerant</p> <p>Naive approach to scaling - scale up by adding more CPU, RAM, Disk, etc.</p> <p>Better alternative is to scale out - eg move the database to a dedicated server. </p> <p>This increases the capacity of both the server and the database.  This technique is called functional decomposition - application is broken down to independent components with distinct responsibilities.</p> <p>Other approaches to scaling:  * Partitioning - splitting data into partitions &amp; distributing it among nodes.  * Replication - replicating data or functionality across nodes.</p> <p>The following sections explore different techniques to scale.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#http-caching","title":"HTTP Caching","text":"<p>Cruder handles static &amp; dynamic resources:  * Static resources don't change often - HTML, CSS, JS files  * Dynamic resources can change - eg a user's profile JSON</p> <p>Static resources can be cached since they don't change often. A client (ie browser) can cache the resource so that subsequent access doesn't make network calls to the server.</p> <p>We can leverage HTTP Caching, which is limited to GET and HEAD HTTP methods.</p> <p>The server issues a <code>Cache-Control</code> header which tells the browser how to handle the resource: </p> <p>The max-age is the TTL of the resource and the ETag is the version identifier.  The age is maintained by the cache &amp; indicates for how long has the resource been cached.</p> <p>If a subsequent request for the resource is received, the resource is served from the cache as long as it is still fresh - TTL hasn't expired. If, however, the resource has changed on the server in the meantime, clients won't get the latest version immediately.</p> <p>Reads, therefore, are not strongly consistent.</p> <p>If a resource has expired, the cache will forward a request to the server asking if it's change. If it's not, it updated its TTL: </p> <p>One technique we can use is treating static resources as immutable. Whenever a resource changes, we don't update it. Instead, we publish a new file (\\w version tag) &amp; update references to it.</p> <p>This has the advantage of atomically changing all static resources with the same version.</p> <p>Put another way, with HTTP caching, we're treating the read path differently from the write path because reads are way more common than writes. This is referred to as the Command-Query Responsibility Segregation (CQRS) pattern.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#reverse-proxies","title":"Reverse Proxies","text":"<p>An alternative is to cache static resources on the server-side using reverse proxies.</p> <p>A reverse proxy is a server-side proxy which intercepts all client calls. It is indistinguishable from the actual server, therefore clients are unaware of its presence. </p> <p>A common use-case for reverse proxies is to cache static resources returned by the server.  Since this cache is shared among clients, it will reduce the application server's burden.</p> <p>Other use-cases for reverse proxies:  * Authenticate requests on behalf of the server.  * Compress a response before forwarding it to clients to speed up transmission speed.  * Rate-limit requests coming from specific IPs or users to protect the server from DDoS attacks.  * Load-balance requests among multiple servers to handle more load.</p> <p>NG-INX and HAProxy are popular implementations which are commonly used. However, caching static resources is commoditized by managed services such as Content delivery networks (CDNs) so we can just leverage those.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#content-delivery-networks","title":"Content delivery networks","text":"<p>CDN - overlay network of geographically distributed caches (reverse proxies), designed to workaround the network protocol.</p> <p>When you use CDN, clients hit URLs that resolve to the CDN's servers. If a requested resource is stored in there, it is given to clients. If not, the CDN transparently fetches it from the origin server.</p> <p>Well-known CDNs - Amazon CloudFront and Akamai.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#overlay-network","title":"Overlay network","text":"<p>The main benefit of CDNs is not caching. It's main benefit is the underlying network architecture.</p> <p>The internet's routing protocol - BGP, is not designed with performance in mind.  When choosing routes for a package, it takes into consideration number of hops instead of latency or congestion.</p> <p>CDNs are built on top of the internet, but exploit techniques to reduce response time &amp; increase bandwidth.</p> <p>It's important to minimize distance between server and client in order to reduce latency. Also, long distances are more error prone.</p> <p>This is why clients communicate with the CDN server closest to them.  One way to achieve that is via DNS load balancing - a DNS extension which infers a client's location based on its IP and returns a list of geographically closest servers.</p> <p>CDNs are also placed at internet exchange points - network nodes where ISPs intersect. This enables them to short-circuit communication between client and server via advanced routing algorithms. These algorithms take into consideration latency &amp; congestion based on constantly updated data about network health, maintained by the CDN providers. In addition to that, TCP optimizations are leveraged - eg pooling TCP connections on critical paths to avoid setting up new connections every time. </p> <p>Apart from caching, CDNs can be leveraged for transporting dynamic resources from client to server for its network transport efficiency. This way, the CDN effectively becomes the application's frontend, shielding servers from DDoS attacks.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#caching","title":"Caching","text":"<p>CDNs have multiple caching layers. The top one is at edge servers, deployed across different geographical regions.</p> <p>Infrequently accessed content might not be available in edge servers, hence, it needs to be fetched from the origin using the overlay network for efficiency.</p> <p>There is a trade-off though - more edge servers can reach more clients, but reduce the cache hit ratio.  Why? More servers means you'll need to \"cache hit\" a content in more locations, increasing origin server load.</p> <p>To mitigate this, there are intermediary caching layers, deployed in fewer geographical regions. Edge servers first fetch content from there, before going to the origin server.</p> <p>Finally, CDNs are partitioned - there is no single server which holds all the data as that's infeasible. It is dispersed across multiple servers and there's an internal mechanism which routes requests to the appropriate server, which contains the target resource.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#partitioning","title":"Partitioning","text":"<p>When an application's data grows, at some point it won't fit in a single server.  That's when partitioning can come in handy - splitting data (shards) across different servers.</p> <p>An additional benefit is that the application's load capacity increases since load is dispersed across multiple servers vs. a single one.</p> <p>When a client makes a request to a partitioned system, it needs to know where to route the request. Usually, a gateway service (reverse proxy) is in charge of this. Mapping of partitions to servers is usually maintained in a fault-tolerant coordination service such as zookeeper or etcd. </p> <p>Partitioning, unfortunately, introduces a fair amount of complexity:  * Gateway service is required to route requests to the right nodes.  * Data might need to be pulled from multiple partitions &amp; aggregated (eg joins across partitions)  * Transactions are needed to atomically update data across multiple partitions, which limits scalability.  * If a partition is much more frequently accessed than others, it becomes a bottleneck, limiting scalability.  * Adding/removing partitions at runtime is challenging because it involves reorganizing data.</p> <p>Caches are ripe for partitioning as they:  * don't need to atomically update data across partitions  * don't need to join data across partitions as caches usually store key-value pairs  * losing data due to changes in partition configuration is not critical as caches are not the source of truth</p> <p>In terms of implementation, data is distributed using two main mechanisms - hash and range partitioning. An important prerequisite for partitioning is for the number of possible keys be very large. Eg a boolean key is not appropriate for partitioning.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#range-partitioning","title":"Range partitioning","text":"<p>With this mechanism, data is split in lexicographically sorted partitions: </p> <p>To make range scanning easier, data within a partition is kept in sorted order on disk.</p> <p>Challenges:  * How to split partitions? - evenly distributing keys doesn't make sense in eg the english alphabet due to some letters being used more often. This leads to unbalanced partitions.  * Hotspots - if you eg partition by date &amp; most requests are for data in the current day, those requests will always hit the same partition. Workaround - add a random prefix at the cost of increased complexity.</p> <p>When data increases or shrinks, the partitions need to be rebalanced - nodes need to be added or removed. This has to happen in a way which minimizes disruption - minimizing data movement around nodes due to the rebalancing.</p> <p>Solutions:  * Static partitioning - define a lot of partitions initially &amp; nodes serve multiple partitions vs. a single one.     * drawback 1 - partition size is fixed     * drawback 2 - trade-off between lots of partitions &amp; too few. Too many -&gt; decreased performance. Too few -&gt; limit scalability.  * Dynamic partitioning - system starts with a single partition &amp; is rebalanced into sub-partitions over time.      * If data grows, partition is split into two sub-partitions.      * If data shrinks, sub-partitions are removed &amp; data is merged.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#hash-partitioning","title":"Hash partitioning","text":"<p>Use a hash function which deterministically maps a key to a seemingly random number, which is assigned to a partition. Hash functions usually distribute keys uniformly. </p> <p>Challenge - assigning hashes to partitions.  * Naive approach - use the module operator. Problem - rebalancing means moving almost all the data around.  * Better approach - consistent hashing.</p> <p>How consistent hashing works:  * partitions and keys are randomly distributed in a circle.  * Each key is assigned to the closest partition which appears in the circle in clockwise order. </p> <p>Adding a new partition only rebalances the keys which are now assigned to it in the circle: </p> <p>Main drawback of hash partitioning - sorted order is lost. This can be mitigated by sorting data within a partition based on a secondary key.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#file-storage","title":"File storage","text":"<p>Using CDNs enables our files to be quickly accessible by clients. However, our server will run out of disk space as the number of files we need to store increases.</p> <p>To work around this limit, we can use a managed file store such as AWS S3 or Azure Blob Storage.</p> <p>Managed file stores scalable, highly available &amp; offer strong durability guarantees. In addition to that, managed file stores enable anyone with access to its URL to point to it, meaning we can point our CDNs to the file store directly.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#blob-storage-architecture","title":"Blob storage architecture","text":"<p>This section explores how distributed blob stores work under the hood by exploring the architecture of Azure Storage.</p> <p>Side note - AS works with files, tables and queues, but discussion is only focused on the file (blob) store.</p> <p>AS Architecture in a nutshell:  * Composed of different racks of clusters, which are geographically distributed. Each node in a cluster has separate network &amp; (redundant) power supply.  * file URL are composed of account name + file name - https://{account_name}.blob.core.windows.net/{filename}  * Customer sets up account name &amp; AS DNS uses that to identify the storage cluster where the data is located. The filename is used to detect the correct node within the cluster.  * A location service acts as a control plane, which creates accounts &amp; allocates them to clusters.    * Creating a new account leads to the location service 1) choosing an appropriate cluster, 2) updating the cluster configuration to accept requests for that account name and 3) updates the DNS records   * The storage cluster itself is composed of a front-end, partition and stream layers. </p> <p>The Stream layer implements a distributed append-only file system in which the data is stored in \"streams\". A stream is represented as a sequence of \"extents\", which are replicated using chain replication. A stream manager acts as the control plane within the stream layer - it 1) allocates extents to the list of chained servers and 2) handles faults such as missing extent replicas.</p> <p>Partition layer is the abstraction where high-level file operations are translated to low-level stream operations. </p> <p>The partition layer also has a dedicated control plane, called the partition manager. It maintains an index of all files within the cluster. The partition manager range-partitions the index into separate partition servers. It also load-balances partitions across servers by splitting/merging them as necessary. The partition manager also replicates accounts across clusters for the purposes of migration in the event of disaster and for load-balancing purposes. </p> <p>Finally, the front-end layer is a stateless reverse proxy which authenticates requests and routes them to appropriate partitions.</p> <p>Side note - AS was strongly consisten from the get-go, while AWS S3 started offering strong consistency in 2021.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#network-load-balancing","title":"Network load balancing","text":"<p>So far we've scaled by offloading files into a dedicated file storage service + leveraging CDN for caching &amp; its enhanced networking.</p> <p>However, we still have a single application server. To avoid this, we can instantiate multiple application servers hidden behind a load balancer, which routes requests to them, balancing the load on each.</p> <p>This is possible because the application server is stateless. State is encapsulated within the database &amp; file store. Scaling out a stateful application is a lot harder as it requires replication &amp; coordination.</p> <p>General rule - push state to dedicated services, which are stable &amp; highly efficient.</p> <p>The load balancer will also track our application's health and stop routing requests to faulty servers. This leads to increased availability.</p> <p>Availability == probability of a valid request succeeding.</p> <p>How is theoretical availability calculated? - 1 - (product of the servers' failure rate).</p> <p>Eg. two servers with 99% availability == 1 - (0.01 * 0.01) == 99.99%.</p> <p>Caveats:  * There is some delay between a server crashing and a load balancer detecting it.  * Formula assumes failure rates are independent, but often times they're not.  * Removing a server from the pool increases the load on other servers, risking their failure rates increasing.</p> <p>Load balancing</p> <p>Example algorithms:  * Round robin  * Consistent hashing  * Taking into account server's load - this requires load balancers to periodically poll the server for its load    * This can lead to surprising behavior - server reports 0 load, it gets hammered with requests and gets overloaded.</p> <p>In practice, round robin achieves better results than load distribution.  However, an alternative algorithm which randomizes requests across the least loaded servers achieves better results.</p> <p>Service discovery This is the process load balancers use to discover the pool of servers under your application identifier.</p> <p>Approaches:  * (naive) maintain a static configuration file mapping application to list of IPs. This is painful to maintain.   * Use Zookeeper or etcd to maintain the latest configuration</p> <p>Adding/removing servers dynamically is a key functionality for implementing autoscaling, which is supported in most cloud providers.</p> <p>Health checks Load balancers use health checks to detect faulty servers and remove them from the pool.</p> <p>Types of health check:  * Passive - healh check is piggybacked on top of existing requests. Timeout or status 503 means the server is down and it is removed.  * Active - servers expose a dedicated <code>/health</code> endpoint and the load balancer actively polls it.    * This can work by just returning OK or doing a more sophisticated check of the server's resources (eg DB connection, CPU load, etc).     * Important note - a bug in the <code>/health</code> endpoint can bring the whole application down. Some smart load balancers can detect this anomaly &amp; disregard the health check.</p> <p>Health checks can be used to implement rolling updates - updating an application to a new version without downtime:  * During the update, a number of servers report as unavailable.   * In-flight requests are completed (drained) before servers are restarted with the new version.</p> <p>This can also be used to restart eg a degraded server due to (for example) a memory leak:  * A background thread (watchdog) monitors a metric (eg memory) and if it goes beyond a threshold, server forcefully crashes.  * The watchdog implementation is critical - it should be well tested because a bug can degrade the entire application.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#dns-load-balancing","title":"DNS Load balancing","text":"<p>A simple way to implement a load balancer is via DNS.</p> <p>We add the servers' public IPs as DNS records and clients can pick one: </p> <p>The main problem is the lack of fault tolerance - if one of the servers goes down, The DNS server will continue routing requests to it.  Even if an operator manages to reconfigure the DNS records, it takes time for the changes to get propagated due to DNS caching.</p> <p>The one use-case where DNS load balancing is used is to route traffic across multiple data centers.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#network-load-balancing_1","title":"Network load balancing","text":"<p>A more flexible approach is implementing a load balancer, operating at the TCP layer of the network stack.  * Network load balancers have one or more physical network cards mapped to virtual IPs (VIPs). A VIP is associated with a pool of servers.  * Clients only see the VIP, exposed by the load balancer.  * The load balancer routes client requests to a server from the pool and it detects faulty servers using passive health checks.  * The load balancer maintains a separate TCP connection to the downstream server. This is called TCP termination.  * Direct server return is an optimization where responses bypass the L4 load balancer &amp; go to the client directly.  * Network load balancers are announced to a data center's router, which can load balance requests to the network load balancers. </p> <p>Managed solutions for network load balancing:  * AWS Network Load Balancer  * Azure Load Balancer</p> <p>Load balancing at the TCP layer is very fast, but it doesn't support features involving higher-level protocols such as TLS termination.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#application-layer-load-balancing","title":"Application layer load balancing","text":"<p>Application layer load balancers (aka L7 load balancers) are HTTP reverse proxies which distribute requests over a pool of servers.</p> <p>There are two TCP connections at play - between client and load balancer and between load balancer and origin server.</p> <p>Features L7 load balancers support:  * Multiplexing multiple HTTP connections over the same TCP connection  * Rate limiting HTTP requests  * Terminate TLS connections  * Sticky sessions - routing requests belonging to the same session onto the same server (eg by reading a cookie)</p> <p>Drawbacks:  * Much lower throughput than L4 load balancers.  * If a load balancer goes down, the application behind it goes down as well.</p> <p>One way to off-load requests from an application load balancer is to use the sidecar proxy pattern. This can be applied to clients internal to the organization. Applications have a L7 load balancer instance running on the same machine. All application requests are routed through it.</p> <p>This is also referred to as a service mesh.</p> <p>Popular sidecar proxy load balancers - NGINX, HAProxy, Envoy.</p> <p>Main advantage - delegates load balancing to the client, avoiding a single point of failure.</p> <p>Main disadvantage - there needs to be a control plane which manages all the side cars.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#data-storage","title":"Data storage","text":"<p>The next scaling target is the database. Currently, it's hosted on a single server.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#replication","title":"Replication","text":"<p>We can increase the read capacity of the database by using leader-follower replication: </p> <p>Clients send writes to the leader. The leader persists those in its write-ahead log.  Replicas connect to the leader &amp; stream log entries from it to update their state. They can disconnect and reconnect at any time as they maintain the log sequence number they've reached, from which they continue the stream.</p> <p>This helps with:  * Increasing read capacity  * Increasing database availability  * Expensive analytics queries can be isolated to a follower in order to not impact the rest of the clients</p> <p>Replication can be configured as:  * fully synchronous - leader broadcasts writes to followers &amp; immediately returns a response to the client    * Minimum latency, but not fault tolerant. Leader can crash after acknowledgment, but before broadcast.   * fully asynchrnous - leader waits for acknowledgment from followers before acknowledging to the client   * Highly consistent but not performant. A single slow replica slows down all the writes.   * hybrid of the two - some of the followers receive writes synchronously, others receive it asynchronously   * This is the default behavior for PostgreSQL   * If a leader crashes, we can fail over to the synchrnous follower without incurring any data loss.</p> <p>Replication increases read capacity, but it still requires the database to fit on a single machine.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#partitioning_1","title":"Partitioning","text":"<p>Enables us to scale a database for both reads and writes.</p> <p>Traditional relational databases don't support partitioning out of the box, so we can implement in in the application layer.</p> <p>In practice, this is quite challenging:  * We need to decide how to split data to be evenly distributed among partitions.  * We need to handle rebalancing when a partition becomes too hot or too big.  * To support atomic transactions, we need to add support for two-phase commit (2PC).  * Queries spanning multiple partitions need to be split into sub-queries (ie aggregations, joins).</p> <p>Fundamental problem of relational databases is that they were designed under the assumption that they can fit on a single machine. Due to that, hard-to-scale features such as ACID and joins were supported.</p> <p>In addition to that, relational databases were designed when disk space was costly, so normalizing the data was encouraged to reduce disk footprint. This came at a significant cost later to unnormalize the data via joins.</p> <p>Times have changed - storage is cheap, but CPU time isn't.</p> <p>Since the 2000s, large tech companies have started to invest in data storage solutions, designed with high availability and scalability in mind.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#nosql","title":"NoSQL","text":"<p>Some of the early designs were inefficient compared to traditional SQL databases, but the Dynamo and Bigtable papers were foundational for the later growth of these technologies.</p> <p>Modern NoSQL database systems such as HBase and Cassandra are based on them.</p> <p>Initially, NoSQL databases didn't support SQL, but nowadays, they support SQL-like syntax.</p> <p>Traditional SQL systems support strong consistency models, while NoSQL databases relax consistency guarantees (ie eventual/causal consistency) in favor of high availability. NoSQL systems usually don't support joins and rely on the data being stored unnormalized.</p> <p>Main NoSQL flavors store data as key-value pairs or as document store. The main difference is that the document store enables indexing on internal structure. A strict schema is not applied in both cases.</p> <p>Since NoSQL databases are natively created with partitioning in mind, there is limited (if any) support for transactions. However, due to data being stored in unnormalized form, there is less need for transactions or joins.</p> <p>A very bad practice is try to store a relational model in a NoSQL database. This will result in the worst of both worlds. If used correctly, NoSQL can handle most of the use-cases a relational database can handle, but with scalability from day one.</p> <p>The main prerequisite for using NoSQL database is to know the access patterns beforehand and model the database with those in mind since NoSQL databases are hard to alter later.</p> <p>For example, let's take a look at DynamoDB:  * A table consists of items. Each table has a partition key and optionally a sort key (aka clustering key).  * The partition key dictates how data is partitioned and distributed among nodes.   * The clustering key dictates how the data is sorted within partitions - this enables efficient range queries.  * DynamoDB maintains three replicas per partition which are synchronized using state machine replication.  * Writes are routed to the leader. Acknowledgment is sent once 2 out of 3 replicas have received the update.  * Reads can be eventually consistent (if you query any replica) or strongly consistent (if you query the leader).</p> <p>DynamoDB's API supports:  * CRUD on single items.  * Querying multiple items with the same partition key and filtering based on the clustering key.  * Scanning the entire table.</p> <p>Joins aren't supported by design. We should model our data to not need joins in the first place.</p> <p>Example - modeling a table of orders:  * We should model the table with access patterns in mind. Let's say the most common operation is grabbing orders of a customer, sorted by order creation time.  * Hence, we can use the customer ID as the partition key and the order creation time as the clustering key. </p> <p>SQL databases require that you store a given entity type within a single table. NoSQL enables you to store multiple entity types within the same table.</p> <p>Eg if we want to also include a customer's full name in an order, we can add a new entry against the same partition key within the same table: </p> <p>A single query can grab both entities within this table, associated to a given customer, because they're stored against the same partition key. </p> <p>More complex access patterns can be modeled via secondary indexes, which is supported by DynamoDB:  * Local secondary indexes allow for more sort keys within the same table.  * Global secondary indexes enable different partition and sort keys, with the caveat that index updates are asynchronous and eventually consistent.</p> <p>Main Drawback of NoSQL databases - much more thought must be put upfront to design the database with the access patterns in mind.  Relational databases are much more flexible, enabling you to change your access patterns at runtime.</p> <p>To learn more about NoSQL databases, the author recommends The DynamoDB Book, even if you plan to use a different NoSQL database.</p> <p>The latest trend is to combine the scalability of NoSQL databases with the ACID guarantees of relational databases. This trend is referred to as NewSQL.</p> <p>NoSQL databases favor availability over consistency in the event of network partitions. NewSQL prefer consistency. The argument is that with the right design, the reduction of availability due to preferring strong consistency is barely noticeable.</p> <p>CochroachDB and Spanner are well-known NewSQL databases.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#caching_1","title":"Caching","text":"<p>Whenever a significant portion of requests is for a few frequently accessed objects, then that workflow is suitable for caching.</p> <p>Caches improves the app's performance and reduces the load on the data store:  * It's a high-speed storage layer that buffers responses from the origin data store.  * It provides best-effort guarantees since it isn't the source of truth. Its state can always be rebuilt from the origin.</p> <p>In order for a cache to be cost-effective, the proportion of requests which hit the cache vs. hitting the origin should be high (hit ratio).</p> <p>The hit ratio depends on:  * The total number of cacheable objects - the fewer, the better  * The probability of accessing the same object more than once - the higher, the better  * The size of the cache - the larger, the better</p> <p>The higher in the call stack a cache is, the more objects it can capture. </p> <p>Be wary, though, that caching is an optimization, which doesn't make a scalable architecture.  Your original data store should be able to withstand all the requests without a cache in front of it. It's acceptable for the requests to become slower, but it's not acceptable for your entire data store to crash.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#policies","title":"Policies","text":"<p>Whenever there's a cache miss, the missing object has to be requested from the origin.</p> <p>There are two ways (policies) to handle this:  * Side Cache - The application requests the object from the origin and stores it in the cache. The cache is treated as a key-value store.  * Inline Cache - The cache communicates with the origin directly requesting the missing object on behalf of the application. The app only accesses the cache.</p> <p>Side cache (write-through-aside) example: </p> <p>Inline cache (aka Write-through) example: </p> <p>Something which wasn't mentioned in the book is a write-back cache. It acts as a write-through cache, but asynchronously updates the data store. This one is way more complex to implement: </p> <p>Whenever the cache has limited capacity, entries must be evicted from it.  The eviction policy defines that. The most common eviction policy is LRU - least-recently used elements are evicted first.</p> <p>The expiration policy defines how long are objects stored in the cache before they're refreshed from the origin (TTL).</p> <p>The higher the TTL, the higher the hit ratio, but also the higher the chance of serving stale objects. Eviction need not occur immediately. It can be deferred to the next time an object is accessed. This might be preferable so that if the origin data store is down, your application continues returning data albeit stale.</p> <p>Cache invalidation - automatically expiring objects when they change, is hard to implement in practice. That's why TTL is used as a workaround.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#local-cache","title":"Local cache","text":"<p>Simplest way to implement a cache is to use a library (eg Guava in Java or RocksDB), which implements an in-memory cache. This way, the cache is embedded within the application. </p> <p>Different replicas have different caches, hence, objects are duplicated &amp; memory is wasted. These kinds of caches also cannot be partitioned or replicated - if every client has 1GB of cache, then the total capacity of the cache is 1GB.</p> <p>Consistency issues will also arise - separate clients can see different versions of an object.</p> <p>More application replicas mean more caches, which result in more downstream traffic towards the origin data store. This issue is particularly prevalent when the application restarts, caches are empty and the origin is hit with a lot of concurrent requests. Same thing can happen if an object instantly becomes popular and becomes requested a lot.</p> <p>This is referred to as \"thundering herd\". You can reduce its impact by client-side rate limiting.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#external-cache","title":"External cache","text":"<p>External service, dedicated to caching objects, usually in-memory for performance purposes.</p> <p>Because it's shared, it resolves some of the issues with local caches. at the expense of greater complexity and cost.</p> <p>Popular external caches - Redis and Memcached.</p> <p>External caches can increase their throughput and size via replication and partitioning - Redis does this, for example. Data can automatically partition data &amp; also replicate partitions using leader-follower election.</p> <p>Since cache is shared among clients, they consistently see the same version of an object - be it stale or not. Replication, however, can lead to consistency issues.</p> <p>The number of requests towards the origin doesn't grow with the growth in application instances. </p> <p>External caches move the load from the origin data store towards itself. Hence, it will need to be scaled out eventually.</p> <p>When that happens, as little data should be moved around as possible to avoid having the hit ratio drop significantly.  Consistent hashing can help here.</p> <p>Other drawbacks of external caches:  * Maintenance cost - in contrast to local caches, external ones are separate services which need to be setup &amp; maintained.  * Higher latency than local caches due to the additional network calls.</p> <p>If the external cache is down, how can clients mitigate that? One option is to bypass it &amp; access the origin directly, but that can lead to cascading failures due to the origin not being able to handle the load.</p> <p>Optionally, applications can maintain an in-process cache as a backup in case the external cache crashes.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#microservices","title":"Microservices","text":"<p>A monolithic application consists of a single code base \\w multiple independent components in it: </p> <p>Downsides of a monolithic system:  * Components will become increasingly coupled over time &amp; devs will step on each others toes quite frequently.  * The code base at some point will become too large for anyone to fully understand it - adding new features or fixing bugs becomes more time-consuming than it used to.  * A change in a component leads to the entire application being rebuilt &amp; redeployed.  * A bug in one component can impact multiple unrelated components - eg memory leak.  * Reverting a deployment affects the velocity of all developers, not just the one who introduced a bug.</p> <p>To mitigate this, one could functionally decompose the large code base into a set of independently deployable services, communicating via APIs: </p> <p>The decomposition creates a boundary between components which is hard to violate, unlike boundaries within a code base.</p> <p>Outcome of using microservices:  * Each service is owned and operated by a small team - less communication is necessary across teams.  * Smaller teams communicate more effectively than larger ones, since communication overhead grows quadratically as team size increases.  * The surface area of an application is smaller, making it more digestible to engineers.  * Teams are free to adopt the tech stack &amp; hardware they prefer.</p> <p>Good practices:  * An API should have a small surface area, but encapsulate a significant amount of functionality.  * Services shouldn't be too \"micro\" as that adds additional operational load &amp; complexity.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#caveats","title":"Caveats","text":"<p>Splitting your application into multiple services has benefits but it also has a lot of downsides. It is only worth paying the price if you'll be able to amortize it across many development teams.</p> <ol> <li>Tech Stack</li> </ol> <p>Nothing forbids teams from using a different tech stack than everyone else. Doing so, makes it difficult for developers to move across teams.</p> <p>In addition to that, you'll have to support libraries in multiple languages for your internal tools.</p> <p>It's reasonable to enforce a certain degree of standardization. One way of doing this is to provide a great developer experience for some technologies, but not others.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#communication","title":"Communication","text":"<p>Remote calls are expensive &amp; non-deterministic. Making them within the same process removes a lot of the complexity of distributed systems.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#coupling","title":"Coupling","text":"<p>Microservices ought to be loosely coupled. Changes in one service shouldn't propagate to multiple other services.  Otherwise, you end up with a distributed monolith, which has the downsides of both approaches.</p> <p>Examples of tight coupling:  * Fragile APIs require clients to be updated on every change.  * Shared libraries which have to be updated in lockstep across multiple services.  * Using static IP addresses to reference external services.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#resource-provisioning","title":"Resource provisioning","text":"<p>It should be easy to provision new services with dedicated hardware, data stores, etc.</p> <p>You shouldn't make every team do things in their own (slightly different) way.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#testing","title":"Testing","text":"<p>Testing microservices is harder than testing monoliths because subtle issues can arise from cross-service integrations.</p> <p>To have high confidence, you'll have to develop sophisticated integration testing mechanisms.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#operations","title":"Operations","text":"<p>Ease of deployment is critical so that teams don't deploy their services differently.</p> <p>In addition to that, debugging microservices is quite challenging locally so you'll need a sophisticated observability platform.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#eventual-consistency","title":"Eventual Consistency","text":"<p>The data model no longer resides in a single data store. </p> <p>Hence, atomic updates across a distributed system is more challenging. Additionally, guaranteeing strong consistency has a high cost which is often not worth paying so we fallback to eventual consistency.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#conclusion","title":"Conclusion","text":"<p>It is usually best to start with a monolith &amp; decompose it once there's a good enough reason to do so.</p> <p>Once you start experiencing growing pains, you can start to decompose the monolith, one microservice at a time.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#api-gateway","title":"API Gateway","text":"<p>When using microservices, letting clients make requests to services can be costly.</p> <p>For example, mobile devices might have a hard time performing an expensive operation which involves interacting with multiple APIs as every API call consumes battery life.</p> <p>In addition to that, clients need to be aware of implementation details such as a service's name. This makes it hard to modify the application architecture as it requires changing the clients as well.</p> <p>Once you have a public API out, you have to be prepared to maintain it for a very long time.</p> <p>As a solution, we can hide the internal APIs behind a facade called the API gateway. </p> <p>Here are some of the core responsibilities.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#routing","title":"Routing","text":"<p>You can map public endpoints to internal ones. </p> <p>If there is a 1:1 mapping, the internal endpoint can change, but the public one can stay the same.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#composition","title":"Composition","text":"<p>We might encounter use-cases where we have to stitch data together from multiple sources.</p> <p>The API gateway can offer a higher-level API that composes a response from the data of multiple services.</p> <p>This relieves the client from knowing internal service details &amp; reduces the number of calls it has to perform to get the data it needs.</p> <p>Be wary that the availability of an API decreases as the number of composed internal services increases.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#translation","title":"Translation","text":"<p>The API Gateway can transform from one IPC mechanism to another - eg gRPC to REST. Additionally, it can expose different APIs to different clients. Eg a desktop-facing API can include more data than a mobile-facing one.</p> <p>Graph-based APIs are a popular mechanism to allow the client to decide for itself how much data to fetch - GraphQL is a popular implementation.</p> <p>This reduces development time as there is no need to introduce different APIs for different use-cases.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#cross-cutting-concerns","title":"Cross-cutting concerns","text":"<p>Cross-cutting functionality can be off-loaded from specific services - eg caching static resources or rate-limiting </p> <p>The most critical cross-cutting concerns are authentication and authorization (authN/authZ).</p> <p>A common way to implement those is via sessions - objects passed through by client in subsequent requests. These store a cryptographically secure session object, which the server can read and eg extract a user id.</p> <p>Authentication is best left to the API gateway to enable multiple authentication mechanisms into a service, without it being aware. Authorization is best left to individual services to avoid coupling the API gateway with domain logic.</p> <p>The API gateway passes through a security token into internal services. They can obtain the user identity and roles via it.</p> <p>Tokens can be:  * Opaque - services call an external service to get the information they need about a user.  * Transparent - the token contains the user information within it.</p> <p>Opaque tokens require an external call, while transparent ones save you from it, but it is harder to revoke them if they're compromised. If we want to revoke transparent tokens, we'll need to store them in a \"revoked_tokens\" store or similar.</p> <p>The most popular transparent token standard is JWT - json payload with expiration date, user identity, roles, metadata. The payload is signed via a certificate, trusted by internal services.</p> <p>Another popular auth mechanism is using API keys - popular for implementing third-party APIs.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#caveats_1","title":"Caveats","text":"<p>The API Gateway has some caveats:   * It can become a development bottleneck since it's tightly coupled with the internal APIs its proxying to.  * Whenever an internal API changes, the API gateway needs to be changed as well.  * It needs to scale to the request rate for all the services behind it.</p> <p>If an application has many services and APIs, an API gateway is usually a worthwhile investment.</p> <p>How to implement it:  * Build it in-house, using a reverse proxy as a starting point (eg NGINX)  * Use a managed solution, eg Azure API Management, Amazon API Gateway</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#control-planes-and-data-planes","title":"Control planes and data planes","text":"<p>The API Gateway is a single point of failure. If it goes down, so does our application. Hence, this component should be scalable &amp; highly available.</p> <p>There are some challenges related to external dependencies - eg gateway has a \"configuration\" endpoint which updates your rate-limits for endpoints. This endpoint has a lot less load than normal endpoints. In those cases, we should favor availability vs. consistency. In this case, we should favor consistency.</p> <p>Due to these differing requirements, we're splitting the API gateway into a \"data plane\" which handles external requests and a \"control plane\" which manages configuration &amp; metadata. This split is a common pattern.</p> <p>The data plane includes functionality on the critical path that needs to run for each client request - it needs to be highly available, fast &amp; scale with increase in load. A control plane is not on the critical path &amp; has less need to scale. Its job is to manage the configuration for the data plane.</p> <p>There can be multiple control planes - one which scales service instances based on load, one which manages rate limiting configuration.</p> <p>However, this separation introduces complexity - the data plane needs to be designed to withstand control plane failures. If the data plane crashes as control plane crashes, there is a hard dependency on the control plane.</p> <p>When there's a chain of components that depend on each other, the theoretical availability is their independent availability's product (ie 50% * 50% = 25%). A system can be as available as its least available hard dependency.</p> <p>The data plane, ideally, should continue running with the last seen configuration vs. crashing in the event of control plane failures.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#scale-imbalance","title":"Scale imbalance","text":"<p>Since data planes &amp; control planes have different scaling requirements, it is possible for the data plane to overload the control plane.</p> <p>Example - there's a control plane endpoint which the data plane periodically polls.  If the data plane restarts, there can be a sudden burst in traffic to that endpoint as all of the refreshes align.</p> <p>If part of the data plane restarts but can't reach the control plane, it won't be able to function.</p> <p>To mitigate this, one could use a scalable file store as a buffer between the control &amp; data planes - the control plane periodically dumps its configuration into it.</p> <p>This is actually quite reliable &amp; robust in practice, although it sounds naive. </p> <p>This approach shields the control plane from read load &amp; allows the data plane to continue functioning even if the control plane is down. This comes at the cost of higher latency and weaker consistency since propagating changes from control plane to data plane increases.</p> <p>To decrease propagation latency, we can fallback to our original approach but this time, the control plane propagates configuration changes to the data plane.</p> <p>This way, the control plane can control its pace. </p> <p>An optimization one can use is for the control plane to only push the changes from one configuration version to another to avoid sending too much data in cases where the configuration is too big.</p> <p>The downside is that the data plane will still need to read the initial configuration on startup. To mitigate this, the intermediary data store can still be used for that purpose: </p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#control-theory","title":"Control theory","text":"<p>Control theory is an alternative way to think about control &amp; data planes.</p> <p>The idea is to have a controller which monitors a system &amp; applies a corrective action if there's anything odd - eg desired state differs from actual one.</p> <p>Whenever you have monitor, compare and action, you have a closed loop - a feedback system. The most commonly missing part in similar systems is the monitoring.</p> <p>Example - chain replication. The control plane doesn't just push configuration to the nodes. It monitors their state &amp; executes corrective actions such as removing a node from the chain.</p> <p>Example - CI/CD pipeline which deploys a new version of a service. A naive implementation is to apply the changes &amp; hope they work. A more sophisticated system would perform gradual roll out, which rolls back the update in the event the new version leads to an unhealthy system.</p> <p>In sum, When dealing with a control plane, one should ask themselves - what's missing to close the loop?</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#messaging","title":"Messaging","text":"<p>Example use-case:  * API Gateway endpoint for uploading a video &amp; submitting it for processing by an encoding service  * Naive approach - upload to S3 &amp; call encoding service directly.     * What if service is down? What to do with ongoing request?    * If we fire-ang-forget, but encoding service fails during processing, how do we recover from that failure?  * Better approach - upload to S3 &amp; notify encoding service via messaging. That guarantees it will eventually receive the request.</p> <p>How does it work?  * A message channel acts as temporary buffer between sender &amp; receiver.  * Sending a message is asynchronous. It doesn't require the receiver to be online at creation time.  * A message has a well-defined format, consisting of a header and payload (eg JSON).  * The message can be a command, meant for processing by a worker or an event, notifying services of an interesting event.</p> <p>Services can use inbound and outbound adapters to send/receive messages from a channel: </p> <p>The API Gateway submits a message to the channel &amp; responds to client with <code>202 Accepted</code> and a link to uploaded file. If encoding service fails during processing, the request will be re-sent to another instance of the encoding service.</p> <p>There are a lot of benefits from decoupling the producer (API Gateway) from the consumer (encoding service):  * Producer can send messages even if consumer is temporarily unavailable  * Requests can be load-balanced across a pool of consumer instances, contributing to easy scaling.  * Consumer can read from message channel at its own pace, smoothing out load spikes.  * Messages can be batched - eg a client can submit a single read request for last N messages, optimizing throughput.</p> <p>Message channels can be several kinds:  * Point-to-point - message is delivered to exactly one consumer.  * Publish-subscribe - message is delivered to multiple consumers simultaneously.</p> <p>Downside of message channel is the introduced complexity - additional service to process and an untypical flow of control.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#one-way-messaging","title":"One-way messaging","text":"<p>Also referred to as point-to-point, in this style a message is received &amp; processed by one consumer only. Useful for implementing job processing workflows.</p> <p></p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#request-response-messaging","title":"Request-response messaging","text":"<p>Similar to direct request-response style but messages flow through channels. Useful for implementing traditional request-response communication by piggybacking on message channel's reception guarantee.</p> <p>In this case, every request has a corresponding response channel and a <code>response_id</code>. Producers use this <code>response_id</code> to associate it to the origin request.</p> <p></p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#broadcast-messaging","title":"Broadcast messaging","text":"<p>The producer writes a message to a publish-subscribe channel to broadcast it to all consumers. Used for a process to notify other services of an interesting event without being aware of them or dealing with them receiving the notification.</p> <p></p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#guarantees","title":"Guarantees","text":"<p>Message channels are implemented by a messaging service/broker such as Amazon SQS or Kafka.</p> <p>Different message brokers offer different guarantees.</p> <p>An example trade-off message brokers make is choosing to respect the insertion order of messages. Making this guarantee trumps horizontal scaling which is crucial for this service as it needs to handle vast amount of load.</p> <p>When multiple nodes are involved, guaranteeing order involves some form of coordination.</p> <p>For example, Kafka partitions messages into multiple nodes. To guarantee message ordering, only a single consumer process is allowed to read from a partition.</p> <p>There are other trade-offs a message broker has to choose from:  * delivery guarantees - at-most-once or at-least-once  * message durability  * latency  * messaging standards, eg AMQP  * support for competing consumer instances  * broker limits, eg max size of messages</p> <p>For the sake of simplicity, the rest of the sections assume the following guarantees:  * Channels are point-to-point &amp; support many producer/consumer instances  * Messages are delivered at-least-once  * While a consumer is processing a message, other consumers can't process it. Once a <code>visibility</code> timeout occurs, it will be distributed to a different consumer.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#exactly-once-processing","title":"Exactly-once processing","text":"<p>A consumer has to delete a message from the channel once it's done processing it.</p> <p>Regardless of the approach it takes, there's always a possibility of failure:  * If message is deleted before processing, processing could fail &amp; message is lost  * If message is deleted after processing, deletion can fail, leading to duplicate processing of the same message</p> <p>There is no such thing as exactly-once processing. To workaround this, consumers can require messages to be idempotent and deleting them after processing is complete.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#failures","title":"Failures","text":"<p>When a consumer fails to process a message, the visibility timeout occurs &amp; the message is distributed to another consumer.</p> <p>What if message processing consistently fails? To limit the blast radius of poisonous messages, we can add a max retry count &amp; move the message to a \"dead-letter queue\" channel.</p> <p>Messages that consistently fail will not be lost &amp; they can later be addressed manually by an operator.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#backlogs","title":"Backlogs","text":"<p>Message channels make a system more robust to outages, because a producer can continue writing to a channel while a consumer is down. This is fine as long as the arrival rate is less than or equal to the deletion rate.</p> <p>However, when a consumer can't keep up with a producer, a backlog builds up. The bigger the backlog, the longer it takes to drain it.</p> <p>Why do backlogs occur?  * Producer throughput increases due to eg more instances coming online.  * The consumer's performance has degraded  * Some messages constantly fail &amp; clog the consumer who spends processing time exclusively on them.</p> <p>To monitor backlogs, consumers can compare the arrival time of a message with its creation time.</p> <p>There can be some differences due to physical clocks involved, but the accuracy is usually sufficient.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#fault-isolation","title":"Fault isolation","text":"<p>If there's some producer which constantly emits poisonous messages, a consumer can choose to deprioritize those into a lower-priority queue - once read, consumer removes them from the queue &amp; submits them into a separate lower-priority queue.</p> <p>The consumer still reads from the slow channel but less frequently in order to avoid those messages from clogging the entire system.</p> <p>The slow messages can be detected based on some identifier such as a <code>user_id</code> of a troublesome user.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part03/#summary","title":"Summary","text":"<p>Building scalable applications boils down to:  * breaking down applications to separate services with their own responsibilities. (functional decomposition)  * Splitting data into partitions &amp; distributing them across different nodes (partitioning)  * Replicating functionality or data across nodes (replication)</p> <p>One subtle message which was conveyed so far - there is a pletora of managed services, which enable you to build a lot of applications. Their main appeal is that others need to guarantee their availability/throughput/latency, rather than your team.</p> <p>Typical cloud services to be aware of:  * Way to run instances in the cloud (eg. AWS EC2)  * Load-balancing traffic to them (eg. AWS ELB)  * Distributed file store (eg AWS S3)  * Key-value document store (eg DynamoDB)  * Messaging service (eg Kafka, Amazon SQS)</p> <p>These services enable you to build most kinds of applications you'll need.  Once you have a stable core, you can optimize by using caching via managed Redis/Memcached/CDN.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/","title":"Resiliency","text":"<ul> <li>Common failure causes</li> <li>Redundancy</li> <li>Fault isolation</li> <li>Downstream resiliency</li> <li>Upstream resiliency</li> <li>Summary</li> </ul> <p>The three fundamental scalability patterns discussed so far:  * functional decomposition - splitting a monolithic system into independent parts which composed provide the same functionality  * data partitioning - splitting data across several nodes  * replication - duplicating data across several nodes</p> <p>All the patterns have one thing in common - they increase the number of moving parts in our system. The more moving parts there are in a system, the higher the probability of them failing.</p> <p>If we want to guarantee two nines for a system (99%), then we can only afford the application to be down 15m per day. If we strive for three nines (99.9%), we can afford 43m per month.</p> <p>The more nines we want, the faster our systems need to detect &amp; react to outages.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#common-failure-causes","title":"Common failure causes","text":"<p>A system has a failure when it no longer provides service to its users based on the system's specifications.</p> <p>Failures are caused by a fault - failure of internal component or external dependency. Not all faults can lead to a failure. Some can be tolerated.</p> <p>To build a fault-tolerant system, we need to understand what can go wrong.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#hardware-faults","title":"Hardware faults","text":"<p>Any physical part of a machine can fail - HDD, SSD, NIC, CPU, etc.</p> <p>Hardware faults can also lead to data corruption. Entire data centers can go down due to a power cut or natural disaster.</p> <p>Many of the infrastructure faults can be addressed with redundancy.</p> <p>Most distributed applications don't fail due to these kinds of faults. In reality, common faults happen for more mundane reasons.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#incorrect-error-handling","title":"Incorrect error handling","text":"<p>A 2014 study found that the majority of catastrophic failures in five of the most popular distributed data stores were due to incorrect error handling of non-fatal errors.</p> <p>In most cases, the bugs could have been avoided with more thorough testing:  * Completely ignoring errors.  * Catching an overly generic exception, eg <code>Exception</code> in Java.  * Partially implemented handlers with <code>FIXME</code> and <code>TODO</code> comments.</p> <p>This is not a surprise as error handling tends to be an afterthought.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#configuration-changes","title":"Configuration changes","text":"<p>Configuration changes are among the leading causes for major failures. </p> <p>Sometimes, it is a misconfiguration. At other times, it is a valid configuration which is rarely used, hence, not working as expected anymore.</p> <p>Configuration changes can be even more problematic if their effect is delayed - eg application fetches the new configuration several hours later, making it very hard to trace the exact root cause for an outage.</p> <p>Due to this, configuration changes should be version-controlled, tested &amp; released like any code change.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#single-points-of-failure","title":"Single points of failure","text":"<p>A SPOF is a component, whose failure brings down the entire system down with it.A</p> <p>Common SPOFs:  * Humans which need to accurately execute a sequence of steps. You better automate those.  * DNS - domain name expiring, root level domains going down, expired TLS certificate.</p> <p>SPOFs should be identified when the system is designed. If you can, you should architect away a SPOF using eg redundancy. If you can't, you'll have to at least try and reduce the blast radius in the event the SPOF fails.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#network-faults","title":"Network faults","text":"<p>When a client sends a request to a server and doesn't get a timely response, it can either timeout or block &amp; wait for a response.</p> <p>What can trigger network faults? - server is slow, server has crashed, the network is losing parts of the packers, causing retransmission and delay. Slow network calls are the silent killers of distributed systems.</p> <p>Clients don't know if a response will eventually arrive, hence, it can wait for a long time before timing out, leading to performance degradations which are hard to detect &amp; debug.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#resource-leaks","title":"Resource leaks","text":"<p>Resource leaks very commonly lead to slow processes.</p> <p>Memory leaks are the most common resource leaks. It causes a steady increase in memory consumption over time. Even garbage-collected languages are vulnerable to memory leaks. </p> <p>When a memory leak occurs, the OS starts to use swap memory, which is very slow and the garbage collector kicks in more aggressively, consuming CPU cycles.</p> <p>There are also other resources which can leak:  * Thread pools - a thread is blocked waiting on a synchronous HTTP call &amp; can't be allocated for another job.  * Socket pools - making http connections without a timeout consume a socket from a pool of available ones and you will eventually run out of sockets in the pool.  * A third-party library you depend on might have a resource leak &amp; you inherit its problem.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#load-pressure","title":"Load pressure","text":"<p>Every system has capacity - how much load it can withstand.</p> <p>Organic increase in load allows the time for a system to scale out &amp; increase its capacity. A sudden increase in load, however, can lead to failure.</p> <p>Why can that happen?  * Requests have seasonality - they tend to increase during some time of the day.  * Some requests are more expensive than others like scrapers, which consume data very fast.  * Some requests are malicious, such as those incoming from DDoS attacks.</p> <p>Some load surges can be handled by automatically adding additional capacity (autoscaling). Others can be rejected via rate-limiting.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#cascading-failures","title":"Cascading failures","text":"<p>If a system has hundreds of processes and a small subset of them are unreachable, this can still lead to a global outage.</p> <p>This is because failures can cascade across services &amp; bring the entire system down - this occurs when components depend on each other and a failure in one, leads to a failure in dependent ones.</p> <p>Example:  * clients are querying two database replicas via a load balancer. Each replica handles 50 tps.   * Replica B becomes unavailabe due to a network fault.  * Load balancer removes B from the pool and redirects all the traffic to A.  * If replica A can't handle the double increase in tps, it will eventually crash, bringing the whole system down.  * In essence, the failure in B, cascaded to the other replica A.  * If replica B comes back up again after a while, all requests will be redirected to it and it will also encounter a capacity failure. </p> <p>A failure like this one are very hard to handle and usually require a corrective action such as stopping all traffic temporarily.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#managing-risk","title":"Managing risk","text":"<p>Distributed applications need to accept that faults are inevitable and must be prepared to detect, react &amp; repair.</p> <p>There is a tremendous amount of faults that can occur. But we don't need to do something about all of them.</p> <p>We need to measure probability &amp; impact of occurring. Afterwards, prioritize the more likely &amp; impactful faults. </p> <p>Once we decide to tackle a specific fault, we can focus on reducing its impact or probability.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#redundancy","title":"Redundancy","text":"<p>Redundancy == replication of functionality or state. This is the first line of defense against failures.</p> <p>When functionality (or state) is replicated across multiple nodes, others can take over in the event of failure.</p> <p>This is the main reason distributed applications can achieve better availability than single-node apps.</p> <p>However, in order for redundancy to be effective, there are some prerequisites:  * Complexity introduced by redundancy mustn't cost more availability than it adds.  * The system must reliably detect which components are healthy and which aren't.  * System must be able to run in degraded mode.  * System must be able to recover to fully redundant mode.</p> <p>Example - adding redundancy to protect against hardware faults:  * A load balancer can mitigate this type of fault using a pool of redundant nodes.  * The load balancer increases the system's complexity, but the benefits in terms of scalability and availability outweigh the costs.  * Load balancer detects faulty nodes using health checks.  * When a server is taken out of the pool, the rest of the replicas must have enough capacity to handle the load increase.  * The load balancer also enables the system to go back to fully redundant mode as new servers are added to the pool, requests get routed to them.</p> <p>Replication for stateful services is a lot more challenging and was discussed at length at previous chapters.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#correlation","title":"Correlation","text":"<p>Redundancy helps only if the nodes can't fail all at once for the same reason - ie, failures are not correlated.</p> <p>Example:  * Memory corruption on one server is unlikely to occur on another at the same time.   * However, a data center outage (eg natural disaster) can cause all servers to go down unless they're replicated across multiple data centers.</p> <p>Cloud providers (eg AWS, Azure) replicate their entire stack in multiple regions for that reason:  * Each region comprises of multiple data centers, called availability zones (AZ). Those are cross-connected with high-speed network links.  * AZs are far enough from each other to minimize the risk of correlated failures.  * However, they are still close enough to have low network latency among themselves.  * The low latency enables supporting synchronous replication protocols across multiple AZs.  * Using AZs, we can create apps resilient to data center outages by load balancing instances across multiple AZs, behind a shared load balancer.</p> <p>But, if we want to have even more redundancy in the event a catastrophic disaster brings down all AZs, we can duplicate the entire application stack in multiple regions:  * Load balancing can be achieved by using global DNS load balancing.   * Using this approach, though, requires application state to be replicated asynchronously due to the high network latency between the DCs.  * Before doing this, though, you should have pretty good reason to, because it is expensive to pull off &amp; maintain.  * A common reason for doing this is legal compliance since European customer data must be processed and stored within Europe.  </p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#fault-isolation","title":"Fault isolation","text":"<p>Infrastructure faults can be controlled using redundancy. However, there are faults where redundancy won't help due to the high degree of correlation.</p> <p>Example:  * Specific user sends malformed requests that causes servers handling them to crash due to a bug.  * Since bug is in the code, it doesn't matter how redundant your application is.  * These kinds of requests are often referred to as poison pills.  * A similar case is when a particular request requires a lot of resources to serve it and that degrades everyone else's performance (the noisy neighbor problem).</p> <p>The main issue with this example is that the blast radius of such a fault is the entire application.  Hence, we can focus on reducing it by partitioning user requests, therefore reducing the blast radius to a particular partition where the user is assigned.</p> <p>Even if a user is degrading a partition, the rest of the partitions will be isolated from that fault.</p> <p>For example, if we partition a set of 6 instances into three partitions, a noisy neighbor can only impact 33% of users. As the number of partitions increases, the blast radius is reduced. </p> <p>This is also referred to as the \"bulkhead pattern\", named after the pattern of splitting a ship's hull into compartments. If one compartment is damaged and filled with water, the rest of the compartments are left intact.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#shuffle-sharding","title":"Shuffle sharding","text":"<p>The problem with partitioning is that unlucky users who end up on the degraded partition are consistently impacted as well.</p> <p>A way to mitigate this is by using shuffle sharding:  * Instead of assigning an application instance to a single partition, it is assigned to multiple \"virtual partitions\".  * This makes it much more unlikely that two users will be assigned to the same partition - eg, 6 application instances can form up to 15 partitions.  * The downside is that virtual partitions partially overlap. So faults in one partition can impact users in another one.  * However, clients can be made fault tolerant and retry requests, so that they hit a different instance within a partition every time.  * The net effect is that impacted users within the system will only experience partial degradation instead of consistent degradation.  </p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#cellular-architecture","title":"Cellular architecture","text":"<p>We can enhance partitioning even more by partitioning the entire application stack with its dependencies into cells based on the user. Each cell is completely independent of another one and a gateway service is responsible for routing requests to the right cell.</p> <p>Azure Storage, for example, uses a cellular architecture. The storage clusters are the cells in the cellular architecture: </p> <p>What makes a cellular architecture appealing is that a cell can have a maximum capacity. When the system needs to scale, a new cell is added vs. scaling out existing ones. This enables you to thoroughly test &amp; benchmark your application for the maximum cell size as you won't suddenly face a surprise surge in traffic.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#downstream-resiliency","title":"Downstream resiliency","text":"<p>This section is about tactical resiliency with regards to stopping faults from propagating from one component to another.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#timeout","title":"Timeout","text":"<p>When you make a network call, it's best practice to add a timeout. If timeout is not set, it is possible that a request will never return which is effectively a resource leak. Timeouts detect connectivity issues &amp; stop them from propagating from one component to another.</p> <p>One practical consideration to have in mind is that most HTTP libraries don't have a timeout set by default, so you have to explicitly configure it. Rule of thumb - always set timeouts when making external calls.</p> <p>How to determine timeout duration? - you could base it on the desired false timeout rate - ie you want up to 0.1% of timeouts to be false. You can determine then the duration by setting it to the 99.9th percentile of the downstream service's response time.</p> <p>It is also important to have good monitoring in place on integration points between systems (such as this one) - status codes, latency, success/error rates, etc. This can be managed by a reverse proxy, located on our pod, to not have to do it ourselves explicitly. This is the sidecar pattern, which was already mentioned.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#retry","title":"Retry","text":"<p>What to do when a request times out? - you can either fail fast or retry.</p> <p>If the timeout is due to a short-lived connectivity issue, then retry with backoff is highly probable to succeed.</p> <p>However, if the downstream service is degraded, retrying immediately will only make matters worse. This is why retries need to be slowed down with increasing delays between them until a max number of retries is reached.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#exponential-backoff","title":"Exponential backoff","text":"<p>To configure delay between retries, we can use a capped exponential function - delay is derived by multiplying it by a constant, which exponentially increases with each retry: <pre><code>delay = min(cap, initial_backoff * 2^attempt)\n</code></pre></p> <p>Example - cap is set to 8 -&gt; Retries are at seconds 2, 4, 8, 8, 8, etc.</p> <p>If however, multiple clients get timeouts at the same time &amp; they all apply exponential backoff concurrently, it is highly likely that they'll cause load spikes, aka retry storm: </p> <p>To avoid this, you can apply random jitter so that your retry time doesn't collide with other clients: <pre><code>delay = random(0, min(cap, initial_backoff * 2^attempt))\n</code></pre></p> <p>Actively waiting between retries isn't the only way to achieve retry. In batch applications, it is common for a process to be parked in a retry queue. The same process (or another) can later read from that queue &amp; retry the requests.</p> <p>Retries only make sense when a service is down due to a short-lived connectivity issue. If the error is consistent - eg service is not authorized to access endpoint, retrying won't help. Additionally, one should be wary of retrying calls to non-idempotent endpoints.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#retry-amplification","title":"Retry amplification","text":"<p>If a request goes through a chain of services and each one of them retries, downstream retries are going to get amplified, leading to significant load on the deepest service: </p> <p>When there's a long dependency chain like this, it makes sense to retry at a single level and fail fast in all others.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#circuit-breaker","title":"Circuit breaker","text":"<p>Retries are useful when there are transient errors. If, however, a service is down due to a more persistent error, retries will not help.</p> <p>There's an alternative approach which detect long-term degradations to a service &amp; stops all requests until they are resolved - circuit breaker. It is useful when the downstream errors are non-transient.</p> <p>Goal of circuit breaker - allow a subsystem to fail without slowing down the caller.</p> <p>Here's how it works:   * When it's in <code>closed</code> state, all requests go through.  * When it's in <code>open</code> state, all requests are blocked.  * Once in <code>open</code> state, occasionally a request is passed through to detect if degradation is resolved. If so, transition to <code>closed</code> state again.</p> <p>Understanding the pattern is easy, but the devil is in the details - how many failures are enough to transition to the open state? How long should it wait to attempt retransmission?</p> <p>Configuring this properly relies on accumulating data about past failures &amp; the system's behavior.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#upstream-resiliency","title":"Upstream resiliency","text":"<p>This chapter focuses on patterns for protecting your own service from upstream clients.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#load-shedding","title":"Load shedding","text":"<p>A server can't control how many requests it receives at a time. The OS has a limit on the number of ongoing connections, but the server typically reaches its limit way sooner than the OS exhausts its physical limit. This is due to running out of resources such as memory, threads, sockets, etc. It leads to requests being significantly degraded until the server eventually crashes.</p> <p>To mitigate this issue, a server should reject excess requests once its capacity is reached so that it can effectively process on-going requests.</p> <p>A counter can be used to measure the number of on-going concurrent requests. It's incremented when a new request comes in and decremented once it's processed. Once the counter passes a predefined threshold (we choose it as the operators), follow-up requests are rejected with eg 503 (Service Unavailable).</p> <p>This technique is referred to as load shedding.</p> <p>One nuance we can implement is differentiating between low-priority and high-priority requests &amp; only rejecting low-priority ones.</p> <p>Although this technique is effective, rejecting a request doesn't fully shield a server from it as it is still effectively handled.  This depends on implementation - eg whether you establish a TCP connection &amp; decode the request to make a decision.</p> <p>Hence, load shedding can only help so much as if the load keeps increasing, the cost of rejecting requests will still lead to a degradation.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#load-leveling","title":"Load leveling","text":"<p>An alternative to load shedding which can be leveraged when clients don't need a prompt response is load leveling.</p> <p>It involves leveraging a message queue to let the service handle requests at its own pace.</p> <p>This pattern is useful for leveling off short-lived load spikes until the service catches up. It doesn't work as well, though, if the load spike is consistent, leading to a backlog. </p> <p>Both load shedding and load leveling don't address load increase directly. Instead, they protect a service from getting overloaded. Both mechanisms are usually combined with autoscaling so that the service can scale out to handle a consistent increase in load.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#rate-limiting","title":"Rate-limiting","text":"<p>Rate-limiting (aka throttling) is a mechanism for rejecting requests once a certain quota is met.</p> <p>Multiple quotas can be maintained - eg both requests per second and bytes per second, and they are typically applied per user, API key or IP address.</p> <p>For a quota of 10 requests per second and an average load of 12 requests per second, 2 of those will will be rejected per second. These kinds of failures need to be designated with a special error code so that clients are aware to retry after a while - a common HTTP code for that is 429 (Too Many Requests).</p> <p>Additional info such as which quota was exceeded + <code>Retry-After</code> header can be included.</p> <p>Use-cases:  * Preventing well-intended clients from hammering the service &amp; letting them gracefully back-off and retry.  * Shield against client bugs which lead to excessive downstream load.  * Applying pricing tiers for platform/infra style products.</p> <p>Rate-limiting only partially protects against DDoS attacks as nothing is stopping malicious actors from continuing to hammer the service even after a 429 status code.</p> <p>It isn't free either - you still have to open a TCP connection &amp; inspect the payload to eg extract the API key &amp; determine the user's available quota.</p> <p>To effectively protect against DDoS, you need to leverage Economy of scale - multiple services shielded behind a gateway which protects both of them against DDoS.  The cost of running the gateway is amortized across all the services.</p> <p>Difference with load shedding - load shedding rejects requests based on the local state of an application instance, whereas rate-limiting is applied across all service instances. Due to this caveat, some form of coordination is required.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#single-process-implementation","title":"Single-process implementation","text":"<p>How to implement rate-limiting for a single process?</p> <p>Naive approach - store a linked list of requests per API key. Periodically, old entries are purged from the list. The problem with this approach is the large memory footprint.</p> <p>Better Alternative:  * Divide time into buckets with fixed duration (eg 1s) and keep track of seen requests per bucket.   * When a request comes, we increment the corresponding bucket's counter by one based on the request's timestamp - this technique doesn't scale memory consumption with the number of requests.   * We only maintain at most two buckets, since we don't care about buckets older than one epoch (ie older than 1s).  * A sliding window is used to calculate the current quota in the middle of an epoch - based on epoch overlap, % of the quota is taken (ie 60% of 3 requests is ~2).   * The sliding window technique is an approximation but it's good enough &amp; is worth it for the smaller memory footprint. Also, the smaller the epoch, the better the approximation.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#distributed-implementation","title":"Distributed implementation","text":"<p>When you want to scale rate-limiting beyond a single instance, you'll need to move the logic, described above from in-memory into an external key-value storage.</p> <p>The challenge then is synchronizing concurrent reads/writes:  * One option is to leverage transactions, but that is not effective enough for such a critical path component. A subtler issue is that the service has a hard dependency on the external data store.  * A better alternative is using the atomic <code>compare-and-swap (CAS)</code> style APIs distributed data stores provide. A particularly useful one is <code>getAndIncrement</code>.  * To remove the hard dependency, we can batch updates in-memory and we periodically flush them to the distributed store. This makes the calculations slightly inaccurate but accurate enough.  * In the event the data store is down, we temporarily fallback to using in-memory store exclusively. </p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#constant-work","title":"Constant work","text":"<p>In the event an application gets overloaded, some of its mechanisms start behaving differently - eg how a query is executed in an overloaded database is different from how it's executed in happy times.</p> <p>This type of behavior is referred to as multi-modal.  Some of the modes can trigger rare bugs due to code assuming the happy path at all times. They also make life harder for operators because the mental model of the application's behavior expands.</p> <p>Hence, as a general rule of thumb - strive to reduce the number of modes in an application.</p> <p>For example, prefer a key-value store vs. a traditional relational database in a data plane due to their predictable performance. A relational database has a lot of hidden optimizations based on the load, which leads to a multi-modal system.</p> <p>in an ideal world, the worst and average-case behavior shouldn't differ. You can use the constant work pattern to achieve this.</p> <p>The idea is to have the system perform the same amount of work under high load as under normal load. If there is any variation during stress periods, it better be because the system is performing better.</p> <p>Such a system is antifragile. A resilient system keeps operating under increased load. An antifragile system performs better.</p> <p>An example of the constant work pattern is propagating changes from the control plane to the data plane:  * The control plane stores a bag of settings for each user, like the quotas used by the rate limiter.  * When a setting changes, the control plane needs to broadcast the change to the data plane. This mechanism makes the work proportional to the number of changes.  * This can lead to an issue if multiple configuration changes are made for all users concurrently, leading to a burst of update messages, which the data plane can't handle.  * Alternatively, the control plane can periodically dump the configuration in a highly-available file store (eg Amazon S3), which includes the configuration for all users, not just updated ones.  * Data planes then periodically lead the dump in bulk and refresh their local state.  * No matter how many updates happen, the work done by the control &amp; data planes is constant.</p> <p>A niche extension is preallocating the configuration with keys for every user in the system. If the data plane can process that configuration, this guarantees that the processing time will stay the same regardless of what happens. This is typically used in cellular architectures where the maximum number of users in a system is limited.</p> <p>Advantages:  * Not only is it reliable, it's also easier to implement than an event-sourcing approach where you have to process individual stacked updates.  * This is robust against all sorts of failures due to its self-healing properties. If eg the config file gets corrupted, the next update will automatically resolve the issue.</p> <p>The main disadvantage is that constant work is more expensive than doing just the necessary work. But it leads to increased reliability and it reduces the implementation complexity.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part04/#summary","title":"Summary","text":"<p>As the number of components in a system increases, so does the possible failures. Anything that can happen will happen.</p> <p>Usually, a production engineer is more worried about minimizing and tolerating failure vs. scaling the system. This is because scaling is only necessary until you hit the next scalability bottleneck. Handling failures is a consistent worry, regardless of your scale.</p> <p>Failures are inevitable. When you can't design them away, focus on reducing their blast radius.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/","title":"Maintainability","text":"<ul> <li>Testing</li> <li>Continuous delivery and deployment</li> <li>Monitoring</li> <li>Observability</li> <li>Traces</li> <li>Manageability</li> <li>Summary</li> </ul> <p>The majority of the cost of software is spent in maintaining it after its initial development:  * Fixing bugs  * Adding new features  * Operating it</p> <p>We should aspire our systems to be easy to modify, extend and operate so that they're easy to maintain.</p> <p>How to do that?  * Good testing is a minimal requirement to be able to extend a system while ensuring it doesn't break.  * Once the change is merged into the repo, it should be rolled out into production without that affecting the application's availability.  * Operators need to be able to monitor the system's health, investigate degradations and restore it when it gets into a bad state.  * This requires altering the system's state without touching the code - via a configuration change or a feature flag.</p> <p>Historically, developers, testers and operators were different teams. Nowadays, developers do it all.</p> <p>This part is all about best practices in testing &amp; operating large scale distributed systems.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#testing","title":"Testing","text":"<p>The longer it takes to detect a bug, the more expensive it is to fix it.</p> <p>How does testing help?  * A software test verifies that some part of the application works correctly, catching bugs early.  * The real benefit, though, is that tests allow you to alter the system's behavior with high confidence that you're not breaking the expected behavior.  * Tests are also an always up-to-date documentation of the codebase.  * Finally, they improve the public interface of a system as developers are forced to look at the system from a client's perspective.</p> <p>Testing is not a silver bullet, though, as you can't predict all the states an application can get into. You can only predict those behaviors developers can predict. Oftentimes, complex behaviors which only occur in production are not captured by tests.</p> <p>Regardless, tests do a good job of validating expected behaviors, although they don't guarantee your code is bug-free.</p> <p>If you want to be confident in your application's behavior, you'll need to add tests for it.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#scope","title":"Scope","text":"<p>Scope defines the code path under test aka system under test (SUT).</p> <p>This designates whether a test is unit, integration or end to end.</p> <p>A unit test validates the behavior of a single component such as a class. A good unit test is relatively static &amp; only changes when the behavior of the component changes.</p> <p>To achieve that, a unit test should:  * work with the public interface of the component only.  * test for state changes in the SUT vs testing for a predetermined sequence of actions.  * test for behaviors - ie, how SUT reacts to a given input given it is in a specific state.</p> <p>An integration test has a larger scope than a unit test as it verifies the correct integration with an external component.</p> <p>Integration tests have different meanings in difference contexts. Martin Fowler defines them as:  * A narrow integration test only exercises the code paths of a service, communicating with an external dependency.  * A broad integration test exercises code paths across multiple services. We will refer to those as end-to-end tests.</p> <p>An end-to-end test validates behavior that spans multiple services, ie a user-facing scenario. These tests usually run in a shared environment (eg staging or prod). They should not impact other users or tests using the same environment.</p> <p>Because of the larger scope, they are slower &amp; more prone to intermittent failures.</p> <p>These can also be painful to maintain - when such a test fails, it's not obvious which particular component failed. But they're a necessary evil as they validate user-facing behavior which is harder to validate with tests with a smaller scope.</p> <p>To minimize end-to-end tests, you can frame them as a user journey test - they validate multi-step interactions of a user with a system vs. testing an individual action.</p> <p>As the scope of a test increases, it becomes more brittle, slow and costly. Intermittently failing tests are nearly as bad as no tests at all since developers quickly learn to ignore them due to the noise.</p> <p>A good trade-off is to have a lot of unit tests, a smaller fraction of integration tests and a few end to end tests: </p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#size","title":"Size","text":"<p>The size of the tests determines how much computing resources (ie nodes) the test needs to run. This largely depends on how realistic the environment where the test runs is.</p> <p>Scope and size are usually correlated, but they are distinct concepts.</p> <p>How to differentiate tests in terms of size:  * A small test runs in a single process, without performing any I/O - it's very fast, deterministic &amp; probability of intermittent failures is low.  * An intermediate test runs on a single node and performs local I/O - this leaves more room for delay &amp; intermittent failures.  * A large test requires multiple nodes to run - leading to more non-determinism and delays.</p> <p>The larger the test is, the longer it takes to run &amp; the flakier it becomes. Hence, we should write the smallest possible test for a given behavior.</p> <p>A technique to avoid increasing the size of a test is using test doubles:  * A fake is a lightweight implementation of the external dependency that behaves similarly to it. Eg an in-memory version of a database.  * A stub is a function which always returns the same value regardless of input.  * A mock has expectations on how it should be called and it's used to test interactions between objects.</p> <p>The problem with test doubles is that they don't behave like the real system, hence the confidence we have with them is lower. That's why - when the real implementation is fast, deterministic &amp; has few dependencies, we can use it directly instead. When that is not an option, we can use a fake of the real implementation, which is maintained by the same developers.</p> <p>Stubbing and mocking are last-resort options as they offer the lowest degree of confidence.</p> <p>For integration tests, using contract tests is a good compromise. A contract test defines the request for an external dependency with expected result. The test uses this contract to mock the dependency.</p> <p>But the dependency also validates its system acts in the way the contract expects it to.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#practical-considerations","title":"Practical considerations","text":"<p>Testing requires trade-offs, similar to everything else.</p> <p>Let's imagine we want to test the behavior of a specific API endpoint of a service with:  * A data store  * An internal service owned by another team  * A third-party API used for billing </p> <p>As previously discussed, we should attempt to write the smallest possible test for the scope we want while minimizing the use of test doubles.</p> <p>Our decisions:  * Assuming our endpoint doesn't use the internal service, we can use a mock in its place.  * If the data store has an in-memory version, we can use it to avoid making network calls.  * We can't call the third-party billing API directly as that entails making actual transactions, but the API might have a testing environment we can use instead.</p> <p>A more nuanced example - testing that we can safely purge the data belonging to a particular user across the entire system as that's mandated by GDPR. Failing to comply can lead to fines up to 20mil EUR or 4% annual turnover.</p> <p>The impact of this functionality silently breaking is high, hence we should maximize our confidence that the functionality works correctly - we'll need to setup an end to end test, that periodically runs in production and uses live services.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#formal-verification","title":"Formal verification","text":"<p>Software tests are not the only way to capture bugs early.</p> <p>Writing a high-level description of how the system behaves (ie specification) allows subtle bugs &amp; architecture shortcomings to be detected before writing any code. A specification helps us reason about the behaviors of our system. It's also a documentation and guide for the ones which implement the system.</p> <p>A specification can range from a one-pager to a formal mathematical description that a computer can check. Writing one doesn't entail describing every corner of the system in detail - we only want to specify those paths that are most likely to contain errors, which are hard to detect via tests.</p> <p>TLA+ is a well-known formal specification language. One can use it to describe the behavior of a system via a set of states &amp; changes across them. Microsoft &amp; Amazon use it to describe their most complex distributed systems, such as S3 and CosmosDB.</p> <p>The goal of using such a tool is to verify the safety &amp; liveness of an application:  * Safety - something is true for all behaviors of the system (invariant).  * Liveness - something eventually happens.</p> <p>TLA+ is very useful for systems running at huge scale as those will eventually run into states, which humans can't imagine.</p> <p>Example - we have a key-value store X and we want to migrate to key-value store Y.</p> <p>How to implement that:  * Service writes to both X and Y (dual writes), while reading from X.  * One-off batch process backfills Y with the data from X.  * Application switches to read/write exclusively from Y.</p> <p>Approach seems reasonable but it doesn't guarantee that both stores will eventually end up in the same state:  * In case write to A succeeds, but write to B fails, the data stores are not in the same state.  * Using an atomic write resolves the liveness issue, but two service instances writing at the same time can lead to writes not coming in the same order. </p> <ul> <li>Finally, using a message channel between application instances and the database can serialize the writes and guarantee global order.</li> </ul> <p>Regardless of the specific problem, the point is that using a formal verification system such as TLA+ can help us catch these kinds of errors before any code is written.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#continuous-delivery-and-deployment","title":"Continuous delivery and deployment","text":"<p>Once a change is merged in a repository, it needs to get released to production. If the rollout happens manually, it won't happen often.</p> <p>This leads to several unreleased changes being merged &amp; the eventual rollout becomes more and more risky. Also, if an issue is discovered, it's hard to figure out which change exactly caused the issue.</p> <p>Furthermote, the developer who deployed the change needs to monitor dashboards and alerts to ensure the change is working.</p> <p>In a nutshell, manual deployments are a waste of time and should be automated. Once a change hits the main branch, it should be automatically deployed to production.</p> <p>Once that's done, the developer is free to pick up their next task vs. babysitting the deployment process. This whole process can be automated via a continuous delivery pipeline (CD).</p> <p>Releasing changes is among the main root causes of failures. This is why, significant time needs to be invested in safeguards, monitoring and automation. Whenever a bad version is detected, the pipeline should automatically rollback to the previous stable version.</p> <p>There is a balance between rollout safety &amp; the time it takes to release a change to production. A good pipeline makes a balance between the two.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#review-and-build","title":"Review and build","text":"<p>There are four stages involved in deployment: </p> <p>It starts with a pull request, which is reviewed. A pipeline needs to validate the change by running a set of linters &amp; tests to ensure it's valid.</p> <p>A team member needs to review and approve the change. A checklist helps here:  * Does the change include unit/integration/end to end tests as needed?  * Does the change incldue metrics, logs, traces?  * Can this change break production by introducing a backwards-incompatible change?  * Can the change be rollbacked safely?</p> <p>This process can be reused for non-code related changes as well - static assets, end to end tests, configuration files. All of this should be version controlled in a repository.</p> <p>It's very important to release configuration changes via a CD pipeline. They are a common source of outages.</p> <p>Infrastructure should also be defined as code - Infrastructure-as-code. Terraform is the most popular tool for that. This enables all infra changes to be version controlled &amp; reviewed just like normal code.</p> <p>Once a change is moved into the main branch, we proceed to the build stage, where the code is built &amp; release artifacts are created.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#pre-production","title":"Pre-production","text":"<p>In this stage, the change is released in a non-production environment where end-to-end tests are run. Although this environment isn't as realistic as production, it's helpful for verifying end to end integration.</p> <p>There can be multiple pre-production environments.  One can have simple smoke tests, while another can have part of production traffic mirrored to it to make it more realistic.</p> <p>The CD pipeline should assess an artifact's health using the same metrics used in production.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#production","title":"Production","text":"<p>Once pre-production passes, the artifact can be deployed to production.</p> <p>It should be released to a small number of instances initially. The goal is to surface problems not detected so far before hitting all production instances. If this stage passes, it is then gradually rolled out to the rest of the instances.</p> <p>During deployment, part of the instances can't serve production traffic, hence, the rest of the instances need to compensate. You should ensure there is sufficient capacity to handle this operation.</p> <p>If there are multiple regions the service is deployed to, it should first be deployed to the lower traffic region. The remaining region rollouts should be divided into sequential stages to minimize risk.</p> <p>The more stages there are, the longer the deployment will take.  To mitigate this, you can make a parallel rollout once enough confidence has been built in the initial stages.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#rollbacks","title":"Rollbacks","text":"<p>For every step, the CD pipeline needs to verify the artifact's health. If it's not OK, an automatic rollback should be triggered. Example indicators you could use - end to end tests, health metrics, errors and alerts.</p> <p>Just monitoring the health metrics is often not enough. The CD pipeline should also monitor the health of upstream/downstream dependencies to detect any indirect impact.</p> <p>The pipeline should wait some time between stages (bake time) to ensure it's successful since some issues can take a while to surface. The bake time can be reduced after each successful stage.</p> <p>You could also determine the bake time based on the number of requests you've received so that your API can be properly exercised to raise confidence it works properly.</p> <p>When any indicator fails, the CD pipeline stops - at this stage, it can either trigger a rollback or alert the oncall engineer for manual intervention. Based on the engineer's input, the CD pipeline can either retry or rollback entirely.</p> <p>In some circumstances, the operator might choose to wait for a new version to get released and roll it forward. This might be necessary due to a backwards incompatible change. One of the common reasons is changing the serialization format for persistence or IPC.</p> <p>For safely introducing a backwards incompatible change, it can be broken down into several smaller backward-compatible changes.</p> <p>Example - handling backwards-incompatible message schema change between producer and consumer:  * Prepare change - consumer is modified to support new and old formats.  * Activate change - producer is modified to write messages in the new format.  * Cleanup change - The consumer stops supporting the old messaging format altogether. This is only released once enough confidence is accumulated.</p> <p>An automatic upgrade-downgrade test step can be setup in pre-production to verify a change can be safely rolled back.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#monitoring","title":"Monitoring","text":"<p>Monitoring is used to detect production issues quickly &amp; alert human operators responsible for the system. Another use-case is to provide high-level health information about a system via dashboards.</p> <p>Initially, monitoring was black-box - reporting only if service was up. After a while, developers started instrumenting their code to track the health of specific features (white-box monitoring). Black-box monitoring is useful to detect symptoms of a failure, white-box monitoring is useful to identify the root cause.</p> <p>Main use-case for black box monitoring is to track health of external dependencies and validate the impact to users. A common approach is to run periodic scripts (synthetics) which send periodic requests to test endpoints and monitor latency &amp; success rate.</p> <p>A benefit of using synthetics is that they can catch issues not visible to the application (eg connectivity issues) and they can be used to exercise endpoints which aren't used by users that often. Example - DNS server is down &amp; service thinks it's just not receiving a lot of requests. Synthetics would catch that issue as they are unable to resolve a service's IP address.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#metrics","title":"Metrics","text":"<p>Metric == time series of raw measurements for resources (ie CPU load) or behavior (ie number of requests). Each sample is represented by a floating-point number and a timestamp.</p> <p>A metric can also be tagged with labels - set of key-value pairs. This can be used to filter metrics for eg specific region, environment, node, etc. Labels are very useful to be able to filter without creating separate metrics for different environments.  But they're also harder to store &amp; process because every label combination is a distinct metric.</p> <p>Minimum metrics for a service to consider:  * Request throughput  * Internal state (eg in-memory cache)  * Dependencies availability and performance</p> <p>This requires, however, deliberate effort by developers to instrument their code.</p> <p>Example simple call with various metrics you might want to consider instrumenting: <pre><code>def get_resource(id):\n    resource = self._cache.get(id) # in-process cache\n    # Is the id valid?\n    # Was there a cache hit?\n    # How long has the resource been in the cache?\n\n    if resource is not None:\n        return resource\n\n    resource = self._repository.get(id)\n    # Did the remote call fail, and if so, why?\n    # Did the remote call time out?\n    # How long did the call take?\n\n    self._cache[id] = resource\n    # What's the size of the cache?\n\n    return resource\n    # How long did it take for the handler to run?\n</code></pre></p> <p>How does metric reporting work? - one way is via event-based reporting:  * Whenever the handler fails, it reports a failure count of 1 in an event, sent to a local telemetry agent: <pre><code>{\"failureCount\": 1, \"serviceRegion\": \"EastUs2\", \"timestamp\": 1614438079}\n</code></pre>  * Agent batches the events &amp; periodically sends them to a remote telemetry service, which persists them in a specialized store.  * To reduce the storage costs, events can be pre-aggregated into time buckets of eg 1m, 5m, 1h, etc. <pre><code>\"00:00\", 561,\n\"01:00\", 42,\n\"02:00\", 61,\n...\n</code></pre>  * This pre-aggregation can be done client-side by the telemetry agent and server-side. The technique dramatically reduces the cost of metrics.  * But the downside is that we lose the ability to re-aggregate on a lower scale - eg aggregating per 5m, when we've pre-aggregated on 1h.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#service-level-indicators","title":"Service-level indicators","text":"<p>Just because we can alert on anything due to the advent of metrics, doesn't mean we should alert on everything.  It doesn't make sense to be alerted during the middle of the night when there was a spike in memory consumption.</p> <p>One specific metric category which is good to alert on is SLI (service-level indicator) - this is a metric tied to the level of service we provide to our users, eg. response time, error rate, throughput.</p> <p>SLIs are typically aggregated over a rolling time window &amp; represented via a summary statistic, ie average or percentile. They are also typically a ratio of two items - \"good events\" over the total number of events. This makes it easy to measure - 0 means service is broken, 1 means that whatever is measured has no issues.</p> <p>Commonly used SLIs:  * Response time - how many requests occurred faster than a given threshold  * Availability - how long was the service usable, ie. - (successful requests / total requests) </p> <p>Where should we measure - eg with response time, should we measure response time as seen by service, load balancer or clients? Ideally, we should measure as close to the user's experience as possible. If too costly, pick the next best candidate.</p> <p>How should we measure response time? Usually via a long-tailed, right-skewed distribution. Distribution can be summarized via a statistic - ie average, but it's not very good because one large outlier skews the whole average. In example 99 requests of 1s and one request of 10m make for an average of 7s.</p> <p>Percentiles are a better way to summarize a distribution - the value below which a percentage of response times fall.  For example, if 99th percentile is 1s, then 99% of requests are below 1s. Rightmost percentiles (ie 99.9th percentile) are referred to as long-tailed latencies. They represent a small fraction of requests and their latencies.</p> <p>It might be important to monitor those because although they're small in quantity, they could be the most important business users of our application.</p> <p>Don't underestimate latencies - studies show that 100ms delay in load time hurts conversion rates by 7%. Long-tail response times can dramatically impact the service - if you have 2k threads serving 10k requests and 1% of requests start taking 20s to complete, you'll need 2k more threads to handle the load.</p> <p>Also, reducing long-tail latencies usually improves our average-case scenarios as well.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#service-level-objectives","title":"Service-level objectives","text":"<p>Servive-level objective (SLO) == range of acceptable values for an SLI which indicate the service is healthy. SLOs can be used to define SLAs with users - a contractual agreement that dictates what are the consequences once an SLA is not met, usually a fine of sorts.</p> <p>Eg, an SLO can define that 99% of API calls to endpoint X should complete below 200ms over a rolling window of 1 week.</p> <p>Another point of view is that only 1% of requests are allowed to have more than 200ms latency. This 1% is the error budget, which represent the number of failures which can be tolerated. SLOs are helpful for alerting and help the team prioritize repair tasks.  </p> <p>Examples:   * Team can agree that repair items are prioritized over new features once error budget is exhausted.  * Incident importance can be measured by the amount of error budget burned.</p> <p>Small time windows force the team to act quickly and prioritize bug fixes and repair items.  Longer time windows are better suited for longer term decisions on which projects to invest in.</p> <p>How strict should an SLO be?  * Too lenient - we won't detect user-facing issues.  * Too strict - engineering time is wasted with micro optimizations that yield diminishing returns.</p> <p>Note that 100% reliability doesn't guarantee 100% reliable experience to users since we can't eg guarantee their last-mile connection.</p> <p>It's reasonable to start with comfortable ranges for an SLO at first &amp; adjust over time. Overall, anything above 3 nines (99.9%) is very costly &amp; provides diminishing returns.</p> <p>Strive to keep things simple &amp; have as few SLOs as possible that provide good enough indication of service health.</p> <p>SLOs need to be agreed on with stakeholders. If error budger is burned too quickly, repair items need to be prioritized over features.</p> <p>From Google's SRE book:</p> <p>if you can\u2019t ever win a conversation about priorities by quoting a particular SLO, it\u2019s probably not worth having that SLO.</p> <p>Users might become over-reliant on actual behavior of our service rather than documented SLA.  It might make sense to introduce controlled failures to ensure dependencies can cope with targeted SLA - eg, we do weekly DC failovers at Uber to make sure critical services can withstand a disaster where one of the DCs goes down.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#alerts","title":"Alerts","text":"<p>The part of a monitoring system which triggers an action once a metric crosses a threshold.</p> <p>Depending on the severity &amp; type, an alert can:  * Trigger an automated script to eg restart the service.  * Paging the engineer who is on-call for the impacted service.</p> <p>For an alert to be useful, it has to be actionable - The operator should be able to quickly assess the impact &amp; urgency. For example, an alert on CPU usage is not useful, but an alert on an SLO is more useful as it directly impacts users and you know how.</p> <p>One could monitor the SLO's error budget and trigger an alert once a large fraction of it has been consumed.</p> <p>When defining alerts, there is a trade-off between precision and recall:  * Precision - fraction of significant events over total number of alerts. (ie how many alerts actually indicate an incident?)  * Recall - fraction of significant events which triggered an alert. (ie are you missing any significant events you should have alerted on?)</p> <p>Low precision leads to a lot of alerts which are noisy and not actionable. Low recall leads to outages which is often not alerted on.</p> <p>Although it would be great to have 100% of both, improving one typically lowers the other.</p> <p>Practical example on defining an alert:  * We have a SLO target of 99% over 30 deys  * A naive approach for alerting would be to alert whenever the alert goes down below 99%, but it can be a transient failure.  * We can increase the time an alert needs to be active to trigger, but now the alert will take longer to trigger even during an actual outage.  * A better approach is to trigger based on how fast the error budger is being burned (burn rate).  * Burn rate == % of error budget consumed / % of elapsed SLO time window.  * Burn rate = 1 =&gt; error budget is exhausted in 30 days, Burn rate = 2 =&gt; error budget is exhausted in 15 days, etc.  * To improve recall, we can configure the alert to be low-severity for burn rate of 2 and high-severity for burn rate of 10.</p> <p>Most alerts should be configured based on SLOs.  But we should also have alerts on known failure modes such as memory leaks we haven't had time to design away. In this case, automatic mitigation by restarting the service should suffice.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#dashboards","title":"Dashboards","text":"<p>Apart from alerting, metrics are used to render dashboards which present the system's real-time health. Dashboards can, however, become a dumping ground for charts which are not useful and forgotten.</p> <p>When creating dashboards, we need to first consider who the audience is and work backwards. </p> <p>Example dashboards:  * SLO Dashboard - used by org's stakeholders to gain quick insight into the system's health. During an incident, it shows user impact.  * Public API Dashboard - show health of public endpoints, used by operators to trace the error path during an outage.  * Service dashboard - service-specific implementation details used by developers who have more context about the service.     * Apart from service metrics, it should also track upstream and downstream health of dependencies/dependents.     * These include other services, but also infra components such as message queues or load balancers.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#best-practices","title":"Best practices","text":"<p>Dashboards should be defined using domain-specific languages and version-controlled like code. This lets them be kept in sync across different environments. They're updated from a single pull request without the need for manual steps, which is error-prone.</p> <p>The most important charts should be at the top of the dashboards because that's what the operators see first. They should also be rendered with a default timezone to ease communication between operators.</p> <p>All charts in the dashboard should use the same time resolution - 1m, 5m, 1h, etc, and range - 24h, 7d, etc. We can pick the default resolution based on most frequent use-case - 1h range &amp; 1m resolution is useful for incidents, while 1y range &amp; 1d resolution is useful for capacity planning.</p> <p>The number of metrics and data points on a chart should be kept to a minimum as it makes dashboards slow &amp; hard to understand.</p> <p>A chart should contain metrics within a similar range, otherwise, the largest one will hide all others. You can split related statistics in multiple charts - eg 10th, average &amp; 90th percentile can be one chart, 99.9th and 0.1th percentile can be another.</p> <p>A chart should also contain annotations:  * Description, links to runbooks, related dashboards and escalation contacts.  * Horizontal lines for alert thresholds.  * Vertical line for each relevant deployment.</p> <p>Metrics shouldn't just be emitted when there are errors.  Otherwise, charts will render will gaps, from which you can't derive whether there were no errors or your service stopped emitting metrics. To avoid this, emit a metric with 0 in case there are no errors and 1 when there are.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#being-on-call","title":"Being on call","text":"<p>Healthy on-call is possible when services are built with realiability and operatbility in mind. To incentivize developers to do that, they can be the ones on-call so that they aim to reduce operational toll to minimum. They are also the ones which make most sense to be on-call, because they know the service most intimately.</p> <p>Being on-call is stressful. Even if there are no alerts, it is stressful to not be able to freely leave your desk after working hours. This is why, being on-call should be compensated and on-call engineer shouldn't be expected to progress on feature work.</p> <p>Healthy on-call is possible when alerts are actionable - they link to relevant dashboards &amp; runbook.</p> <p>Unless a false positive, all actions taken by operator should be communicated in a global outages channel. This lets other engineers more quickly catch up and pick up from where the on-call engineer left off in case of escalations.</p> <p>The first step during an outage should be to mitigate it, not fix it. Once mitigated, understand the root-cause (in normal working hours) and figure out a way to prevent it from happening again. The greater the outage impact (measured by the SLOs), the more time you ought to spend on preventing it.</p> <p>When an incident burns a significant amount of the error budget, there should be a postmortem.  Its goal is to understand the root cause &amp; come up with repair items that prevent it from happening again.</p> <p>Additionally, if the incident spiraled out of control, there should be an agreement within the team to spot feature work and focus solely on the repair items.</p> <p>The SRE books are a great reference to learn more about creating a healthy on-call rotation.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#observability","title":"Observability","text":"<p>A distributed system is not always 100% healthy. Using some of the techniques described in the book, we can tolerate and mitigate failures but with every moving part, the system becomes more complex.</p> <p>With more complexity, it becomes very hard to reason about the possible emergent behaviors the system can have.</p> <p>Human operators are still needed for a distributed systems as there are things which cannot be automated - eg debugging a failure.</p> <p>To effectiely debug a failure, an operator needs to inspect metrics &amp; dashboards. But that's not sufficient.</p> <p>Observability is a set of tools that provide granular insights into a production system. A good observability system minimizes the time it takes to validate a failure hypothesis.</p> <p>This requires a rich suite of granular events, as one can't know what will be useful in the future upfront. At the core of observability are metrics, event logs and traces.</p> <p>Metrics are stored in time-series databases, that have high throughput, but struggle with high dimensionality (having many columns). Event logs and traces are stored in stores that support highly-dimensional data (a lot of columns), but have lower throughput.</p> <p>Metrics are used for monitoring, event logs and traces are used for debugging.</p> <p>Monitoring is part of observability. Monitoring is focused on tracking a system's health, while observability also includes tools for debugging production issues.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#logs","title":"Logs","text":"<p>Log == immutable list of time-stamped events which happened over time.</p> <p>The format can be either free text or a structured format such as JSON or protobuf.</p> <p>Example structured log: <pre><code>{\n    \"failureCount\": 1,\n    \"serviceRegion\": \"EastUs2\",\n    \"timestamp\": 1614438079\n}\n</code></pre></p> <p>Logs can originate from services we write or external dependencies such as a message broker or data store. Most languages have structure logging libraries, which help with creating these kinds of logs.</p> <p>Logs are typically dumped in a file, which is then sent to a log collector (ie ELK stack, AWS CloudWatch) asynchronously.</p> <p>Logs provide a lot of information about what's happening in a service, given they're instrumented properly. They help with tracing the execution of a particular request or tracing back eg a service crash.</p> <p>They are simple to emit, but that's about the only advantage they have compared to metrics and other telemetry data:  * Logging libraries can add overhead to our services if they don't flush logs to disk asynchronously.  * If the disk gets full, either we stop emitting logs or the service starts degrading.  * They're not cheap to store &amp; ingest regardless of whether we use cloud or in-house deployment due to the volume.  * Structured logs are more expensive than free-form ones due to their high dimensionality.  * Usually have low signal to noise ratio as they're very fine-grained and service specific.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#best-practices_1","title":"Best practices","text":"<p>To make logs more effective, all the data related to a particular context (eg a user request) needs to be tagged with eg a request id. To effectively implement this, code paths typically need to pass through a context object which enriches the logs. This is effective to avoid joining data, but you still might have to do it if a request spans multiple services.</p> <p>Additionally, events should have useful data about the work unit - who created it, what it was for, whether it succeeded or failed, how long it took. Finally, logged data should be stripped from any sensitive information such as PII.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#costs","title":"Costs","text":"<p>To keep the cost under control, you could:  * Setup log levels and only log the more interesting ones (ie ERROR). You can control this via a flag an operator can toggle.  * Use sampling - only log every Nth event. Events can also be prioritized, ie errors are more interesting than successful calls.</p> <p>Even so, someone can unintentionally introduce a bug which leads to a surge of logs. Therefore, log collectors should rate limit requests. What's more, these options help with a single-node system. As we add more nodes, logging will inevitably increase.</p> <p>Finally, we can always decide to aggregate metrics &amp; emit those vs. raw logs but we lose the ability to drill down if necessary.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#traces","title":"Traces","text":"<p>Tracing captures the entire lifespan of a request throughout the services of our distributed system.</p> <p>Trace == a list of related spans, that represent the execution flow of a request. Span == a time interval which represents a logical operation with a bag of key-value pairs. </p> <p>When a request is initiated, it is assigned a unique trace ID. It is propagated across services so that you can trace the entire lifecycle. Each stage of a trace, is represented by a span. Once a span ends, it is propagated to a collector service which assembles it into a trace. Popular distribute trace collectors - Open Zipkin, AWS X-Ray.</p> <p>Traces enable a developer to:  * Debug issues caused by specific requests. Useful for client-raised issues.  * Debug rare issues which affect a small number of requests.  * Debug common issues which affect a large number of requests and they all have an issue in common.  * Identify bottlenecks in an end-to-end request.  * Identify which users hit what downstream services. Can be used for rate-limiting or billing purposes.</p> <p>Tracing is challenging to retrofit into an existing system as it requires each component to proactively instrument them.</p> <p>In addition to that, third-party services and libraries we use also need to support traces.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#putting-it-all-together","title":"Putting it all together","text":"<p>The main drawback of event logs is that they are too fine-grained and service-specific.</p> <p>When a user request passes through a system, it can go through several services.  Logs can only be used within the context of a single service.</p> <p>Similarly, a single event doesn't give much info about the health or state of the system.</p> <p>Metrics and traces are much more useful. They can be thought of as derived views built from event logs &amp; optimized for specific use-cases.  * Metrics == time series of summary statistics, derived by aggregating counters or observations over multiple events.  * Traces == aggregation of events belonging to the lifecycle of a specific user request.</p> <p>In both cases, we can emit individual counters or spans and have the backend aggregate them into more useful metrics/traces.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#manageability","title":"Manageability","text":"<p>Operators use observability tools to understand the behavior of a system, but they also need a way to modify it without making any code changes.</p> <p>Examples - releasing a new version to production or changing the way a system behaves via a configuration change.</p> <p>An application generally depends on various configuration settings. Some affect its behavior (ie max cache size), others contain secrest (eg GCP credentials). Since these settings vary per environment, they shouldn't be hardcoded.</p> <p>To decouple the application from its configuration, you can use a dedicated store - eg AWS AppConfig or Azure App Configuration.</p> <p>The CD pipeline can read the configuration at deployment time &amp; override your defaults for the specific environment.</p> <p>The drawback is that you can't change the configuration without redeploying the service. Alternatively, the configuration can be made to react to configuration changes by periodically re-reading the configuration and applying the changes.</p> <p>Once that's possible, new features can be released with feature flags you can toggle. This enables you to release a new build with a disabled feature at first and enable it later for eg a fraction of application users to build up confidence.</p> <p>The same mechanism can be used to implement A/B tests.</p>"},{"location":"booknotes/system-design/understanding-distributed-systems/part05/#summary","title":"Summary","text":"<p>Everyone who spends a few months in a team that operates a production system should be quite familiar with these topics.</p> <p>Although we all want to design new services, the majority of time is spent on maintaining existing ones - fixing bugs, adding new features, operating the service.</p> <p>In the author's opinion, the only way to become a better system designer is by embracing these maintenance activities. Aspire to make your systems easy to modify, extend and operate so that they can be easily maintained.</p> <p>This enables you to have a sixth-sense that enables you to critique the design of other services and ultimately you become a better systems designer.</p> <p>Take every chance you get to learn from production systems. Be on call.  Participate in as many post-mortems as possible. Ask yourself - how could the incident have been avoided.</p> <p>These activities will pay off in the long-term way more than reading about the latest architectural trends.</p>"},{"location":"booknotes/system-design-interview/","title":"System Design Interview - An Insider's Guide (vol 1 &amp; 2)","text":"<p>These notes are based on the System Design Interview books - vol 1 and vol 2.</p> <p>Instead of getting the physical books, I've bought the online course, so there might be some mismatch with the physical book's chapter indices. In addition to that, there could be some content updates for the online course, but not the physical books.</p> <p>Note: These notes are a work in progress. I'll remove this remark once I go through the whole book.</p> <ul> <li>Chapter 2 - Scale From Zero To Millions Of Users</li> <li>Chapter 3 - Back-of-the-envelope Estimation</li> <li>Chapter 4 - A Framework For System Design Interviews</li> <li>Chapter 5 - Design A Rate Limiter</li> <li>Chapter 6 - Design Consistent Hashing</li> <li>Chapter 7 - Design A Key-Value Store</li> <li>Chapter 8 - Design A Unique ID Generator In Distributed Systems</li> <li>Chapter 9 - Design A URL Shortener</li> <li>Chapter 10 - Design A Web Crawler</li> <li>Chapter 11 - Design A Notification System</li> <li>Chapter 12 - Design A News Feed System</li> <li>Chapter 13 - Design A Chat System</li> <li>Chapter 14 - Design A Search Autocomplete System</li> <li>Chapter 15 - Design YouTube</li> <li>Chapter 16 - Design Google Drive</li> <li>Chapter 17 - Proximity Service</li> <li>Chapter 18 - Nearby Friends</li> <li>Chapter 19 - Google Maps</li> <li>Chapter 20 - Distributed Message Queue</li> <li>Chapter 21 - Metrics Monitoring And Alerting System</li> <li>Chapter 22 - Ad Click Event Aggregation</li> <li>Chapter 23 - Hotel Reservation System</li> <li>Chapter 24 - Distributed Email Service</li> <li>Chapter 25 - S3-like Object Storage</li> <li>Chapter 26 - Real-time Gaming Leaderboard</li> <li>Chapter 27 - Payment System</li> <li>Chapter 28 - Digital Wallet</li> <li>Chapter 29 - Stock Exchange</li> </ul>"},{"location":"booknotes/system-design-interview/chapter02/","title":"Scale From Zero to Millions of Users","text":"<p>Here, we're building a system that supports a few users &amp; gradually scale it to support millions.</p>"},{"location":"booknotes/system-design-interview/chapter02/#single-server-setup","title":"Single server setup","text":"<p>To start off, we're going to put everything on a single server - web app, database, cache, etc. </p> <p>What's the request flow in there?  * User asks DNS server for the IP of my site (ie <code>api.mysite.com -&gt; 15.125.23.214</code>). Usually, DNS is provided by third-parties instead of hosting it yourself.  * HTTP requests are sent directly to server (via its IP) from your device  * Server returns HTML pages or JSON payloads, used for rendering.</p> <p>Traffic to web server comes from either a web application or a mobile application:  * Web applications use a combo of server-side languages (ie Java, Python) to handle business logic &amp; storage. Client-side languages (ie HTML, JS) are used for presentation.  * Mobile apps use the HTTP protocol for communication between mobile &amp; the web server. JSON is used for formatting transmitted data. Example payload: <pre><code>{\n  \"id\":12,\n  \"firstName\":\"John\",\n  \"lastName\":\"Smith\",\n  \"address\":{\n     \"streetAddress\":\"21 2nd Street\",\n     \"city\":\"New York\",\n     \"state\":\"NY\",\n     \"postalCode\":10021\n  },\n  \"phoneNumbers\":[\n     \"212 555-1234\",\n     \"646 555-4567\"\n  ]\n}\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter02/#database","title":"Database","text":"<p>As the user base grows, storing everything on a single server is insufficient.  We can separate our database on another server so that it can be scaled independently from the web tier: </p>"},{"location":"booknotes/system-design-interview/chapter02/#which-databases-to-use","title":"Which databases to use?","text":"<p>You can choose either a traditional relational database or a non-relational (NoSQL) one.  * Most popular relational DBs - MySQL, Oracle, PostgreSQL.  * Most popular NoSQL DBs - CouchDB, Neo4J, Cassandra, HBase, DynamoDB</p> <p>Relational databases represent &amp; store data in tables &amp; rows. You can join different tables to represent aggregate objects. NoSQL databases are grouped into four categories - key-value stores, graph stores, column stores &amp; document stores. Join operations are generally not supported.</p> <p>For most use-cases, relational databases are the best option as they've been around the most &amp; have worked quite well historically.</p> <p>If not suitable though, it might be worth exploring NoSQL databases. They might be a better option if:  * Application requires super-low latency.  * Data is unstructured or you don't need any relational data.  * You only need to serialize/deserialize data (JSON, XML, YAML, etc).  * You need to store a massive amount of data.</p>"},{"location":"booknotes/system-design-interview/chapter02/#vertical-scaling-vs-horizontal-scaling","title":"Vertical scaling vs. horizontal scaling","text":"<p>Vertical scaling == scale up. This means adding more power to your servers - CPU, RAM, etc.</p> <p>Horizontal scaling == scale out. Add more servers to your pool of resources.</p> <p>Vertical scaling is great when traffic is low. Simplicity is its main advantage, but it has limitations:  * It has a hard limit. Impossible to add unlimited CPU/RAM to a single server.  * Lack of fail over and redundancy. If server goes down, whole app/website goes down with it.</p> <p>Horizontal scaling is more appropriate for larger applications due to vertical scaling's limitations. Its main disadvantage is that it's harder to get right.</p> <p>In design so far, the server going down (ie due to failure or overload) means the whole application goes down with it.  A good solution for this problem is to use a load balancer.</p>"},{"location":"booknotes/system-design-interview/chapter02/#load-balancer","title":"Load balancer","text":"<p>A load balancer evenly distributes incoming traffic among web servers in a load-balanced set: </p> <p>Clients connect to the public IP of the load balancer. Web servers are unreachable by clients directly. Instead, they have private IPs, which the load balancer has access to.</p> <p>By adding a load balancer, we successfully made our web tier more available and we also added possibility for fail over.</p> <p>How it works?  * If server 1 goes down, all traffic will be routed to server 2. This prevents website from going offline. We'll also add a fresh new server to balance the load.  * If website traffic spikes and two servers are not sufficient to handle traffic, load balancer can handle this gracefully by adding more servers to the pool.</p> <p>Web tier looks lit now. But what about the data tier?</p>"},{"location":"booknotes/system-design-interview/chapter02/#database-replication","title":"Database replication","text":"<p>Database replication can usually be achieved via master/slave replication (side note - nowadays, it's usually referred to as primary/secondary replication).</p> <p>A master database generally only supports writes. Slave databases store copies of the data from the master &amp; only support read operations. This setup works well for most applications as there's usually a higher read to write ratio. Reads can easily be scaled by adding more slave instances. </p> <p>Advantages:  * Better performance - enables more read queries to be processed in parallel.  * Reliability - If one database gets destroyed, data is still preserved.  * High availability - Data is accessible as long as one instance is not offline.</p> <p>So what if one database goes offline?  * If slave database goes offline, read operations are routed to the master/other slaves temporarily.   * If master goes down, a slave instance will be promoted to the new master. A new slave instance will replace the old master. </p> <p>Here's the refined request lifecycle:  * user gets IP address of load balancer from DNS  * user connects to load balancer via IP  * HTTP request is routed to server 1 or server 2  * web server reads user data from a slave database instance or routes data modifications to the master instance.</p> <p>Sweet, let's now improve the load/response time by adding a cache &amp; shifting static content to a CDN.</p>"},{"location":"booknotes/system-design-interview/chapter02/#cache","title":"Cache","text":"<p>Cache is a temporary storage which stores frequently accessed data or results of expensive computations.</p> <p>In our web application, every time a web page is loaded, expensive queries are sent to the database.  We can mitigate this using a cache.</p>"},{"location":"booknotes/system-design-interview/chapter02/#cache-tier","title":"Cache tier","text":"<p>The cache tier is a temporary storage layer, from which results are fetched much more rapidly than from within a database. It can also be scaled independently from the database. </p> <p>The example above is a read-through cache - server checks if data is available in the cache. If not, data is fetched from the database.</p>"},{"location":"booknotes/system-design-interview/chapter02/#considerations-for-using-cache","title":"Considerations for using cache","text":"<ul> <li>When to use it - usually useful when data is read frequently but modified infrequently. Caches usually don't preserve data upon restart so it's not a good persistence layer.</li> <li>Expiration policy - controls whether (and when) cached data expires and is removed from it. Make it too short - DB will be queried frequently. Make it too long - data will become stale.</li> <li>Consistency - How in sync should the data store &amp; cache be? Inconsistency happens if data is changed in DB, but cache is not updated.</li> <li>Mitigating failures - A single cache server could be a single point of failure (SPOF). Consider over-provisioning it with a lot of memory and/or provisioning servers in multiple locations.</li> <li>Eviction policy - What happens when you want to add items to a cache, but it's full? Cache eviction policy controls that. Common policies - LRU, LFU, FIFO.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter02/#content-delivery-network-cdn","title":"Content Delivery Network (CDN)","text":"<p>CDN == network of geographically dispersed servers, used for delivering static content - eg images, HTML, CSS, JS files.</p> <p>Whenever a user requests some static content, the CDN server closest to the user serves it: </p> <p>Here's the request flow:   * User tries fetching an image via URL. URLs are provided by the CDN, eg <code>https://mysite.cloudfront.net/logo.jpg</code>  * If the image is not in the cache, the CDN requests the file from the origin - eg web server, S3 bucket, etc.  * Origin returns the image to the CDN with an optional TTL (time to live) parameter, which controls how long that static resource is to be cached.  * Subsequent users fetch the image from the CDN without any requests reaching the origin as long as it's within the TTL.</p>"},{"location":"booknotes/system-design-interview/chapter02/#considerations-of-using-cdn","title":"Considerations of using CDN","text":"<ul> <li>Cost - CDNs are managed by third-parties for which you pay a fee. Be careful not to store infrequently accessed data in there.</li> <li>Cache expiry - consider appropriate cache expiry. Too short - frequent requests to origin. Too long - data becomes stale.</li> <li>CDN fallback - clients should be able to workaround the CDN provider if there is a temporary outage on their end.</li> <li>Invalidation - can be done via an API call or by passing object versions.</li> </ul> <p>Refined design of our web application: </p>"},{"location":"booknotes/system-design-interview/chapter02/#stateless-web-tier","title":"Stateless web tier","text":"<p>In order to scale our web tier, we need to make it stateless.</p> <p>In order to do that, we can store user session data in persistent data storage such as our relational database or a NoSQL database.</p>"},{"location":"booknotes/system-design-interview/chapter02/#stateful-architecture","title":"Stateful architecture","text":"<p>Stateful servers remember client data across different requests. Stateless servers don't. </p> <p>In the above case, users are coupled to the server which stores their session data. If they make a request to another server, it won't have access to the user's session.</p> <p>This can be solved via sticky sessions, which most load balancers support, but it adds overhead. Adding/removing servers is much more challenging, which limits our options in case of server failures.</p>"},{"location":"booknotes/system-design-interview/chapter02/#stateless-architecture","title":"Stateless architecture","text":"<p>In this scenario, servers don't store any user data themselves.  Instead, they store it in a shared data store, which all servers have access to.</p> <p>This way, HTTP requests from users can be served by any web server.</p> <p>Updated web application architecture: </p> <p>The user session data store could either be a relational database or a NoSQL data store, which is easier to scale for this kind of data. The next step in the app's evolution is supporting multiple data centers.</p>"},{"location":"booknotes/system-design-interview/chapter02/#data-centers","title":"Data centers","text":"<p>In the above example, clients are geo-routed to the nearest data center based on the IP address.</p> <p>In the event of an outage, we route all traffic to the healthy data center: </p> <p>To achieve this multi-datacenter setup, there are several issues we need to address:  * traffic redirection - tooling for correctly directing traffic to the right data center. GeoDNS can be used in this case.  * data synchronization - in case of failover, users from DC1 go to DC2. A challenge is whether their user data is there.  * test and deployment - automated deployment &amp; testing is crucial to keep deployments consistent across DCs.</p> <p>To further scale the system, we need to decouple different system components so they can scale independently.</p>"},{"location":"booknotes/system-design-interview/chapter02/#message-queues","title":"Message queues","text":"<p>Message queues are durable components, which enable asynchronous communication. </p> <p>Basic architecture:  * Producers create messages.  * Consumers/Subscribers subscribe to new messages and consume them.</p> <p>Message queues enable producers to be decoupled from consumers.  If a consumer is down, a producer can still publish a message and the consumer will receive it at a later point.</p> <p>Example use-case in our application - photo processing:  * Web servers publish \"photo processing tasks\" to a message queue  * A variable number of workers (can be scaled up or down) subscribe to the queue and process those tasks. </p>"},{"location":"booknotes/system-design-interview/chapter02/#logging-metrics-automation","title":"Logging, metrics, automation","text":"<p>Once your web application grows beyond a given point, investing in monitoring tooling is critical.  * Logging - error logs can be emitted to a data store, which can later be read by service operators.  * Metrics - collecting various types of metrics helps us collect business insight &amp; monitor the health of the system.  * Automation - investing in continuous integration such as automated build, test, deployment can detect various problems early and also increases developer productivity.</p> <p>Updated system design: </p>"},{"location":"booknotes/system-design-interview/chapter02/#database-scaling","title":"Database scaling","text":"<p>There are two approaches to database scaling - vertical and horizontal.</p>"},{"location":"booknotes/system-design-interview/chapter02/#vertical-scaling","title":"Vertical scaling","text":"<p>Also known as scaling up, it means adding more physical resources to your database nodes - CPU, RAM, HDD, etc. In Amazon RDS, for example, you can get a database node with 24 TB of RAM.</p> <p>This kind of database can handle lots of data - eg stackoverflow in 2013 had 10mil monthly unique visitors \\w a single database node.</p> <p>Vertical scaling has some drawbacks, though:  * There are hardware limits to the amount of resources you can add to a node.  * You still have a single point of failure.  * Overall cost is high - the price of powerful servers is high.</p>"},{"location":"booknotes/system-design-interview/chapter02/#horizontal-scaling","title":"Horizontal scaling","text":"<p>Instead of adding bigger servers, you can add more of them: </p> <p>Sharding is a type of database horizontal scaling which separates large data sets into smaller ones. Each shard shares the same schema, but the actual data is different.</p> <p>One way to shard the database is based on some key, which is equally distributed on all shards using the modulo operator: </p> <p>Here's how the user data looks like in this example: </p> <p>The sharding key (aka partition key) is the most important factor to consider when using sharding. In particular, the key should be chosen in a way that distributes the data as evenly as possible.</p> <p>Although a useful technique, it introduces a lot of complexities in the system:  * Resharding data - you need to do it if a single shard grows too big. This can happen rather quickly if data is distributed unevenly. Consistent hashing helps to avoid moving too much data around.  * Celebrity problem (aka hotspot) - one shard could be accessed much more frequently than others and can lead to server overload. We may have to resort to using separate shards for certain celebrities.  * Join and de-normalization - It is hard to perform join operations across shards. A common workaround is to de-normalize your tables to avoid making joins.</p> <p>Here's how our application architecture looks like after introducing sharding and a NoSQL database for some of the non-relational data: </p>"},{"location":"booknotes/system-design-interview/chapter02/#millions-of-users-and-beyond","title":"Millions of users and beyond","text":"<p>Scaling a system is iterative.</p> <p>What we've learned so far can get us far, but we might need to apply even more sophisticated techniques to scale the application beyond millions of users.</p> <p>The techniques we saw so far can offer a good foundation to start from.</p> <p>Here's a summary:  * Keep web tier stateless  * Build redundancy at every layer  * Cache frequently accessed data  * Support multiple data centers  * Host static assets in CDNs  * Scale your data tier via sharding  * Split your big application into multiple services  * Monitor your system &amp; use automation</p>"},{"location":"booknotes/system-design-interview/chapter03/","title":"Back-of-the-envelope Estimation","text":"<p>You are sometimes asked in a system design interview to estimate performance requirements or system capacity.</p> <p>These are usually done with thought experiments and common performance numbers, according to Jeff Dean (Google Senior Fellow).</p> <p>To do this estimation effectively, there are several mechanisms one should be aware of.</p>"},{"location":"booknotes/system-design-interview/chapter03/#power-of-two","title":"Power of two","text":"<p>Data volumes can become enormous, but calculation boils down to basics.</p> <p>For precise calculations, you need to be aware of the power of two, which corresponds to given data units:  * 2^10 == ~1000 == 1kb  * 2^20 == ~1mil == 1mb  * 2^30 == ~1bil == 1gb  * 2^40 == ~1tril == 1tb  * 2^50 == ~1quad == 1pb</p>"},{"location":"booknotes/system-design-interview/chapter03/#latency-numbers-every-programmer-should-know","title":"Latency numbers every programmer should know","text":"<p>There's a well-known table of the duration of typical computer operations, created by Jeff Dean.</p> <p>These might be a bit outdated due to hardware improvements, but they still give a good relative measure among the operations:  * L1 cache reference == 0.5ns  * Branch mispredict == 5ns  * L2 cache reference == 7ns  * Mutex lock/unlock == 100ns  * Main memory reference == 100ns  * Compress 1kb == 10,000ns == 10us  * Send 2kb over 1gbps network == 20,000ns == 20us  * Read 1mb sequentially from memory == 250us  * Round trip within same DC == 500us  * Disk seek == 10ms  * Read 1mb sequentially from network == 10ms  * Read 1mb sequentially from disk == 30ms  * Send packet CA-&gt;Netherlands-&gt;CA == 150ms</p> <p>A good visualization of the above: </p> <p>Some conclusions from the above numbers:  * Memory is fast, disk is slow  * Avoid disk seeks if possible  * Compression is usually fast  * Compress data before sending over the network if possible  * Data centers round trips are expensive</p>"},{"location":"booknotes/system-design-interview/chapter03/#availability-numbers","title":"Availability numbers","text":"<p>High availability == ability of a system to be continuously operational. In other words, minimizing downtime.</p> <p>Typically, services aim for availability in the range of 99% to 100%.</p> <p>An SLA is a formal agreement between a service provider and a customer.  This formally defines the level of uptime your service needs to support.</p> <p>Cloud providers typically set their uptime at 99.9% or more. Eg AWS EC2 has an SLA of 99.99%</p> <p>Here's a summary of the allowed downtime based on different SLAs: </p>"},{"location":"booknotes/system-design-interview/chapter03/#example-estimate-twitter-qps-and-storage-requirements","title":"Example - estimate Twitter QPS and storage requirements","text":"<p>Assumptions:  * 300mil MAU  * 50% of users use twitter daily  * Users post 2 tweets per day on average  * 10% of tweets contain media  * Data is stored for 5y</p> <p>Estimations:  * Write RPS estimation == 150mil * 2 / 24h / 60m / 60s = 3400-3600 tweets per second, peak=7000 TPS  * Media storage per day == 300mil * 10% == 30mil media storage per day      * If we assume 1mb per media -&gt; 30mil * 1mb = 30tb per day      * in 5y -&gt; 30tb * 365 * 5 == 55pb  * tweet storage estimation:      * 1 tweet = 64byte id + 140 bytes text + 1000 bytes metadata      * 3500 * 60 * 60 * 24 = 302mb per day      * In 5y -&gt; 302 * 365 * 5 == 551gb in 5y</p>"},{"location":"booknotes/system-design-interview/chapter03/#tips","title":"Tips","text":"<p>Back-of-the-envelope Estimations are about the process, not the results. Interviewers might test your problem-solving skills.</p> <p>Some tips to take into consideration:  * Rounding and approximation - don't try to calculate 99987/9.1, round it to 100000/10 instead, which is easier to calculate.  * Write down your assumptions before going forward with estimations  * Label your units explicitly. Write 5mb instead of 5.  * Commonly asked estimations to make - QPS (queries per second), peak QPS, storage, cache, number of servers.</p>"},{"location":"booknotes/system-design-interview/chapter04/","title":"A Framework for System Design Interviews","text":"<p>System design interviews are intimidating.</p> <p>How could you possible design a popular system in an hour when it has taken other people decades?  * It is about the problem solving aspect, not the final solution you came up with.  * Goal is to demonstrate your design skills, defend your choices, respond to feedback constructively.</p> <p>What's the goal of a system design interview?  * It is much more than evaluating a person's technical design skills.  * It is also about evaluating collaboration skills, working under pressure, resolving ambiguity, asking good questions.  * Catching red flags - over-engineering, narrow mindedness, stubborness, etc.</p>"},{"location":"booknotes/system-design-interview/chapter04/#a-4-step-process-for-effective-system-design-interview","title":"A 4-step process for effective system design interview","text":"<p>Although every interview is different, there are some steps to cover in any interview.</p>"},{"location":"booknotes/system-design-interview/chapter04/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - understand the problem and establish design scope","text":"<p>Don't rush to give answers before understanding the problem well enough first. In fact, this might be a red flag - answering without a thorough understanding of requirements.</p> <p>Don't jump to solutions. Ask questions to clarify requirements and assumptions.</p> <p>We have a tendency as engineers to jump ahead and solve a hard problem. But that often leads to designing the wrong system.</p> <p>When you get your answers (or you're asked to make assumptions yourself), write them down on the whiteboard to not forget.</p> <p>What kind of questions to ask? - to understand the exact requirements. Some examples:  * What specific features do we need to design?  * How many users does the product have?  * How fast is the company anticipated to scale up? - in 3, 6, 12 months?  * What is the company's tech stack? What existing services can you leverage to simplify the design?</p> <p>For example, say you have to design a news feed. Here's an example conversation:  * Candidate: Is it mobile, web, both?  * Interviewer: both.  * C: What's the most important features?  * I: Ability to make posts &amp; see friends' news feed.  * C: How is the feed sorted? Just chronologically or based on eg some weight to posts from close friends.  * I: To keep things simple, assume posts are sorted chronologically.  * C: Max friends on a user?  * I: 5000  * C: What's the traffic volume?  * I: 10mil daily active users (DAU)  * C: Is there any media in the feed? - images, video?  * I: It can contain media files, including video &amp; images.</p>"},{"location":"booknotes/system-design-interview/chapter04/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>The goal of this step is to develop a high-level design, while collaborating with the interviewer.  * Come up with a blueprint, ask for feedback. Many good interviewers involve the interviewer.  * Draw boxes on the whiteboard with key components - clients, APIs, web servers, data stores, cache, cdn, message queue, etc.  * Do back-of-the-envelope calculations to evaluate if the blueprint fits the scale constraints. Communicate with interviewer if this type of estimation is required beforehand.</p> <p>You could go through some concrete use-cases, which can help you refine the design. It might help you uncover edge cases.</p> <p>Should we include API schema and database design? Depends on the problem. For larger scale systems, this might be too low level. Communicate with the interviewer to figure this out.</p> <p>Example - designing a news feed.</p> <p>High-level features:  * feed publishing - user creates a post and that post is written in cache/database, after which it gets populated in other news feeds.  * news feed building - news feed is built by aggregating friends' posts in chronological order.</p> <p>Example diagram - feed publishing: </p> <p>Example diagram - news feed building: </p>"},{"location":"booknotes/system-design-interview/chapter04/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>Objectives you should have achieved so far:  * Agreed on overall goals &amp; features  * Sketched out a high-level blueprint of the design  * Obtained feedback about the high-level design  * Received initial ideas on areas you need to focus on in the deep dive</p> <p>At this stage, you should work with interviewer to prioritize which components you should focus on.</p> <p>Example things you might have to focus on:  * High-level design.  * System performance characteristics.  * In most cases, dig into the details of some system component.</p> <p>What details could you dig into? Some examples:  * For URL shortener - the hash function which converts long URLs into small ones  * For Chat system - reducing latency and supporting online/offline status</p> <p>Time management is essential - don't get carried away with details which don't show off your skills. For example, don't get carried away about how facebook's EdgeRank algorithm works as it doesn't demonstrate your design skills.</p> <p>Example design deep dive for feed publishing: </p> <p>Example design deep dive for news feed building: </p>"},{"location":"booknotes/system-design-interview/chapter04/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>At this stage, the interviewer might ask you some follow-up questions or give you the freedom to discuss anything you want.</p> <p>A few directions to follow:  * Identify system bottlenecks and discuss improvements. No design is perfect, there are always things to improve.  * Give a recap of your design.  * Failure modes - server failure, network loss, etc.  * Operational issues - monitoring, alerting, rolling out the system.  * What needs to change to support the next scale curve? Eg 1mil -&gt; 10mil users  * Propose other refinements.</p> <p>Do's:  * Ask for clarification, don't assume your assumption is correct.  * Understand problem requirements.  * There is no right or perfect answer. A solution for a small company is different from one for a larger one.  * Let interviewer know what you're thinking.  * Suggest multiple approaches.  * Agree on blueprint and then design the most critical components first.  * Share ideas with interviewer. They should work with you.</p> <p>Dont's:  * Come unprepared for typical interview questions.  * Jump into a solution without understanding the requirements first.  * Don't go into too much details on a single component at first. Design at a high-level initially.  * Feel free to ask for hints if you get stuck.  * Communicate, don't think silently.</p>"},{"location":"booknotes/system-design-interview/chapter04/#time-allocation-on-each-step","title":"Time allocation on each step","text":"<p>45 minutes are not enough to cover any design into sufficient detail.</p> <p>Here's a rough guide on how much time you should spend on each step:  * Understand problem &amp; establish design scope - 3-10m  * Propose high-level design &amp; get buy-in - 10-15m  * Design deep dive - 10-25m  * Wrap-up - 3-5m</p>"},{"location":"booknotes/system-design-interview/chapter05/","title":"Design a Rate Limiter","text":"<p>The rate limiter's purpose in a distributed system is to control the rate of traffic sent from clients to a given server.</p> <p>It controls the maximum number of requests allowed in a given time period.  If the number of requests exceeds the threshold, the extra requests are dropped by the rate limiter.</p> <p>Examples:  * User can write no more than 2 posts per second.  * You can create 10 accounts max per day from the same IP.  * You can claim rewards max 10 times per week.</p> <p>Almost all APIs have some sort of rate limiting - eg Twitter allows 300 tweets per 3h max.</p> <p>What are the benefits of using a rate limiter?  * Prevents DoS attacks.  * Reduces cost - fewer servers are allocated to lower priority APIs.     Also, you might have a downstream dependency which charges you on a per-call basis, eg making a payment, retrieving health records, etc.  * Prevents servers from getting overloaded.</p>"},{"location":"booknotes/system-design-interview/chapter05/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>There are multiple techniques which you can use to implement a rate limiter, each with its pros and cons.</p> <p>Example Candidate-Interviewer conversation:  * C: What kind of rate limiter are we designing? Client-side or server-side?  * I: Server-side  * C: Does the rate limiter throttle API requests based on IP, user ID or anything else?  * I: The system should be flexible enough to support different throttling rules.  * C: What's the scale of the system? Startup or big company?  * I: It should handle a large number of requests.  * C: Will the system work in a distributed environment?  * I: Yes.  * C: Should it be a separate service or a library?  * I: Up to you.  * C: Do we need to inform throttled users?  * I: Yes.</p> <p>[!IMPORTANT] Summary of requirements: * Accurately limit excess requests * Low latency &amp; as little memory as possible * Distributed rate limiting. * Exception handling * High fault tolerance - if cache server goes down, rate limiter should continue functioning.</p>"},{"location":"booknotes/system-design-interview/chapter05/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>We'll stick with a simple client-server model for simplicity.</p>"},{"location":"booknotes/system-design-interview/chapter05/#where-to-put-the-rate-limiter","title":"Where to put the rate limiter?","text":"<p>It can be implemented either client-side, server-side or as a middleware.</p> <p>Client-side - Unreliable, because client requests can easily be forged by malicious actors. We also might not have control over client implementation.</p> <p>Server-side: </p> <p>As a middleware between client and server: </p> <p>How it works, assuming 2 requests per second are allowed: </p> <p>In cloud microservices, rate limiting is usually implemented in the API Gateway. This service supports rate limiting, ssl termination, authentication, IP whitelisting, serving static content, etc.</p> <p>So where should the rate limiter be implemented? On the server-side or in the API gateway?</p> <p>It depends on several things:  * Current tech stack - if you're implementing it server-side, then your language should be sufficient enough to support it.  * If implemented on the server-side, you have control over the rate limiting algorithm.  * If you already have an API gateway, you might as well add the rate limiter in there.  * Building your own rate limiter takes time. If you don't have sufficient resources, consider using an off-the-shelf third-party solution instead.</p>"},{"location":"booknotes/system-design-interview/chapter05/#algorithms-for-rate-limiting","title":"Algorithms for rate limiting","text":"<p>There are multiple algorithms for rate limiting, each with its pros and cons.</p> <p>Some of the popular algorithms - token bucket, leaking bucket, fixed window counter, sliding window log, sliding window counter.</p>"},{"location":"booknotes/system-design-interview/chapter05/#token-bucket-algorithm","title":"Token bucket algorithm","text":"<p>Simple, well understood and commonly used by popular companies. Amazon and Stripe use it for throttling their APIs. </p> <p>It works as follows:  * There's a container with predefined capacity  * Tokens are periodically put in the bucket  * Once full, no more tokens are added  * Each request consumes a single token  * If no tokens left, request is dropped</p> <p></p> <p>There are two parameters for this algorithm:  * Bucket size - maximum number of tokens allowed in the bucket  * Refill rate - number of tokens put into the bucket every second</p> <p>How many buckets do we need? - depends on the requirements:  * We might need different buckets per API endpoints if we need to support 3 tweets per second, 5 posts per second, etc.  * Different buckets per IP if we want to make IP-based throttling.  * A single global bucket if we want to globally setup 10k requests per second max.</p> <p>[!IMPORTANT] Pros: * Easy to implement * Memory efficient * Throttling gets activated in the event of sustained high traffic only. If bucket size is large, this algorithm supports short bursts in traffic as long as they're not prolonged.</p> <p>Cons: * Parameters might be challenging to tune properly</p>"},{"location":"booknotes/system-design-interview/chapter05/#leaking-bucket-algorithm","title":"Leaking bucket algorithm","text":"<p>Similar to token bucket algorithm, but requests are processed at a fixed rate.</p> <p>How it works:  * When request arrives, system checks if queue is full. If not, request is added to the queue, otherwise, it is dropped.  * Requests are pulled from the queue and processed at regular intervals. </p> <p>Parameters:  * Bucket size - aka the queue size. It specifies how many requests will be held to be processed at fixed intervals.  * Outflow rate - how many requests to be processed at fixed intervals.</p> <p>Shopify uses leaking bucket for rate-limiting.</p> <p>[!IMPORTANT] Pros:  * Memory efficient  * Requests processed at fixed interval. Useful for use-cases where a stable outflow rate is required.</p> <p>Cons:  * A burst of traffic fills up the queue with old requests. Recent requests will be rate limited.  * Parameters might not be easy to tune.</p>"},{"location":"booknotes/system-design-interview/chapter05/#fixed-window-counter-algorithm","title":"Fixed window counter algorithm","text":"<p>How it works:  * Time is divided in fix windows with a counter for each one  * Each request increments the counter  * Once the counter reaches the threshold, subsequent requests in that window are dropped </p> <p>One major problem with this approach is that a burst of traffic in the edges can allow more requests than allowed to pass through: </p> <p>[!IMPORTANT] Pros:  * Memory efficient  * Easy to understand  * Resetting available quota at the end of a unit of time fits certain use cases</p> <p>Cons:  * Spike in traffic could cause more requests than allowed to go through a given time window</p>"},{"location":"booknotes/system-design-interview/chapter05/#sliding-window-log-algorithm","title":"Sliding window log algorithm","text":"<p>To resolve the previous algorithm's issue, we could use a sliding time window instead of a fixed one.</p> <p>How it works:  * Algorithm keeps track of request timestamps. Timestamp data is usually kept in a cache, such as Redis sorted set.  * When a request comes in, remove outdated timestamps.  * Add timestamp of the new request in the log.  * If the log size is same or lower than threshold, request is allowed, otherwise, it is rejected.</p> <p>Note that the 3rd request in this example is rejected, but timestamp is still recorded in the log: </p> <p>[!IMPORTANT] Pros:  * Rate limiting accuracy is very high</p> <p>Cons:  * Memory footprint is very high</p>"},{"location":"booknotes/system-design-interview/chapter05/#sliding-window-counter-algorithm","title":"Sliding window counter algorithm","text":"<p>A hybrid approach which combines the fixed window + sliding window log algorithms. </p> <p>How it works:  * Maintain a counter for each time window. Increment for given time window on each request.  * Derive sliding window counter = <code>prev_window * prev_window_overlap + curr_window * curr_window_overlap</code> (see screenshot above)  * If counter exceeds threshold, request is rejected, otherwise it is accepted.</p> <p>[!IMPORTANT] Pros:  * Smooths out spikes in traffic as rate is based on average rate of previous window  * Memory efficient</p> <p>Cons:  * Not very accurate rate limiting, as it's based on overlaps. But experiments show that only ~0.003% of requests are inaccurately accepted.</p>"},{"location":"booknotes/system-design-interview/chapter05/#high-level-architecture","title":"High-level architecture","text":"<p>We'll use an in-memory cache as it's more efficient than a database for storing the rate limiting buckets - eg Redis. </p> <p>How it works:  * Client sends request to rate limiting middleware  * Rate limiter fetches counter from corresponding bucket &amp; checks if request is to be let through  * If request is let through, it reaches the API servers</p>"},{"location":"booknotes/system-design-interview/chapter05/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>What wasn't answered in the high-level design:  * How are rate limiting rules created?  * How to handle rate-limited requests?</p> <p>Let's check those topics out, along with some other topics.</p>"},{"location":"booknotes/system-design-interview/chapter05/#rate-limiting-rules","title":"Rate limiting rules","text":"<p>Example rate limiting rules, used by Lyft for 5 marketing messages per day: </p> <p>Another example \\w max login attempts in a minute: </p> <p>Rules like these are generally written in config files and saved on disk.</p>"},{"location":"booknotes/system-design-interview/chapter05/#exceeding-the-rate-limit","title":"Exceeding the rate limit","text":"<p>When a request is rate limited, a 429 (too many requests) error code is returned.</p> <p>In some cases, the rate-limited requests can be enqueued for future processing.</p> <p>We could also include some additional HTTP headers to provide additional metadata info to clients: <pre><code>X-Ratelimit-Remaining: The remaining number of allowed requests within the window.\nX-Ratelimit-Limit: It indicates how many calls the client can make per time window.\nX-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again without being throttled.\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter05/#detailed-design","title":"Detailed design","text":"<ul> <li>Rules are stored on disk, workers populate them periodically in an in-memory cache.</li> <li>Rate limiting middleware intercepts client requests.</li> <li>Middleware loads the rules from the cache. It also fetches counters from the redis cache.</li> <li>If request is allowed, it proceeds to API servers. If not, a 429 HTTP status code is returned. Then, request is either dropped or enqueued.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter05/#rate-limiter-in-a-distributed-environment","title":"Rate limiter in a distributed environment","text":"<p>How will we scale the rate limited beyond a single server?</p> <p>There are several challenges to consider:  * Race condition  * Synchronization</p> <p>In case of race conditions, the counter might not be updated correctly when mutated by multiple instances: </p> <p>Locks are a typical way to solve this issue, but they are costly. Alternatively, one could use Lua scripts or Redis sorted sets, which solve the race conditions.</p> <p>If we maintain user information within the application memory, the rate limiter is stateful and we'll need to use sticky sessions to make sure requests from the same user is handled by the same rate limiter instance. </p> <p>To solve this issue, we can use a centralized data store (eg Redis) so that the rate limiter instances are stateless. </p>"},{"location":"booknotes/system-design-interview/chapter05/#performance-optimization","title":"Performance optimization","text":"<p>There are two things we can do as a performance optimization for our rate limiters:  * Multi-data center setup - so that users interact with instances geographically close to them.  * Use eventual consistency as a synchronization model to avoid excessive locking.</p>"},{"location":"booknotes/system-design-interview/chapter05/#monitoring","title":"Monitoring","text":"<p>After the rate limiter is deployed, we'd want to monitor if it's effective.</p> <p>To do so, we need to track:  * If the rate limiting algorithm is effective  * If the rate limiting rules are effective</p> <p>If too many requests are dropped, we might have to tune some of the rules or the algorithm parameters.</p>"},{"location":"booknotes/system-design-interview/chapter05/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We discussed a bunch of rate-limiting algorithms:  * Token bucket - good for supporting traffic bursts.  * Leaking bucket - good for ensuring consistent inbound request flow to downstream services  * Fixed window - good for specific use-cases where you want time divided in explicit windows  * Sliding window log - good when you want high rate-limiting accuracy at the expense of memory footprint.  * Sliding window counter - good when you don't want 100% accuracy with a very low memory footprint.</p> <p>Additional talking points if time permits: - Hard vs. soft rate limiting   - Hard - requests cannot exceed the specified threshold    - Soft - requests can exceed threshold for some limited time     | Feature | Hard Rate Limiting | Soft Rate Limiting |     | :--- | :--- | :--- |     | Behavior at Limit | Immediately rejects new requests | Allows a temporary burst over the limit |     | Strictness | Very strict, no exceptions | Flexible, has a grace period or burst allowance |     | Use Case | Protecting critical resources, enforcing strict API usage tiers | Ensuring good user experience, handling temporary traffic spikes |     | Analogy | A locked gate once full | A waiting area for temporary overflow |</p> <ul> <li>Rate limiting at different layers - L7 (application) vs L3 (network)</li> <li> <p>At its core, Layer 7 rate limiting operates at the application level, affording it a deep understanding of the traffic it's managing. This allows for the creation of sophisticated and context-aware rules. Instead of just looking at the source of the traffic, Layer 7 rate limiting can inspect the content of the requests themselves.</p> </li> <li> <p>Layer 3 rate limiting functions at the network layer, primarily focusing on the IP protocol. Its main concern is the volume of traffic originating from specific IP addresses.        |Feature|Layer 7 (Application)|Layer 3 (Network)|       |:-------|:----------------------|:----------------|        |Focus    |Application-specific requests and content  |IP addresses and traffic volume       |Granularity  |High (User ID, API key, URL path, etc.)    |Low (Primarily IP address)       |Typical Use Cases    |API protection, brute-force prevention, content scraping mitigation    |Volumetric DDoS mitigation       |Resource Intensity|  High|   Low|       |Complexity   |High   |Low|       |Accuracy |High, less prone to false positives    |Lower, can block legitimate users|</p> </li> <li> Client-side measures to avoid being rate limited:<ul> <li> Client-side cache to avoid excessive calls</li> <li> Understand limit and avoid sending too many requests in a small time frame</li> <li> Gracefully handle exceptions due to being rate limited</li> <li> Add sufficient back-off and retry logic</li> </ul> </li> </ul>"},{"location":"booknotes/system-design-interview/chapter06/","title":"Design Consistent Hashing","text":"<p>For horizontal scaling, it is important to distribute requests across servers efficiently.</p> <p>Consistent hashing is a common approach to achieve this.</p>"},{"location":"booknotes/system-design-interview/chapter06/#the-rehashing-problem","title":"The rehashing problem","text":"<p>One way to determine which server a request gets routed to is by applying a simple hash+module formula: <pre><code>serverIndex = hash(key) % N, where N is the number of servers\n</code></pre></p> <p>This makes it so requests are distributed uniformly across all servers. However, whenever new servers are added or removed, the result of the above equation is very different, meaning that a lot of requests will get rerouted across servers.</p> <p>[!NOTE] This causes a lot of cache misses as clients will be connected to new instances which will have to fetch the user data from cache all over again.</p>"},{"location":"booknotes/system-design-interview/chapter06/#consistent-hashing","title":"Consistent hashing","text":"<p>Consistent hashing is a technique which allows only a K/N servers to be remapped whenever N changes, where K is the number of keys.</p> <p>For example, K=100, N=10 -&gt; 10 re-mappings, compared to close to 100 in the normal scenario.</p>"},{"location":"booknotes/system-design-interview/chapter06/#hash-space-and-hash-ring","title":"Hash space and hash ring","text":"<p>A hash ring is a visualization of the possible key space of a given hash algorithm, which is combined into a ring-like structure: </p> <p>[!Note] Murmurhash3 is popular because of its speed on x86 platforms and good distribution for low collision</p>"},{"location":"booknotes/system-design-interview/chapter06/#hash-servers","title":"Hash servers","text":"<p>Using the same hash function for the requests, we map the servers based on server IP or name onto the hash ring: </p>"},{"location":"booknotes/system-design-interview/chapter06/#hash-keys","title":"Hash keys","text":"<p>The hashes of the requests also get resolved somewhere along the hash ring. </p> <p>[!Important] We're not using the modulo operator since hash ring contains all possible value of output of hash function in ring.</p> <p></p>"},{"location":"booknotes/system-design-interview/chapter06/#server-lookup","title":"Server lookup","text":"<p>Now, to determine which server is going to serve each request, we go clockwise from the request's hash until we reach the first server hash: </p>"},{"location":"booknotes/system-design-interview/chapter06/#add-a-server","title":"Add a server","text":"<p>Via this approach, adding a new server causes only one of the requests to get remapped to a new server: </p>"},{"location":"booknotes/system-design-interview/chapter06/#remove-a-server","title":"Remove a server","text":"<p>Likewise, removing a server causes only a single request to get remapped: </p>"},{"location":"booknotes/system-design-interview/chapter06/#two-issues-in-the-basic-approach","title":"Two issues in the basic approach","text":"<p>The first problem with this approach is that hash partitions can be uneven across servers: </p> <p>The second problem derives from the first - it is possible that requests are unevenly distributed across servers: </p>"},{"location":"booknotes/system-design-interview/chapter06/#virtual-nodes","title":"Virtual nodes","text":"<p>To solve this issue, we can map a servers on the hash ring multiple times, creating virtual nodes and assigning multiple partitions to the same server: </p> <p>Now, a request is mapped to the closest virtual node on the hash ring: </p> <p>The more virtual nodes we have, the more evenly distributed the requests will be. This will enable to add server with heterogenous resources to be part of ring.</p> <p>[!NOTE]  An experiment showed that between 100-200 virtual nodes leads to a standard deviation between 5-10% for key distrubution among virtual node.</p>"},{"location":"booknotes/system-design-interview/chapter06/#wrap-up","title":"Wrap up","text":"<p>Benefits of consistent hashing:  * Very low number of keys are distributed in a re-balancing event  * Easy to scale horizontally as data is uniformly distributed  * Hotspot issue is mitigated by uniformly distributing data, related to eg a celebrity, which is often accessed</p> <p>Examples of real-world applications of consistent hashing:  * Amazon's DynamoDB partitioning component  * Data partitioning in Cassandra  * Discord chat application  * Akamai CDN  * Maglev network load balancer</p>"},{"location":"booknotes/system-design-interview/chapter07/","title":"Design a Key-Value Store","text":"<p>Key-value stores are a type of non-relational databases.  * Each unique identifier is stored as a key with a value associated to it.  * Keys must be unique and can be plain text or hashes.  * Performance-wise, short keys work better.</p> <p>Example keys:  * plain-text - \"last_logged_in_at\"  * hashed key - <code>253DDEC4</code></p> <p>We're now about to design a key-value store which supports:  * <code>put(key, value)</code> - insert <code>value</code> associated to <code>key</code>  * <code>get(key)</code> - get <code>value</code> associated to <code>key</code></p>"},{"location":"booknotes/system-design-interview/chapter07/#understand-the-problem-and-establish-design-scope","title":"Understand the problem and establish design scope","text":"<p>[!Important]  - There's always a trade-off to be made between read/write and memory usage. - Another trade-off is between consistency and availability.</p> <p>Here are the characteristics we're striving to achieve:  * Key-value pair size is small - 10kb(what if this is 10MB,100MB ,what would change?)  * We need to be able to store a lot of data.  * High availability - system responds quickly even during failures.  * High scalability - system can be scaled to support large data sets.  * Automatic scaling - addition/deletion of servers should happen automatically based on traffic.  * Tunable consistency.  * Low latency.</p>"},{"location":"booknotes/system-design-interview/chapter07/#single-server-key-value-store","title":"Single server key-value store","text":"<p>Single server key-value stores are easy to develop.</p> <p>We can just maintain an in-memory hash map which stores the key-value pairs.</p> <p>Memory however, can be a bottleneck, as we can't fit everything in-memory. Here are our options to scale:  * Data compression  * Store only frequently used data in-memory. The rest store on disk.</p> <p>Even with these optimizations, a single server can quickly reach its capacity.</p>"},{"location":"booknotes/system-design-interview/chapter07/#distributed-key-value-store","title":"Distributed key-value store","text":"<p>A distributed key-value store consists of a distributed hash table, which distributes keys across many nodes.</p> <p>When developing a distributed data store, we need to factor in the CAP theorem</p>"},{"location":"booknotes/system-design-interview/chapter07/#cap-theorem","title":"CAP Theorem","text":"<p>This theorem states that a data store can't provide more than two of the following guarantees - consistency, availability, partition tolerance;  * Consistency - all clients see the same data at the same time, no matter which node they're connected to.  * Availability - all clients get a response, regardless of which node they connect to.  * Partition tolerance - A network partition means that not all nodes within the cluster can communicate. Partition tolerance means that the system is operational even in such circumstances. </p> <p>A distributed system which supports consistency and availability cannot exist in the real world as network failures are inevitable.</p> <p>Example distributed data store in an ideal situation: </p> <p>In the real world, a network partition can occur which hinders communication with eg node 3: </p> <p>If we favor consistency over availability, all write operations need to be blocked when the above scenario occurs.</p> <p>If we favor availability on the other hand, the system continues accepting reads and writes, risking some clients receiving stale data. When node 3 is back online, it will be re-synced with the latest data.</p> <p>What you choose is something you need to clarify with the interviewer. There are different trade-offs with each option.</p>"},{"location":"booknotes/system-design-interview/chapter07/#system-components","title":"System components","text":"<p>This section goes through the key components needed to build a distributed key-value store.</p>"},{"location":"booknotes/system-design-interview/chapter07/#data-partition","title":"Data partition","text":"<p>For a large enough data set, it is infeasible to maintain it on a single server. Hence, we can split the data into smaller partitions and distribute them across multiple nodes.</p> <p>The challenge then, is to distribute data evenly and minimize data movement when the cluster is resized.</p> <p>Both these problems can be addressed using consistent hashing (discussed in previous chapter):  * Servers are put on a hash ring  * Keys are hashed and put on the closest server in clockwise direction </p> <p>This has the following advantages:  * Automatic scaling - servers can be added/removed at will with minimal impact on key location  * Heterogeneity - servers with higher capacity can be allocated with more virtual nodes</p>"},{"location":"booknotes/system-design-interview/chapter07/#data-replication","title":"Data replication","text":"<p>To achieve high availability &amp; reliability, data needs to be replicated on multiple nodes.</p> <p>We can achieve that by allocating a key to multiple nodes on the hash ring: </p> <p>[!Note]  One caveat to keep in mind that your key might get allocated to virtual nodes mapped to the same physical node.  To avoid this, we can only choose unique physical nodes when replicating data.</p> <p>An additional reliability measure is to replicate data across multiple data centers as nodes in the same data centers can fail at the same time.</p>"},{"location":"booknotes/system-design-interview/chapter07/#consistency","title":"Consistency","text":"<p>Since data is replicated, it must be synchronized.</p> <p>Quorum consensus can guarantee consistency for both reads and writes:  * N - number of replicas  * W - write quorum. Write operations must be acknowledged by W nodes.  * R - read quorum. Read operations must be acknowledged by R nodes. </p> <p>The configuration of W and R is a trade-off between latency and consistency.  * W = 1, R = 1 -&gt; low latency, eventual consistency  * W + R &gt; N -&gt; strong consistency, high latency</p> <p>Other configurations:  * R = 1, W = N -&gt; strong consistency, fast reads, slow writes  * R = N, W = 1 -&gt; strong consistency, fast writes, slow reads</p>"},{"location":"booknotes/system-design-interview/chapter07/#consistency-models","title":"Consistency models","text":"<p>There are multiple flavors of consistency we can tune our key-value store for:  * Strong consistency - read operations return a value corresponding to most up-to-date data. Clients never see stale data.  * Weak consistency - read operations might not see the most up-to-date data.  * Eventual consistency - read operations might not see most up-to-date data, but with time, all keys will converge to the latest state.</p> <p>Strong consistency usually means a node not accepting reads/writes until all nodes have acknowledged a write. This is not ideal for highly available systems as it can block new operations.</p> <p>Eventual consistency (supported by Cassandra and DynamoDB) is preferable for our key-value store. This allows concurrent writes to enter the system and clients need to reconcile the data mismatches.</p>"},{"location":"booknotes/system-design-interview/chapter07/#inconsistency-resolution-versioning","title":"Inconsistency resolution: versioning","text":"<p>Replication provides high availability, but it leads to data inconsistencies across replicas.</p> <p>Example inconsistency: </p> <p>This kind of inconsistency can be resolved using a versioning system using a vector clock. A vector clock is a [server, version] pair, associated with a data item. Each time a data item is changed in a server, it's associated vector clock changes to [server_id, curr_version+1].</p> <p>Example inconsistency resolution:   * Client writes D1, handled by Sx, which writes version [Sx, 1]  * Another client reads D1, updates it and Sx increments version to [Sx, 2]  * Client writes D3 based on D2 in Sy -&gt; D3([Sx, 2][Sy, 1]).  * Simultaneously, another one writes D4 in Sz -&gt; D4([Sx, 2][Sz, 1])  * A client reads D3 and D4 and detects a conflict. It makes a resolution and adds the updated version in Sx -&gt; D5([Sx, 3][Sy, 1][Sz, 1])</p> <p>A conflict is detected by checking whether a version is an ancestor of another one. That can be done by verifying that all version stamps are less than or equal to the other one.  * [s0, 1][s1, 1] is an ancestor of [s0, 1][s1, 2] -&gt; no conflict.  * [s0, 1] is not an ancestor of [s1, 1] -&gt; conflict needs to be resolved.</p> <p>This conflict resolution technique comes with trade-offs:  * Client complexity is increased as clients need to resolve conflicts.  * Vector clocks can grow rapidly, increasing the memory footprint of each key-value pair.</p>"},{"location":"booknotes/system-design-interview/chapter07/#handling-failures","title":"Handling failures","text":"<p>At a large enough scale, failures are inevitable. It is important to determine your error detection &amp; resolution strategies.</p>"},{"location":"booknotes/system-design-interview/chapter07/#failure-detection","title":"Failure detection","text":"<p>In a distributed system, it is insufficient to conclude that a server is down just because you can't reach it. You need at least another source of information.</p> <p>One approach to do that is to use all-to-all multi-casting. This, however, is inefficient when there are many servers in the system. </p> <p>A better solution is to use a decentralized failure detection mechanism, such as a gossip protocol:  * Each node maintains a node membership list with member IDs and heartbeat counters.  * Each node periodically increments its heartbeat counter  * Each node periodically sends heartbeats to a random set of other nodes, which propagate it onwards.  * Once a node receives a heartbeat, its membership list is updated.  * If a heartbeat is not received after a given threshold, the member is marked offline.</p> <p></p> <p>In the above scenario, s0 detects that s2 is down as no heartbeat is received for a long time.  It propagates that information to other nodes which also verify that the heartbeat hasn't been updated. Hence, s2 is marked offline.</p>"},{"location":"booknotes/system-design-interview/chapter07/#handling-temporary-failures","title":"Handling temporary failures","text":"<p>A nice little trick to improve availability in the event of failures is hinted handoff.</p> <p>What it means is that if a server if temporarily offline, you can promote another healthy service in its place to process data temporarily. After the server is back online, the data &amp; control is handed back to it.</p>"},{"location":"booknotes/system-design-interview/chapter07/#handling-permanent-failures","title":"Handling permanent failures","text":"<p>Hinted handoff is used when the failure is intermittent.</p> <p>If a replica is permanently unavailable, we implement an anti-entropy protocol to keep the replicas in-sync.</p> <p>This is achieved by leveraging merkle trees in order to reduce the amount of data transmitted and compared to a minimum.</p> <p>The merkle tree works by building a tree of hashes where leaf nodes are buckets of key-value pairs.</p> <p>If any of the buckets across two replicas is different, then the merkle tree's hashes will be different all the way to the root: </p> <p>Using merkle trees, two replicas will compare only as much data as is different between them, instead of comparing the entire data set.</p>"},{"location":"booknotes/system-design-interview/chapter07/#handling-data-center-outage","title":"Handling data center outage","text":"<p>A data center outage could happen due to a natural disaster or serious hardware failure.</p> <p>To ensure resiliency, make sure your data is replicated across multiple data centers.</p>"},{"location":"booknotes/system-design-interview/chapter07/#system-architecture-diagram","title":"System architecture diagram","text":"<p>Main features:  * Clients communicate with the key-value store through a simple API  * A coordinator is a proxy between the clients and the key-value store  * Nodes are distributed on the ring using consistent hashing  * System is decentralized, hence adding and removing nodes is supported and can be automated  * Data is replicated at multiple nodes  * There is no single point of failure</p> <p>Some of the tasks each node is responsible for: </p>"},{"location":"booknotes/system-design-interview/chapter07/#write-path","title":"Write path","text":"<p>  * Write requests are persisted in a commit log  * Data is saved in the memory cache  * When memory cache is full or reaches a given threshold, data is flushed to an SSTable on disk</p> <p>SSTable == Sorted String Table. Holds a sorted list of key-value pairs.</p>"},{"location":"booknotes/system-design-interview/chapter07/#read-path","title":"Read path","text":"<p>Read path when data is in memory: </p> <p>Read path when data is not in memory:   * If data is in memory, fetch it from there. Otherwise, find it in the SSTable.  * A bloom filter is used for efficient lookup in the SSTable.  * The SSTables returns the resulting data, which is returned to the client</p>"},{"location":"booknotes/system-design-interview/chapter07/#summary","title":"Summary","text":"<p>We covered a lot of concepts and techniques, here's a summary: | Goal/Problems               | Technique                                             | |-----------------------------|-------------------------------------------------------| | Ability to store big data   | Use consistent hashing to spread load across servers  | | High availability reads     | Data replication Multi-datacenter setup               | | Highly available writes     | Versioning and conflict resolution with vector clocks | | Dataset partition           | Consistent Hashing                                    | | Incremental scalability     | Consistent Hashing                                    | | Heterogeneity               | Consistent Hashing                                    | | Tunable consistency         | Quorum consensus                                      | | Handling temporary failures | Sloppy quorum and hinted handoff                      | | Handling permanent failures | Merkle tree                                           | | Handling data center outage | Cross-datacenter replication                          |</p>"},{"location":"booknotes/system-design-interview/chapter08/","title":"Design a Unique ID Generator in Distributed Systems","text":"<p>We need to design a unique ID generator, compatible with distributed systems.</p> <p>A primary key with auto_increment won't work here, because generating IDs across multiple database servers has high latency.</p>"},{"location":"booknotes/system-design-interview/chapter08/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What characteristics should the unique IDs have?</li> <li>I: They should be unique and sortable.</li> <li>C: For each record, does the ID increment by 1?</li> <li>I: IDs increment by time, but not necessarily by 1.</li> <li>C: Do IDs contain only numerical values?</li> <li>I: Yes</li> <li>C: What is the ID length requirement?</li> <li>I: 64 bits</li> <li>C: What's the system scale?</li> <li>I: We should be able to generate 10,000 IDs per second</li> </ul>"},{"location":"booknotes/system-design-interview/chapter08/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>Here's the options we'll consider:  * Multi-master replication  * Universally-unique IDs (UUIDs)  * Ticket server  * Twitter snowflake approach</p>"},{"location":"booknotes/system-design-interview/chapter08/#multi-master-replication","title":"Multi-master replication","text":"<p>This uses the database's auto_increment feature, but instead of increasing by 1, we increase by K where K = number of servers.</p> <p>This solves the scalability issues as id generation is confined within a single server, but it introduces other challenges:  * Hard to scale \\w multiple data centers  * IDs do not go up in time across servers  * Adding/removing servers breaks this mechanism</p>"},{"location":"booknotes/system-design-interview/chapter08/#uuid","title":"UUID","text":"<p>A UUID is a 128-byte unique ID.</p> <p>The probability of UUID collision across the whole world is very little.</p> <p>Example UUID - <code>09c93e62-50b4-468d-bf8a-c07e1040bfb2</code>.</p> <p>Pros:  * UUIDs can be generated independently across servers without any synchronization or coordination.  * Easy to scale.</p> <p>Cons:  * IDs are 128 bytes, which doesn't fit our requirement  * IDs do not increase with time  * IDs can be non-numeric</p>"},{"location":"booknotes/system-design-interview/chapter08/#ticket-server","title":"Ticket server","text":"<p>A ticket server is a centralized server for generating unique primary keys across multiple services: </p> <p>Pros:  * Numeric IDs  * Easy to implement &amp; works for small &amp; medium applications</p> <p>Cons:  * Single point of failure.  * Additional latency due to network call.</p>"},{"location":"booknotes/system-design-interview/chapter08/#twitter-snowflake-approach","title":"Twitter snowflake approach","text":"<p>Twitter's snowflake meets our design requirements because it is sortable by time, 64-bits and can be generated independently in each server. </p> <p>Breakdown of the different sections:  * Sign bit - always 0. Reserved for future use.  * Timestamp - 41 bits. Milliseconds since epoch (or since custom epoch). Allows 69 years max.  * Datacenter ID - 5 bits, which enables 32 data centers max.  * Machine ID - 5 bits, which enables 32 machines per data center.  * Sequence number - For every generated ID, the sequence number is incremented. Reset to 0 on every millisecond.</p>"},{"location":"booknotes/system-design-interview/chapter08/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>We'll use twitter's snowflake algorithm as it fits our needs best.</p> <p>Datacenter ID and machine ID are chosen at startup time. The rest is determined at runtime.</p> <pre><code>func (n *singleWorker) NextID() (id SingleWorkerID, err error) {\n\n    n.mu.Lock()\n\n    now := time.Now().UnixNano() / 1000000\n\n    if now &lt; n.lastTimeStamp {\n        return SingleWorkerID(0), fmt.Errorf(\"Clock moved backwards. Refusing to generate id for %d milliseconds\", n.lastTimeStamp-now)\n    }\n\n    if n.lastTimeStamp == now {\n        n.sequence = (n.sequence + 1) &amp; sequenceMask\n\n        if n.sequence == 0 {\n            now = snowflake.TilNextMillis(n.lastTimeStamp)\n        }\n    } else {\n        n.sequence = 0\n    }\n\n    n.lastTimeStamp = now\n\n    id = SingleWorkerID((now-snowflake.TW_EPOCH)&lt;&lt;timestampLeftShift |\n        (n.nodeID &lt;&lt; nodeIdShift) |\n        n.sequence)\n\n    n.mu.Unlock()\n    return\n}\n</code></pre>"},{"location":"booknotes/system-design-interview/chapter08/#step-4-wrap-up","title":"Step 4 - wrap up","text":"<p>We explored multiple ways to generate unique IDs and settled on snowflake eventually as it serves our purpose best.</p>"},{"location":"booknotes/system-design-interview/chapter08/#additional-talking-points","title":"Additional talking points:","text":""},{"location":"booknotes/system-design-interview/chapter08/#clock-synchronization-network-time-protocol-can-be-used-to-resolve-clock-inconsistencies-across-different-machinescpu-cores","title":"Clock synchronization - network time protocol can be used to resolve clock inconsistencies across different machines/CPU cores.","text":"<ul> <li>NTP is absolutely critical for the timestamp portion. By using NTP, every machine in the distributed system that generates Snowflake IDs agrees on the current time. This synchronization ensures:</li> <li>Global Monotonicity: IDs generated later will always have a greater timestamp value than IDs generated earlier, regardless of which machine created them. This allows you to sort records by their Snowflake ID and have them be chronologically ordered.</li> <li>Collision Avoidance: Without synchronized time, if a machine's clock were to go backward (e.g., after a reboot and incorrect sync), it could start generating timestamps it has already used, leading to duplicate IDs. NTP prevents this clock regression.</li> <li>In short, NTP acts as the conductor of an orchestra, ensuring every machine's clock \"plays\" in perfect time. This allows the timestamp component of the Snowflake ID to be a reliable and globally consistent measure of time, which is the foundation of the entire system.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter08/#section-length-tuning-we-could-sacrifice-some-sequence-number-bits-for-more-timestamp-bits-in-case-of-low-concurrency-and-long-term-applications","title":"Section length tuning - we could sacrifice some sequence number bits for more timestamp bits in case of low concurrency and long-term applications.","text":"<ul> <li>The 41 bits in a standard Snowflake ID can represent 2^41 milliseconds. This is a massive number, but it still has a limit: about 69 years.</li> <li>For many applications, like a social media post ID, a 69-year lifespan is perfectly fine. But for certain types of applications, it's a critical flaw.</li> <li>Applications That Outlive the Standard Timestamp</li> <li>Applications that deal with long-term, foundational data need a much longer horizon.</li> <li>Government and Civic Records: Think of land ownership deeds, birth certificates, or national archive records. These must remain valid and unique for hundreds of years.</li> <li>Financial Ledgers: Systems tracking long-term assets, mortgages, or foundational company shares need to operate far beyond a 70-year window.</li> <li>Core Scientific Data: Datasets from long-running experiments or astronomical observations need identifiers that won't clash for generations.</li> <li>For these systems, an ID generator that will stop working correctly after 69 years is not a viable option. When the timestamp \"rolls over,\" the system can no longer generate new IDs that are guaranteed to be greater than old ones, breaking the chronological sorting guarantee.</li> <li>How Adding Bits Solves the Problem<ul> <li>Adding bits to the timestamp exponentially increases its lifespan. 41 bits: ~2.2\u00d710^12ms = 69 years</li> <li>42 bits: ~4.4\u00d710^12 ms = 138 years (doubled the lifespan)</li> <li>43 bits: ~8.8\u00d710^12 ms = 276 years (quadrupled the lifespan)</li> <li>By adding just a couple of bits (often at the expense of sequence bits, as discussed), a designer can easily future-proof the ID generator, ensuring the application remains stable and reliable for centuries to come. It's a strategic trade-off that prioritizes the system's longevity over its ability to handle momentary, extreme traffic spikes.  ### High availability - ID generators are a critical component and must be highly available.</li> </ul> </li> <li>Decentralization: The Core Principle</li> <li>Instead of having one central service that hands out IDs (which would be a major bottleneck and a single point of failure), a Snowflake-style approach makes every machine its own ID generator.</li> <li>Each application server, or \"worker,\" that needs to create an ID can generate it locally without talking to any other service. This is the foundation of its high availability.</li> <li>How it works: Each machine is assigned a unique Machine ID (or Worker ID) during startup. This ID is embedded into every unique ID it generates. Because each machine has its own distinct ID, there's no risk of two different machines generating the same ID, even at the exact same millisecond.</li> <li>Fault Isolation and Resilience</li> <li>This decentralized model provides excellent fault tolerance.</li> <li>If one machine fails: The other machines are completely unaffected. They continue to generate their own unique IDs without interruption. The system as a whole remains available and can still create new records. The only impact is that one machine is temporarily out of the pool.</li> <li>No network dependency for generation: Once a machine has its Machine ID, it doesn't need to communicate with a central coordinator to create an ID. This makes it resilient to network partitions and latency issues that would cripple a centralized ID generator.</li> <li>The Role of a Coordinator (and Its Limits)</li> <li>While the ID generation is decentralized, the assignment of the unique Machine ID often requires a lightweight coordination service, like Apache ZooKeeper.</li> <li>On Startup: A worker machine will register with ZooKeeper to claim a unique Machine ID from a predefined pool (e.g., from 0 to 1023).</li> <li>During Operation: The machine operates independently. It does not need to talk to ZooKeeper again unless it reboots.</li> <li>This means ZooKeeper is only a dependency during the brief startup phase, not during the critical path of ID generation. Even if ZooKeeper goes down, all currently running machines will continue to function perfectly, ensuring high availability for the core service.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter09/","title":"Design a URL Shortener","text":"<p>We're tackling a classical system design problem - designing a URL shortening service like tinyurl.</p>"},{"location":"booknotes/system-design-interview/chapter09/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: Can you give an example of how a URL shortening service works?</li> <li>I: Given URL <code>https://www.systeminterview.com/q=chatsystem&amp;c=loggedin&amp;v=v3&amp;l=long</code> and alias <code>https://tinyurl.com/y7keocwj</code>. You open the alias and get to the original URL.</li> <li>C: What is the traffic volume?</li> <li>I: 100 million URLs are generated per day.</li> <li>C: How long is the shortened URL?</li> <li>I: As short as possible</li> <li>C: What characters are allowed?</li> <li>I: numbers and letters</li> <li>C: Can shortened URLs be updated or deleted?</li> <li>I: For simplicity, let's assume they can't.</li> </ul> <p>Other functional requirements - high availability, scalability, fault tolerance.</p>"},{"location":"booknotes/system-design-interview/chapter09/#back-of-the-envelope-calculation","title":"Back of the envelope calculation","text":"<ul> <li>100 mil URLs per day -&gt; ~1200 URLs per second.</li> <li>Assuming read-to-write ratio of 10:1 -&gt; 12000 reads per second.</li> <li>Assuming URL shortener will run for 10 years, we need to support 365bil records.</li> <li>Average URL length is 100 characters</li> <li>Storage requirements for 10y - 36.5 TB</li> </ul>"},{"location":"booknotes/system-design-interview/chapter09/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":""},{"location":"booknotes/system-design-interview/chapter09/#api-endpoints","title":"API Endpoints","text":"<p>We'll make a REST API.</p> <p>A URL shortening service needs two endpoints:  * <code>POST api/v1/data/shorten</code> - accepts long url and returns a short one.  * <code>GET api/v1/shortURL</code> - return long URL for HTTP redirection.</p>"},{"location":"booknotes/system-design-interview/chapter09/#url-redirecting","title":"URL Redirecting","text":"<p>How it works: </p> <p>What's the difference between 301 and 302 statuses?  * 301 (Permanently moved) - indicates that the URL permanently points to the new URL. This instructs the browser to bypass the tinyurl service on subsequent calls.  * 302 (Temporarily moved) - indicates that the URL is temporarily moved to the new URL. Browser will not bypass the tinyurl service on future calls.</p> <p>Choose 301 if you want to avoid extra server load. Choose 302 if tracking analytics is important.</p> <p>Easiest way to implement the URL redirection is to store the <code>&lt;shortURL, longURL&gt;</code> pair in an in-memory hash-table.</p>"},{"location":"booknotes/system-design-interview/chapter09/#url-shortening","title":"URL Shortening","text":"<p>To support the URL shortening, we need to find a suitable hash function.</p> <p>It needs to support hashing long URL to shortURL and mapping them back.</p> <p>Details are discussed in the detailed design.</p>"},{"location":"booknotes/system-design-interview/chapter09/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>We'll explore the data model, hash function, URL shortening and redirection.</p>"},{"location":"booknotes/system-design-interview/chapter09/#data-model","title":"Data model","text":"<p>In the simplified version, we're storing the URLs in a hash table. That is problematic as we'll run out of memory and also, in-memory doesn't persist across server reboot.</p> <p>That's why we can use a simple relational table instead: </p>"},{"location":"booknotes/system-design-interview/chapter09/#hash-function","title":"Hash function","text":"<p>The hash value consists of characters <code>[0-9a-zA-Z]</code>, which gives a max of 62 characters.</p> <p>To figure out the smallest hash value we can use, we need to calculate n in <code>62^n &gt;= 365bil</code> -&gt; this results in <code>n=7</code>, which can support ~3.5 trillion URLs.</p> <p>For the hash function itself, we can either use <code>base62 conversion</code> or <code>hash + collision detection</code>.</p> <p>In the latter case, we can use something like MD-5 or SHA256, but only taking the first 7 characters. To resolve collisions, we can reiterate \\w an some padding to input string until there is no collision: </p> <p>The problem with this method is that we have to query the database to detect collision. Bloom filters could help in this case.</p> <p>Alternatively, we can use base62 conversion, which can convert an arbitrary ID into a string consisting of the 62 characters we need to support.</p> <p>Comparison between the two approaches: | Hash + collision resolution                                                                   | Base 62 conversion                                                                                                                   | |-----------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------| | Fixed short URL length.                                                                       | Short URL length is not fixed. It goes up with the ID.                                                                               | | Does not need a unique ID generator.                                                          | This option depends on a unique ID generator.                                                                                        | | Collision is possible and needs to be resolved.                                               | Collision is not possible because ID is unique.                                                                                      | | It\u2019s not possible to figure out the next available short URL because it doesn\u2019t depend on ID. | It is easy to figure out what is the next available short URL if ID increments by 1 for a new entry. This can be a security concern. |</p>"},{"location":"booknotes/system-design-interview/chapter09/#url-shortening-deep-dive","title":"URL shortening deep dive","text":"<p>To keep our service simple, we'll use base62 encoding for the URL shortening.</p> <p>Here's the whole workflow: </p> <p>To ensure our ID generator works in a distributed environment, we can use Twitter's snowflake algorithm.</p>"},{"location":"booknotes/system-design-interview/chapter09/#url-redirection-deep-dive","title":"URL redirection deep dive","text":"<p>We've introduced a cache as there are more reads than writes, in order to improve read performance:   * User clicks short URL  * Load balancer forwards the request to one of the service instances  * If shortURL is in cache, return the longURL directly  * Otherwise, fetch the longURL from the database and store in cache. If not found, then the short URL doesn't exist</p>"},{"location":"booknotes/system-design-interview/chapter09/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We discussed:  * API design  * data model  * hash function  * URL shortening  * URL redirecting</p> <p>Additional talking points:  * Rate limiter - We can introduce a rate limiter to protect us against malicious actors, trying to make too many URL shortening requests.  * Web server scaling - We can easily scale the web tier by introducing more service instances as it's stateless.  * Database scaling - Replication and sharding are a common approach to scale the data layer.  * Analytics - Integrating analytics tracking in our URL shortener service can reap some business insights for clients such as \"how many users clicked the link\".  * Availability, consistency, reliability - At the core of every distributed systems. We'd leverage concepts already discussed in Chapter 02.</p>"},{"location":"booknotes/system-design-interview/chapter10/","title":"Design a Web Crawler","text":"<p>We'll focus next on designing a web crawler - a classical system design problem.</p> <p>Web crawlers (aka robots) are used to discover new or updated content on the web, such as articles, videos, PDFs, etc. </p> <p>Use-cases:  * Search engine indexing - for creating a local index of a search engine, eg Google's Googlebot.  * Web archiving - collect data from the web and preserve it for future uses.   * Web mining - it can also be used for data mining. Eg finding important insights such as shareholder meetings for trading firms.  * Web monitoring - monitor the internet for copyright infringements or eg company internal information leaks.</p> <p>The complexities of building a web crawler depend on our target scale. It can be very simple (eg a student project) or a multi-year project, maintained by a dedicated team.</p>"},{"location":"booknotes/system-design-interview/chapter10/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>How it works at a high-level:  * Given a set of URLs, download all the pages these URLs point to  * Extract URLs from the web pages  * Add the new URLs to the list of URLs to be traversed</p> <p>A real web crawler is much more complicated, but this is what it does in a nutshell.</p> <p>You'll need to clarify what kind of features your interviewer would like you to support exactly:  * C: What's the main purpose of the web crawler? Search engine indexing, data mining, something else?  * I: Search engine indexing  * C: How many web pages does it collect per month  * I: 1 billion  * C: What content types are included? HTML, PDF, images?  * I: HTML only  * C: Should we consider newly added/edited content?  * I: Yes  * C: Do we need to persist the crawled web pages?  * I: Yes, for 5 years  * C: What do we do with pages with duplicate content  * I: Ignore them</p> <p>This is an example conversation. It is important to go through this even if the project is simple. Your assumptions and the ones of your interviewer could differ.</p> <p>Other characteristics of a good web crawler:  * Scalable - it should be extremely efficient  * Robust - handle edge-cases such as bad HTML, infinite loops, server crashes, etc  * Polite - not make too many requests to a server within a short time interval  * Extensibility - it should be easy to add support for new types of content, eg images in the future</p>"},{"location":"booknotes/system-design-interview/chapter10/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<p>Given 1 billion pages per month -&gt; ~400 pages per second Peak QPS = 800 pages per second</p> <p>Given average web page size is 500kb -&gt; 500 TB per month -&gt; 30 PB for 5y.</p>"},{"location":"booknotes/system-design-interview/chapter10/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>What's going on in there?  * Seed URLs - These URLs are the starting point for crawlers. It's important to pick the seed URLs well in order to traverse the web appropriately.  * URL Frontier - This component stores the URLs to be downloaded in a FIFO queue.  * HTML Downloader - component downloads HTML pages from URLs in the frontier.  * DNS Resolver - Resolve the IP for a given URL's domain.  * Content Parser - validate that the web page is ok. You don't want to store &amp; process malformed web pages.  * Content Seen? - component eliminates pages which have already been processed. This compares the content, not the URLs. Efficient way to do it is by comparing web page hashes.  * Content Storage - Storage system for HTML documents. Most content is stored on disk \\w most popular ones in memory for fast retrieval.  * URL Extractor - component which extracts links from an HTML document.  * URL Filter - exclude URLs which are invalid, unsupported content types, blacklisted, etc.  * URL Seen? - component which keeps track of visited URLs to avoid traversing them again. Bloom filters are an efficient way to implement this component.  * URL Storage - Store already visited URLs.</p> <p>Those are all the component, but what about the workflow?   1. Add Seed URLs to URL Frontier  2. HTML Downloader fetches a list of URLs from frontier  3. Match URLs to IP Addresses via the DNS resolver  4. Parse HTML pages and discard if malformed  5. Once validated, content is passed to \"Content Seen?\"  6. Check if HTML page is already in storage. If yes - discard. If no - process.  7. Extract links from HTML page  8. Pass extracted links to URL Filter  9. Pass filtered links to \"URL Seen?\" component  10. If URL is in storage - discard. Otherwise - process.  11. If URL is not processed before, it is added to URL Frontier</p>"},{"location":"booknotes/system-design-interview/chapter10/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's now explore some of the most important mechanisms in the web crawler:  * DFS vs. BFS  * URL frontier  * HTML Downloader  * Robustness  * Extensibility  * Detect and avoid problematic content</p>"},{"location":"booknotes/system-design-interview/chapter10/#dfs-vs-bfs","title":"DFS vs. BFS","text":"<p>The web is a directed graph, where the links in a web page are the edges to other pages (the nodes).</p> <p>Two common approaches for traversing this data structure are DFS and BFS.  DFS is usually not a good choice as the traversal depth can get very big</p> <p>BFS is typically preferable. It uses a FIFO queue which traverses URLs in order of encountering them.</p> <p>There are two problems with traditional BFS though:  * Most links in a page are backlinks to the same domain, eg wikipedia.com/page -&gt; wikipedia.com.     When a crawler attempts to visit those links, the server is flooded with requests which is \"impolite\".  * Standard BFS doesn't take URL priority into account</p>"},{"location":"booknotes/system-design-interview/chapter10/#url-frontier","title":"URL Frontier","text":"<p>The URL Frontier helps address these problems. It prioritizes URLs and ensures politeness.</p>"},{"location":"booknotes/system-design-interview/chapter10/#politeness","title":"Politeness","text":"<p>A web crawler should avoid sending too many requests to the same host in a short time frame as it can cause excessive traffic to traversed website.</p> <p>Politeness is implemented by maintaining a download queue per hostname \\w a delay between element processing:   * The queue router ensures that each queue contains URLs from the same host.  * Mapping table - maps each host to a queue.   * FIFO queues maintain URLs belonging to the same host.  * Queue selector - Each worker thread is mapped to a FIFO queue and only downloads URLs from that queue. Queue selector chooses which worker processes which queue.  * Worker thread 1 to N - Worker threads download web pages one by one from the same host. Delay can be added between two download tasks.</p>"},{"location":"booknotes/system-design-interview/chapter10/#priority","title":"Priority","text":"<p>We prioritize URLs by usefulness, which can be determined based on PageRank web traffic, update frequency, etc.</p> <p>Prioritizer manages the priority for each URL:   * Takes URLs as input and calculates priority  * Queues have different priorities. URLs are put into respective queue based on its priority.  * Queue selector - randomly choose queue to select from with bias towards high-priority ones.</p>"},{"location":"booknotes/system-design-interview/chapter10/#freshness","title":"Freshness","text":"<p>Web pages are constantly updated. We need to periodically recrawl updated content.</p> <p>We can choose to recrawl based on the web page's update history. We can also prioritize recrawling important pages which are updated first.</p>"},{"location":"booknotes/system-design-interview/chapter10/#storage-for-url-frontier","title":"Storage for URL Frontier","text":"<p>In the real world, the URLs in the frontier can be millions. Putting everything in-memory is infeasible. But putting it on disk is also slow and can cause a bottleneck for our crawling logic.</p> <p>We've adopted a hybrid approach where most URLs are on disk, but we maintain a buffer in-memory with URLs which are currently processed. We periodically flush that to disk.</p>"},{"location":"booknotes/system-design-interview/chapter10/#html-downloader","title":"HTML Downloader","text":"<p>This component downloads HTML pages from the web using the HTTP protocol.</p> <p>One protocol we need to also bear in mind is the Robots Exclusion Protocol.</p> <p>It is a <code>robots.txt</code> file, available on websites, which website owners use to communicate with web crawlers. It is used to communicate which web pages are ok to be traversed and which ones should be skipped.</p> <p>It looks like this: <pre><code>User-agent: Googlebot\nDisallow: /creatorhub/\\*\nDisallow: /rss/people/\\*/reviews\nDisallow: /gp/pdp/rss/\\*/reviews\nDisallow: /gp/cdp/member-reviews/\nDisallow: /gp/aw/cr/\n</code></pre></p> <p>We need to respect that file and avoid crawling the pages specified in there. We can cache it to avoid downloading it all the time.</p>"},{"location":"booknotes/system-design-interview/chapter10/#performance-optimization","title":"Performance optimization","text":"<p>Some performance optimizations we can consider for the HTML downloader.  * Distributed crawl - We can parallelize crawl jobs to multiple machines which run multiple threads to crawl more efficiently.   * Cache DNS Resolver - We can maintain our own DNS cache to avoid making requests to the DNS resolver all the time, which can be costly. It's updated periodically by cron jobs.  * Locality - We can distribute crawl jobs based on geography. When crawlers are physically closer to website servers, latency is lower.  * Short timeout - We need to add a timeout in case servers are unresponsive beyond a given threshold. Otherwise, our crawlers can spend a lot of time waiting for pages which will never come.</p>"},{"location":"booknotes/system-design-interview/chapter10/#robustness","title":"Robustness","text":"<p>Some approaches to achieve robustness:  * Consistent hashing - To enable easy rescaling of our workers/crawlers/etc, we can use consistent hashing when load balancing jobs among them.  * Save crawl state and data - In the event of server crashes, it would be useful to store intermediary results on disk so that other workers can pick up from where the last one left.  * Exception handling - We need to handle exceptions gracefully without crashing the servers as these are inevitable in a large enough system.  * Data validation - important safety measure to prevent system errors.</p>"},{"location":"booknotes/system-design-interview/chapter10/#extensibility","title":"Extensibility","text":"<p>We need to ensure the crawler is extendable if we want to support new content types in the future: </p> <p>Example extensions:  * PNG Downloader is added in order to crawl PNG images.  * Web monitor is added to monitor for copyright infringements.</p>"},{"location":"booknotes/system-design-interview/chapter10/#detect-and-avoid-problematic-content","title":"Detect and avoid problematic content","text":"<p>Some common types of problematic content to be aware of:  * Redundant content - ~30% of web pages on the internet are duplicates. We need to avoid processing them more than once using hashes/checksums.  * Spider traps - A web page which leads to an infinite loop on the crawler, eg an extremely deep directory structure. This can be avoided by specifying a max length for URLs. But there are other sorts of spider traps as well. We can introduce the ability to manually intervene and blacklist spider trap websites.  * Data noise - Some content has no value for our target use-case. Eg advertisements, code snippets, spam, etc. We need to filter those.</p>"},{"location":"booknotes/system-design-interview/chapter10/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Characteristics of a good crawler - scalable, polite, extensible, robust.</p> <p>Other relevant talking points:  * Server-side rendering - Numerous sites dynamically generate HTML. If we parse the HTML without generating it first, we'll miss the information on the site. To solve this, we do server-side rendering first before parsing a page.  * Filter out unwanted pages - Anti-spam component is beneficial for filtering low quality pages.  * Database replication and sharding - useful techniques to improve the data layer's availability, scalability, reliability.  * Horizontal scaling - key is to keep servers stateless to enable horizontally scaling every type of server/worker/crawler/etc.  * Availability, consistency, reliability - concepts at the core of any large system's success.  * Analytics - We might also have to collect and analyze data in order to fine tune our system further.</p>"},{"location":"booknotes/system-design-interview/chapter11/","title":"Design a Notification System","text":"<p>Notification systems are a popular feature in many applications - it alerts a user for important news, product updates, events, etc.</p> <p>There are multiple flavors of a notification:  * Mobile push notification  * SMS  * Email</p>"},{"location":"booknotes/system-design-interview/chapter11/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What types of notifications does the system support?</li> <li>I: Push notifications, SMS, Email</li> <li>C: Is it a real-time system?</li> <li>I: Soft real-time. We want user to receive notification as soon as possible, but delays are okay if system is under high load.</li> <li>C: What are the supported devices?</li> <li>I: iOS devices, android devices, laptop/desktop.</li> <li>C: What triggers notifications?</li> <li>I: Notifications can be triggered by client applications or on the server-side.</li> <li>C: Will users be able to opt-out?</li> <li>I: Yes</li> <li>C: How many notifications per day?</li> <li>I: 10mil mobile push, 1mil SMS, 5mil email</li> </ul>"},{"location":"booknotes/system-design-interview/chapter11/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>This section explores the high-level design of the notification system.</p>"},{"location":"booknotes/system-design-interview/chapter11/#different-types-of-notifications","title":"Different types of notifications","text":"<p>How do the different notification types work at a high level?</p>"},{"location":"booknotes/system-design-interview/chapter11/#ios-push-notification","title":"iOS push notification","text":"<p>  * Provider - builds and sends notification requests to Apple Push Notification Service (APNS). To do that, it needs some inputs:    * Device token - unique identifier used for sending push notifications     * Payload - JSON payload for the notification, eg: <pre><code>{\n   \"aps\":{\n      \"alert\":{\n         \"title\":\"Game Request\",\n         \"body\":\"Bob wants to play chess\",\n         \"action-loc-key\":\"PLAY\"\n      },\n      \"badge\":5\n   }\n}\n</code></pre>  * APNS - service, provided by Apple for sending mobile push notifications  * iOS Device - end client, which receives the push Notifications</p>"},{"location":"booknotes/system-design-interview/chapter11/#android-push-notification","title":"Android Push Notification","text":"<p>Android adopts a similar approach. A common alternative to APNS is Firebase Cloud Messaging: </p>"},{"location":"booknotes/system-design-interview/chapter11/#sms-message","title":"SMS Message","text":"<p>For SMS, third-party providers like Twilio are available: </p>"},{"location":"booknotes/system-design-interview/chapter11/#email","title":"Email","text":"<p>Although clients can setup their own mail servers, most clients opt-in to use third-party services, like Mailchimp: </p> <p>Here's final design after including all notification providers: </p>"},{"location":"booknotes/system-design-interview/chapter11/#contact-info-gathering-form","title":"Contact info gathering form","text":"<p>In order to send notifications, we need to gather some inputs from the user first. That is done at user signup: </p> <p>Example database tables for storing contact info: </p>"},{"location":"booknotes/system-design-interview/chapter11/#notification-sendingreceiving-flow","title":"Notification sending/receiving flow","text":"<p>Here's the high-level design of our notification system:   * Service 1 to N - other services in the system or cron jobs which trigger notification sending events.  * Notification system - accepts notification sending messages and propagates to the correct provider.  * Third-party services - responsible for delivering the messages to the correct users via the appropriate medium. This part should be build \\w extensibility in case we change third-party service providers in the future.  * iOS, Android, SMS, Email - Users receive notifications on their devices.</p> <p>Some problems in this design:  * Single point of failure - only a single notification service  * Hard to scale - since notification system handles everything, it is hard to independently scale eg the cache/database/service layer/etc.  * Performance bottleneck - handling everything in one system can be a bottleneck especially for resource-intensive tasks such as building HTML pages.</p>"},{"location":"booknotes/system-design-interview/chapter11/#high-level-design-improved","title":"High-level design (improved)","text":"<p>Some changes from the original naive design:  * Move database &amp; cache out of the notification service  * Add more notification servers &amp; setup autoscaling &amp; load balancing  * Introduce message queues to decouple system components</p> <p>  * Service 1 to N - services which send notifications within our system  * Notification servers - provide APIs for sending notifications. Visible to internal services or verified clients. Do basic validation. Fetch notification templates from database. Put notification data in message queues for parallel processing.  * Cache - user info, device info, notification templates  * DB - stores data about users, notifications, settings, etc.  * Message queues - Remove dependencies across components. They serve as buffers for notifications to be sent out. Each notification provider has a different message queue assigned to avoid outages in one third-party provider to affect the rest.  * Workers - pull notification events from message queues and send them to corresponding third-party services.  * Third-party services - already covered in initial design.  * iOS, Android, SMS, Email - already covered in initial design.</p> <p>Example API call to send an email: <pre><code>{\n   \"to\":[\n      {\n         \"user_id\":123456\n      }\n   ],\n   \"from\":{\n      \"email\":\"from_address@example.com\"\n   },\n   \"subject\":\"Hello World!\",\n   \"content\":[\n      {\n         \"type\":\"text/plain\",\n         \"value\":\"Hello, World!\"\n      }\n   ]\n}\n</code></pre></p> <p>Example lifecycle of a notification:  * Service makes a call to make a notification  * Notification service fetch metadata (user info, settings, etc) from database/cache   * Notification event is sent to corresponding queue for processing for each third-party provider.  * Workers pull notifications from the message queues and send them to third-party services.  * Third-party services deliver nofications to end users.</p>"},{"location":"booknotes/system-design-interview/chapter11/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>In this section, we discuss some additional considerations for our improved design.</p>"},{"location":"booknotes/system-design-interview/chapter11/#reliability","title":"Reliability","text":"<p>Some questions to consider in terms of making the system reliable:  * What happens in the event of data loss?  * Will recipients receive notifications exactly once?</p> <p>To avoid data loss, we can persist notifications in a notification log database on the workers, which retry them in case a notification doesn't go through: </p> <p>What about duplicate notifications?</p> <p>It will occasionally happen as we can't guarantee exactly-once delivery (unless the third-party API provides idempotency keys). If they don't we can still try to reduce probability of this happening by having a dedup mechanism on our end, which discards an event id if it is already seen.</p>"},{"location":"booknotes/system-design-interview/chapter11/#additional-components-and-considerations","title":"Additional components and considerations","text":""},{"location":"booknotes/system-design-interview/chapter11/#notification-templates","title":"Notification templates","text":"<p>To avoid building every notification from scratch on the client side, we'll introduce notification templates as many notifications can reuse them: <pre><code>BODY:\nYou dreamed of it. We dared it. [ITEM NAME] is back \u2014 only until [DATE].\n\nCTA:\nOrder Now. Or, Save My [ITEM NAME]\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter11/#notification-setting","title":"Notification setting","text":"<p>Before sending any notification, we first check if user has opted in for the given communication channel via this database table: <pre><code>user_id bigInt\nchannel varchar # push notification, email or SMS\nopt_in boolean # opt-in to receive notification\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter11/#rate-limiting","title":"Rate limiting","text":"<p>To avoid overwhelming users with too many notifications, we can introduce some client-side rate limiting (on our end) so that they don't opt out of notifications immediately once they get bombarded.</p>"},{"location":"booknotes/system-design-interview/chapter11/#retry-mechanism","title":"Retry mechanism","text":"<p>If a third-party provider fails to send a notification, it will be put into a retry queue. If problem persists, developers are notified.</p>"},{"location":"booknotes/system-design-interview/chapter11/#security-in-push-notifications","title":"Security in push notifications","text":"<p>Only verified and authenticated clients are allowed to send push notifications through our APIs. We do this by requiring an appKey and appSecret, inspired by Android/Apple notification servers.</p>"},{"location":"booknotes/system-design-interview/chapter11/#monitor-queued-notifications","title":"Monitor queued notifications","text":"<p>A critical metric to keep track of is number of queued notifications. If it gets too big, we might have to add more workers: </p>"},{"location":"booknotes/system-design-interview/chapter11/#events-tracking","title":"Events tracking","text":"<p>We might have to track certain events related to a notification, eg open rate/click rate/etc.</p> <p>Usually, this is done by integrating with an Analytics service, so we'll need to integrate our notification system with one. </p>"},{"location":"booknotes/system-design-interview/chapter11/#updated-design","title":"Updated design","text":"<p>Putting everything together, here's our final design: </p> <p>Other features we've added:  * Notification servers are equipped with authentication and rate limiting.  * Added a retry mechanism to handle notification failures.  * Notification templates are added to provide a coherent notification experience.  * Monitoring and tracking systems are added to keep track of system health for future improvements.</p>"},{"location":"booknotes/system-design-interview/chapter11/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We introduced a robust notification system which supports push notifications, sms and email. We introduced message queues to decouple system components.</p> <p>We also dug deeper into some components and optimizations:  * Reliability - added robust retry mechanism in case of failures  * Security - Appkey/appSecret is used to ensure only verified clients can make notifications.  * Tracking and monitoring - implemented to monitor important stats.  * Respect user settings - Users can opt-out of receiving notifications. Service checks the user settings first, before sending notifications.  * Rate limiting - Users would appreciate if we don't bombard them with a dozen of notifications all of a sudden.</p>"},{"location":"booknotes/system-design-interview/chapter12/","title":"Design a News Feed System","text":"<p>News feed == constantly updating list of stories on your home page.</p> <p>It includes status updates, photos, videos, links, etc.</p> <p>Similar interview questions - design facebook news feed, twitter timeline, instagram feed, etc.</p>"},{"location":"booknotes/system-design-interview/chapter12/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>First step is to clarify what the interviewer has in mind exactly:  * C: Mobile, web app?  * I: Both  * C: What are the important features?  * I: User can publish posts and see friends' posts on news feed.  * C: Is news feed sorted in reverse chronological order or based on rank, eg best friends' posts first.  * I: To keep it simple, let's assume reverse chrono order  * C: Max number of friends?  * I: 5000  * C: Traffic volume?  * I: 10mil DAU  * C: Can the feed contain media?  * I: It can contain images and video</p>"},{"location":"booknotes/system-design-interview/chapter12/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>There are two parts to the design:  * Feed publishing - when user publishes a post, corresponding data is written to cache and DB. Post is populated to friends' news feed.  * Newsfeed building - built by aggregating friends' posts in news feed.</p>"},{"location":"booknotes/system-design-interview/chapter12/#newsfeed-api","title":"Newsfeed API","text":"<p>The Newsfeed API is the primary gateway for users to the news feed services.</p> <p>Here's some of the main endpoints.  * <code>POST /v1/me/feed</code> - publish a post. Payload includes <code>content</code> + <code>auth_token</code>.  * <code>GET /v1/me/feed</code> - retrieve news feed. Payload includes <code>auth_token</code>.</p>"},{"location":"booknotes/system-design-interview/chapter12/#feed-publishing","title":"Feed publishing","text":"<p>  * User makes a new post via API.  * Load balancer - distributes traffic to web servers.  * Web servers - redirect traffic to internal services.  * Post service - persist post in database and cache.  * Fanout service - push posts to friends' news feeds.  * Notification service - inform new friends that content is available.</p>"},{"location":"booknotes/system-design-interview/chapter12/#newsfeed-building","title":"Newsfeed building","text":"<p>  * User sends request to retrieve news feed.  * Load balancer redirects traffic to web servers.  * Web servers - route requests to newsfeed service.  * Newsfeed service - fetch news feed from cache.  * Newsfeed cache - store pre-computed news feeds for fast retrieval.</p>"},{"location":"booknotes/system-design-interview/chapter12/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>Let's discuss the two flows we covered in more depth.</p>"},{"location":"booknotes/system-design-interview/chapter12/#feed-publishing-deep-dive","title":"Feed publishing deep dive","text":""},{"location":"booknotes/system-design-interview/chapter12/#web-servers","title":"Web servers","text":"<p>besides a gateway to the internal services, these do authentication and apply rate limits, in order to prevent spam.</p>"},{"location":"booknotes/system-design-interview/chapter12/#fanout-service","title":"Fanout service","text":"<p>This is the process of delivering posts to friends. There are two types of fanouts - fanout on write (push model) and fanout on read (pull model).</p> <p>Fanout on write (push model) - posts are pre-computed during post publishing.</p> <p>Pros:  * news feed is generated in real-time and can be delivered instantly to friends' news feed.  * fetching the news feed is fast as it's precomputed</p> <p>Cons:  * if a friend has many friends, generating the news feed takes a lot of time, which slows down post publishing speed. This is the hotkey problem.  * for inactive users, pre-computing the news feed is a waste.</p> <p>Fanout on read (pull model) - news feed is generated during read time.</p> <p>Pros:  * Works better for inactive users, as news feeds are not generated for them.  * Data is not pushed to friends, hence, no hotkey problem.</p> <p>Cons:  * Fetching the news feed is slow as it's not pre-computed.</p> <p>We'll adopt a hybrid approach - we'll pre-compute the news feed for people without many friends and use the pull model for celebrities and users with many friends/followers.</p> <p>System diagram of fanout service:   * Fetch friend IDs from graph database. They're suited for managing friend relationships and recommendations.  * Get friends info from user cache. Filtering is applied here for eg muted/blocked friends.  * Send friends list and post ID to the message queue.  * Fanout workers fetch the messages and store the news feed data in a cache. They store a <code>&lt;post_id, user_id&gt;</code> mappings** in it which can later be retrieved.</p> <p>** I think there is some kind of error in this part of the book. It doesn't make sense to store a <code>&lt;post_id, user_id&gt;</code> mapping in the cache. Instead, it should be a <code>&lt;user_id, post_id&gt;</code> mapping as that allows one to quickly fetch all posts for a given user, which are part of their news feed. In addition to that, the example in the book shows that you can store multiple user_ids or post_ids as keys in the cache, which is typically not supported in eg a hashmap, but it is actually supported when you use the <code>Redis Sets</code> feature, but that is not explicitly mentioned in the chapter.</p>"},{"location":"booknotes/system-design-interview/chapter12/#news-feed-retrieval-deep-dive","title":"News feed retrieval deep dive","text":"<p>  * user sends request to retrieve news feed.  * Load balancer distributes request to a set of web servers.  * Web servers call news feed service.  * News feed service gets a list of <code>post_id</code> from the news feed cache.  * Then, the posts in the news feed are hydrated with usernames, content, media files, etc.  * Fully hydrated news feed is returned as a JSON to the user.  * Media files are also stored in CDN and fetched from there for better user experience.</p>"},{"location":"booknotes/system-design-interview/chapter12/#cache-architecture","title":"Cache architecture","text":"<p>Cache is very important for a news feed service. We divided it into 5 layers:   * news feed - stores ids of news feeds  * content - stores every post data. Popular content is stored in hot cache.  * social graph - store user relationship data.  * action - store info about whether a user liked, replied or took actions on a post.  * counters - counters for replies, likes, followers, following, etc.</p>"},{"location":"booknotes/system-design-interview/chapter12/#step-4-wrap-up","title":"Step 4 - wrap up","text":"<p>In this chapter, we designed a news feed system and we covered two main use-cases - feed publishing and feed retrieval.</p> <p>Talking points, related to scalability:  * vertical vs. horizontal database scaling  * SQL vs. NoSQL  * Master-slave replication  * Read replicas  * Consistency models  * Database sharding</p> <p>Other talking points:  * keep web tier stateless  * cache data as much as possible  * multiple data center setup  * Loose coupling components via message queues  * Monitoring key metrics - QPS and latency.</p>"},{"location":"booknotes/system-design-interview/chapter13/","title":"Design a Chat System","text":"<p>We'll be designing a chat system similar to Messenger, WhatsApp, etc.</p> <p>In this case, it is very important to nail down the exact requirements because chat systems can differ a lot - eg ones focused on group chats vs. one-on-one conversations.</p>"},{"location":"booknotes/system-design-interview/chapter13/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What kind of chat app should we design? One-on-one convos or group chat?</li> <li>I: It should support both cases.</li> <li>C: Mobile app, web app, both?</li> <li>I: Both</li> <li>C: What's the app scale? Startup or massive application?</li> <li>I: It should support 50mil DAU</li> <li>C: For group chat, what is the member limit?</li> <li>I: 100 people</li> <li>C: What features are important? Eg attachments?</li> <li>I: 1-on-1 and group chats. Online indicator. Text messages only.</li> <li>C: Is there message size limit?</li> <li>I: Text length is less than 100,000 chars long.</li> <li>C: End-to-end encryption required?</li> <li>I: Not required, but will discuss if time permits.</li> <li>C: How long should chat history be stored?</li> <li>I: Forever</li> </ul> <p>Summary of features we'll focus on:  * One-on-one chat with low delivery latency  * Small group chats (100 ppl)  * Online presence  * Same account can be logged in via multiple devices.  * Push notifications  * Scale of 50mil DAU</p>"},{"location":"booknotes/system-design-interview/chapter13/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>Let's understand how clients and servers communicate first.  * In this system, clients can be mobile devices or web browsers.  * They don't connect to each other directly. They are connected to a server.</p> <p>Main functions the chat service should support:  * Receive messages from clients  * Find the right recipients for a message and relay it  * If recipient is not online, hold messages for them until they get back online. </p> <p>When clients connect to the server, they can do it via one or more network protocols. One option is HTTP. That is okay for the sender-side, but not okay for receiver-side.</p> <p>There are multiple options to handle a server-initiated message for the client - polling, long-polling, web sockets.</p>"},{"location":"booknotes/system-design-interview/chapter13/#polling","title":"Polling","text":"<p>Polling requires the client to periodically ask the server for status updates: </p> <p>This is easy to implement but it can be costly as there are many requests, which often yield no results</p>"},{"location":"booknotes/system-design-interview/chapter13/#long-polling","title":"Long polling","text":"<p>With long polling, clients hold the connection open while waiting for an event to occur on the server-side. This still has some wasted requests if users don't chat much, but it is more efficient than polling.</p> <p>Other caveats:  * Server has no good way to determine if client is disconnected.  * senders and receivers might be connected to different servers.</p>"},{"location":"booknotes/system-design-interview/chapter13/#websocket","title":"WebSocket","text":"<p>Most common approach when bidirectional communication is needed: </p> <p>The connection is initiated by the client and starts as HTTP, but can be upgraded after handshake. In this setup, both clients and servers can initiate messages.</p> <p>One caveat with web sockets is that this is a persistent protocol, making the servers stateful. Efficient connection management is necessary when using it.</p>"},{"location":"booknotes/system-design-interview/chapter13/#high-level-design","title":"High-level design","text":"<p>Although we mentioned how web sockets can be useful for exchanging messages, most other standard features of our chat can use the normal request/response protocol over HTTP.</p> <p>Given this remark, our service can be broken down into three parts - stateless API, stateful websocket API and third-party integration for notifications: </p>"},{"location":"booknotes/system-design-interview/chapter13/#stateless-services","title":"Stateless Services","text":"<p>Traditional public-facing request/response services are used to manage login, signup, user profile, etc.</p> <p>These services sit behind a load-balancer, which distributes requests across a set of service replicas.</p> <p>The service discovery service, in particular, is interesting and will be discussed more in-depth in the deep dive.</p>"},{"location":"booknotes/system-design-interview/chapter13/#stateful-service","title":"Stateful Service","text":"<p>The only stateful service is our chat service. It is stateful as it maintain a persistent connection with clients which connect to it.</p> <p>In this case, a client doesn't switch to other chat services as long as the existing one stays alive.</p> <p>Service discovery coordinates closely with the chat services to avoid overload.</p>"},{"location":"booknotes/system-design-interview/chapter13/#third-party-integration","title":"Third-party Integration","text":"<p>It is important for a chat application to support push notifications in order to get notified when someone sends you a message.</p> <p>This component won't be discussed extensively as it's already covered in the Design a notification system chapter.</p>"},{"location":"booknotes/system-design-interview/chapter13/#scalability","title":"Scalability","text":"<p>On a small scale, we can fit everything in a single server.</p> <p>With 1mil concurrent users, assuming each connection takes up 10k memory, a single server will need to use 10GB of memory to service them all.</p> <p>Despite this, we shouldn't propose a single-server setup as it raises a red flag in the interviewer. One big drawback of a single server design is the single point of failure.</p> <p>It is fine, however, to start from a single-server design and extend it later as long as you explicitly state that during the interview.</p> <p>Here's our refined high-level design:   * clients maintain a persistent web socket connection with a chat server for real-time messaging  * The chat servers facilitate message sending/receiving  * Presense servers manage online/offline status  * API servers handle traditional request/response-based responsibilities - login, sign up, change profile, etc.  * Notification servers manage push notifications  * Key-value store is used for storing chat history. When offline user goes online, they will see their chat history and missed messages.</p>"},{"location":"booknotes/system-design-interview/chapter13/#storage","title":"Storage","text":"<p>One important decision for the storage/data layer is whether we should go with a SQL or NoSQL database.</p> <p>To make the decision, we need to examine the read/write access patterns.</p> <p>Traditional data such as user profile, settings, user friends list can be stored in a traditional relational database. Replication and sharding are common techniques to meet scalability needs for relational databases.</p> <p>Chat history data, on the other hand, is very specific kind of data of chat systems due to its read/write pattern:  * Amount of data is enormous, a study revealed that Facebook and WhatsApp process 60bil messages per day.  * Only recent chats are accessed frequently. Users typically don't go too far back in chat history.  * Although chat history is accessed infrequently, we should still be able to search within it as users can use a search bar for random access.  * Read to write ratio is 1:1 on chat apps.</p> <p>Selecting the correct storage system for this kind of data is crucial. Author recommends using a key-value store:  * they allow easy horizontal scaling  * they provide low latency access to data  * Relational databases don't handle long-tail (less-frequently accessed but large part of a distribution) of data well. When indexes grow large, random access is expensive.  * Key-value stores are widely adopted for chat systems. Facebook and Discord both use key-value stores. Facebook uses HBase, Discord uses Cassandra.</p>"},{"location":"booknotes/system-design-interview/chapter13/#data-models","title":"Data models","text":"<p>Let's take a look at the data model for our messages.</p> <p>Message table for one-on-one chat: </p> <p>One caveat is that we'll use the primary key (message_id) instead of created_at to determine message sequence as messages can be sent at the same time.</p> <p>Message table for a group chat: </p> <p>In the above table, <code>(channel_id, message_id)</code> is the primary key, while <code>channel_id</code> is also the sharding key.</p> <p>One interesting discussion is how should the <code>message_id</code> be generated, as it is used for message ordering. It should have two important attributes:  * IDs must be unique  * IDs must be sortable by time</p> <p>One option is to use the <code>auto_increment</code> feature of relational databases. But that's not supported in key-value stores. An alternative is to use Snowflake - Twitter's algorithm for generating 64-byte IDs which are globally unique and sortable by time.</p> <p>Finally, we could also use a local sequence number generator, which is unique only within a group.  We can afford this because we only need to guarantee message sequence within a chat, but not between different chats.</p>"},{"location":"booknotes/system-design-interview/chapter13/#step-3-design-deep-dive","title":"Step 3 - Design deep-dive","text":"<p>In a system design interview, typically you are asked to go deeper into some of the components.</p> <p>In this case, we'll go deeper into the service discovery component, messaging flows and online/offline indicator.</p>"},{"location":"booknotes/system-design-interview/chapter13/#service-discovery","title":"Service discovery","text":"<p>The primary goal of service discovery is to choose the best server based on some criteria - eg geographic location, server capacity, etc.</p> <p>Apache Zookeeper is a popular open-source solution for service discovery. It registers all available chat servers and picks the best one based on a predefined criteria.   * User A tries to login to the app  * Load balancer sends request to API servers.  * After authentication, service discovery chooses the best chat server for user A. In this case, chat server 2 is chosen.  * User A connects to chat server 2 via web sockets protocol.</p>"},{"location":"booknotes/system-design-interview/chapter13/#message-flows","title":"Message flows","text":"<p>The message flows are an interesting topic to deep dive into. We'll explore one on one chats, message synchronization and group chat.</p>"},{"location":"booknotes/system-design-interview/chapter13/#1-on-1-chat-flow","title":"1 on 1 chat flow","text":"<p>  * User A sends a message to chat server 1  * Chat server 1 obtains a message_id from Id generator  * Chat server 1 sends the message to the \"message sync\" queue.  * Message is stored in a key-value store.  * If User B is online, message is forwarded to chat server 2, where User B is connected.  * If offline, push notification is sent via the push notification servers.  * Chat server 2 forwards the message to user B.</p>"},{"location":"booknotes/system-design-interview/chapter13/#message-synchronization-across-devices","title":"Message synchronization across devices","text":"<p>  * When user A logs in via phone, a web socket is established for that device with chat server 1.  * Each device maintains a variable called <code>cur_max_message_id</code>, keeping track of latest message received on given device.  * Messages whose recipient ID is currently logged in (via any device) and whose message_id is greater than <code>cur_max_message_id</code> are considered new</p>"},{"location":"booknotes/system-design-interview/chapter13/#small-group-chat-flow","title":"Small group chat flow","text":"<p>Group chats are a bit more complicated: </p> <p>Whenever User A sends a message, the message is copied across each message queue of participants in the group (User B and C).</p> <p>Using one inbox per user is a good choice for small group chats as:  * it simplifies message sync since each user only need to consult their own queue.  * storing a message copy in each participant's inbox is feasible for small group chats.</p> <p>This is not acceptable though, for larger group chats.</p> <p>As for the recipient, in their queue, they can receive messages from different group chats: </p>"},{"location":"booknotes/system-design-interview/chapter13/#online-presence","title":"Online presence","text":"<p>Presence servers manage the online/offline indication in chat applications.</p> <p>Whenever the user logs in, their status is changed to \"online\": </p> <p>Once the user send a logout message to the presence servers (and subsequently disconnects), their status is changed to \"offline\": </p> <p>One caveat is handling user disconnection. A naive approach to handle that is to mark a user as \"offline\" when they disconnect from the presence server. This makes for a poor user experience as a user could frequently disconnect and reconnect to presence servers due to poor internet.</p> <p>To mitigate this, we'll introduce a heartbeat mechanism - clients periodically send a heartbeat to the presence servers to indicate online status. If a heartbeat is not received within a given time frame, user is marked offline: </p> <p>How does a user's friend find out about a user's presence status though?</p> <p>We'll use a fanout mechanism, where each friend pair have a queue assigned and status changes are sent to the respective queues: </p> <p>This is effective for small group chats. WeChat uses a similar approach and its user group is capped to 500 users.</p> <p>If we need to support larger groups, a possible mitigation is to fetch presence status only when a user enters a group or refreshes the members list.</p>"},{"location":"booknotes/system-design-interview/chapter13/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>We managed to build a chat system which supports both one-on-one and group chats.  * We used web sockets for real-time communication between clients and servers.</p> <p>System components:   * chat servers (real-time messages)  * presence servers (online/offline status)  * push notification servers  * key-value stores for chat history  * API servers for everything else</p> <p>Additional talking points:  * Extend chat app to support media - video, images, voice. Compression, cloud storage and thumbnails can be discussed.  * End-to-end encryption - only sender and receiver can read messages.  * Caching messages on client-side is effective to reduce server-client data transfer.  * Improve load time - Slack built a geographically distributed network to cache user data, channels, etc for better load time.  * Error handling  * Chat server error - what happens if a chat server goes down. Zookeeper can facilitate a hand off to another chat server.  * Message resend mechanism - retrying and queueing are common approaches for re-sending messages.</p>"},{"location":"booknotes/system-design-interview/chapter14/","title":"Design A Search Autocomplete System","text":"<p>Search autocomplete is the feature provided by many platforms such as Amazon, Google and others when you put your cursor in your search bar and start typing something you're looking for: </p>"},{"location":"booknotes/system-design-interview/chapter14/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: Is the matching only supported at the beginning of a search term or eg at the middle?</li> <li>I: Only at the beginning</li> <li>C: How many autocompletion suggestions should the system return?</li> <li>I: 5</li> <li>C: Which suggestions should the system choose?</li> <li>I: Determined by popularity based on historical query frequency</li> <li>C: Does system support spell check?</li> <li>I: Spell check or auto-correct is not supported.</li> <li>C: Are search queries in English?</li> <li>I: Yes, if time allows, we can discuss multi-language support</li> <li>C: Is capitalization and special characters supported?</li> <li>I: We assume all queries use lowercase characters</li> <li>C: How many users use the product?</li> <li>I: 10mil DAU</li> </ul> <p>Summary:  * Fast response time. An article about facebook autocomplete reviews that suggestions should be returned with 100ms delay at most to avoid stuttering  * Relevant - autocomplete suggestions should be relevant to search term  * Sorted - suggestions should be sorted by popularity  * Scalable - system can handle high traffic volumes  * Highly available - system should be up even if parts of the system are unresponsive</p>"},{"location":"booknotes/system-design-interview/chapter14/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<ul> <li>Assume we have 10mil DAU</li> <li>On average, person performs 10 searches per day</li> <li>10mil * 10 = 100mil searches per day = 100 000 000 / 86400 = 1200 searches.</li> <li>given 4 works of 5 chars search on average -&gt; 1200 * 20 = 24000 QPS. Peak QPS = 48000 QPS.</li> <li>20% of daily queries are new -&gt; 100mil * 0.2 = 20mil new searches * 20 bytes = 400mb new data per day.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter14/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>At a high-level, the system has two components:  * Data gathering service - gathers user input queries and aggregates them in real-time.  * Query service - given search query, return topmost 5 suggestions.</p>"},{"location":"booknotes/system-design-interview/chapter14/#data-gathering-service","title":"Data gathering service","text":"<p>This service is responsible for maintaining a frequency table: </p>"},{"location":"booknotes/system-design-interview/chapter14/#query-service","title":"Query service","text":"<p>Given a frequency table like the one above, this service is responsible for returning the top 5 suggestions based on the frequency column: </p> <p>Querying the data set is a matter of running the following SQL query: </p> <p>This is acceptable for small data sets but becomes impractical for large ones.</p>"},{"location":"booknotes/system-design-interview/chapter14/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>In this section, we'll deep dive into several components which will improve the initial high-level design.</p>"},{"location":"booknotes/system-design-interview/chapter14/#trie-data-structure","title":"Trie data structure","text":"<p>We use relational databases in the high-level design, but to achieve a more optimal solution, we'll need to leverage a suitable data structure.</p> <p>We can use tries for fast string prefix retrieval.  * It is a tree-like data structure  * The root represents the empty string  * Each node has 26 children, representing each of the next possible characters. To save space, we don't store empty links.  * Each node represents a single word or prefix  * For this problem, apart from storing the strings, we'll need to store the frequency against each leaf </p> <p>To implement the algorithm, we need to:  * first find the node representing the prefix (time complexity O(p), where p = length of prefix)  * traverse subtree to find all leafs (time complexity O(c), where c = total children)  * sort retrieved children by their frequencies (time complexity O(clogc), where c = total children) </p> <p>This algorithm works, but there are ways to optimize it as we'll have to traverse the entire trie in the worst-case scenario.</p>"},{"location":"booknotes/system-design-interview/chapter14/#limit-the-max-length-of-prefix","title":"Limit the max length of prefix","text":"<p>We can leverage the fact that users rarely use a very long search term to limit max prefix to 50 chars.</p> <p>This reduces the time complexity from <code>O(p) + O(c) + O(clogc)</code> -&gt; <code>O(1) + O(c) + O(clogc)</code>.</p>"},{"location":"booknotes/system-design-interview/chapter14/#cache-top-search-queries-at-each-node","title":"Cache top search queries at each node","text":"<p>To avoid traversing the whole trie, we can cache the top k most frequently accessed works in each node: </p> <p>This reduces the time complexity to <code>O(1)</code> as top K search terms are already cached. The trade-off is that it takes much more space than a traditional trie.</p>"},{"location":"booknotes/system-design-interview/chapter14/#data-gathering-service_1","title":"Data gathering service","text":"<p>In previous design, when user types in search term, data is updated in real-time. This is not practical on a bigger scale due to:  * billions of queries per day  * Top suggestions may not change much once trie is built</p> <p>Hence, we'll instead update the trie asynchronously based on analytics data: </p> <p>The analytics logs contain raw rows of data related to search terms \\w timestamps: </p> <p>The aggregators' responsibility is to map the analytics data into a suitable format and also aggregate it to lesser records.</p> <p>The cadence at which we aggregate depends on the use-case for our auto-complete functionality.  If we need the data to be relatively fresh &amp; updated real-time (eg twitter search), we can aggregate once every eg 30m. If, on the other hand, we don't need the data to be updated real-time (eg google search), we can aggregate once per week.</p> <p>Example weekly aggregated data: </p> <p>The workers are responsible for building the trie data structure, based on aggregated data, and storing it in DB.</p> <p>The trie cache keeps the trie loaded in-memory for fast read. It takes a weekly snapshot of the DB.</p> <p>The trie DB is the persistent storage. There are two options for this problem:  * Document store (eg MongoDB) - we can periodically build the trie, serialize it and store it in the DB.  * Key-value store (eg DynamoDB) - we can also store the trie in hashmap format. </p>"},{"location":"booknotes/system-design-interview/chapter14/#query-service_1","title":"Query service","text":"<p>The query service fetches top suggestions from Trie Cache or fallbacks to Trie DB on cache miss: </p> <p>Some additional optimizations for the Query service:  * Using AJAX requests on client-side - these prevent the browser from refreshing the page.  * Data sampling - instead of logging all requests, we can log a sample of them to avoid too many logs.  * Browser caching - since auto-complete suggestions don't change often, we can leverage the browser cache to avoid extra calls to backend.</p> <p>Example with Google search caching search results on the browser for 1h: </p>"},{"location":"booknotes/system-design-interview/chapter14/#trie-operations","title":"Trie operations","text":"<p>Let's briefly describe common trie operations.</p>"},{"location":"booknotes/system-design-interview/chapter14/#create","title":"Create","text":"<p>The trie is created by workers using aggregated data, collected via analytics logs.</p>"},{"location":"booknotes/system-design-interview/chapter14/#update","title":"Update","text":"<p>There are two options to handling updates:  * Not updating the trie, but reconstructing it instead. This is acceptable if we don't need real-time suggestions.  * Updating individual nodes directly - we prefer to avoid it as it's slow. Updating a single node required updating all parent nodes as well due to the cached suggestions: </p>"},{"location":"booknotes/system-design-interview/chapter14/#delete","title":"Delete","text":"<p>To avoid showing suggestions including hateful content or any other content we don't want to show, we can add a filter between the trie cache and the API servers: </p> <p>The database is asynchronously updated to remove hateful content.</p>"},{"location":"booknotes/system-design-interview/chapter14/#scale-the-storage","title":"Scale the storage","text":"<p>At some point, our trie won't be able to fit on a single server. We need to devise a sharding mechanism.</p> <p>One option to achieve this is to shard based on the letters of the alphabet - eg <code>a-m</code> goes on one shard, <code>n-z</code> on the other.</p> <p>This doesn't work well as data is unevenly distributed due to eg the letter <code>a</code> being much more frequent than <code>x</code>.</p> <p>To mitigate this, we can have a dedicated shard mapper, which is responsible for devising a smart sharding algorithm, which factors in the uneven distribution of search terms: </p>"},{"location":"booknotes/system-design-interview/chapter14/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Other talking points:  * How to support multi-language - we store unicode characters in trie nodes, instead of ASCII.  * What if top search queries differ across countries - we can build different tries per country and leverage CDNs to improve response time.  * How can we support trending (real-time) search queries? - current design doesn't support this and improving it to support it is beyond the scope of the book. Some options:     * Reduce working data set via sharding    * Change ranking model to assign more weight to recent search queries    * Data may come as streams which you filter upon and use map-reduce technologies to process it - Hadoop, Apache Spark, Apache Storm, Apache Kafka, etc.</p>"},{"location":"booknotes/system-design-interview/chapter15/","title":"Design YouTube","text":"<p>This chapter is about designing a video sharing platform such as youtube. Its solution can be applied to also eg designing Netflix, Hulu.</p>"},{"location":"booknotes/system-design-interview/chapter15/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What features are important?</li> <li>I: Upload video + watch video</li> <li>C: What clients do we need to support?</li> <li>I: Mobile apps, web apps, smart TV</li> <li>C: How many DAUs do we have?</li> <li>I: 5mil</li> <li>C: Average time per day spend on YouTube?</li> <li>I: 30m</li> <li>C: Do we need to support international users?</li> <li>I: Yes</li> <li>C: What video resolutions do we need to support?</li> <li>I: Most of them</li> <li>C: Is encryption required?</li> <li>I: Yes</li> <li>C: File size requirement for videos?</li> <li>I: Max file size is 1GB</li> <li>C: Can we leverage existing cloud infra from Google, Amazon, Microsoft?</li> <li>I: Yes, building everything from scratch is not a good idea.</li> </ul> <p>Features, we'll focus on:  * Upload videos fast  * Smooth video streaming  * Ability to change video quality  * Low infrastructure cost  * High availability, scalability, reliability  * Supported clients - web, mobile, smart TV</p>"},{"location":"booknotes/system-design-interview/chapter15/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<ul> <li>Assume product has 5mil DAU</li> <li>Users watch 5 videos per day</li> <li>10% of users upload 1 video per day</li> <li>Average video size is 300mb</li> <li>Daily storage cost needed - 5mil * 10% * 300mb = 150TB</li> <li>CDN Cost, assuming 0.02$ per GB - 5mil * 5 videos * 0.3GB * 0.02$ = USD 150k per day</li> </ul>"},{"location":"booknotes/system-design-interview/chapter15/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>As previously discussed, we won't be building everything from scratch.</p> <p>Why?  * In a system design interview, choosing the right technology is more important than explaining how the technology works.  * Building scalable blob storage over CDN is complex and costly. Even big tech don't build everything from scratch. Netflix uses AWS and Facebook uses Akamai's CDN.</p> <p>Here's our system design at a high-level:   * Client - you can watch youtube on web, mobile and TV.  * CDN - videos are stored in CDN.  * API Servers - Everything else, except video streaming goes through the API servers. Feed recommendation, generating video URL, updating metadata db and cache, user signup.</p> <p>Let's explore high-level design of video streaming and uploading.</p>"},{"location":"booknotes/system-design-interview/chapter15/#video-uploading-flow","title":"Video uploading flow","text":"<p>  * Users watch videos on a supported client  * Load balancer evenly distributes requests across API servers  * All user requests go through API servers, except video streaming  * Metadata DB - sharded and replicated to meet performance and availability requirements  * Metadata cache - for better performance, video metadata and user objects are cached  * A blob storage system is used to store the actual videos  * Transcoding/encoding servers - transform videos to various formats (eg MPEG, HLS, etc) which are suitable for different devices and bandwidth  * Transcoded storage stores result files from transcoding  * Videos are cached in CDN - clicking play streams the video from CDN  * Completion queue - stores events about video transcoding results  * Completion handler - a set of workers which pull event data from completion queue and update metadata cache and database</p> <p>Let's now explore the flow of uploading videos and video metadata. Metadata includes info about video URL, size, resolution, format, etc.</p> <p>Here's how the video uploading flow works:   * Videos are uploaded to original storage  * Transcoding servers fetch videos from storage and start transcoding  * Once transcoding is complete, two steps are executed in parallel:    * Transcoded videos are sent to transcoded storage and distributed to CDN    * Transcoding completion events are queued in completion queue, workers pick up the events and update metadata database &amp; cache  * API servers inform user that uploading is complete</p> <p>Here's how the metadata update flow works:   * While file is being uploaded, user sends a request to update the video metadata - file name, size, format, etc.  * API servers update metadata database &amp; cache</p>"},{"location":"booknotes/system-design-interview/chapter15/#video-streaming-flow","title":"Video streaming flow","text":"<p>Whenever users watch videos on YouTube, they don't download the whole video at once. Instead, they download a little and start watching it while downloading the rest. This is referred to as streaming. Stream is served from closest CDN server for lowest latency.</p> <p>Some popular streaming protocols:  * MPEG-DASH - \"Moving Picture Experts Group\"-\"Dynamic Adaptive Streaming over HTTP\"  * Apple HLS - \"HTTP Live Streaming\"  * Microsoft Smooth Streaming  * Adobe HTTP Dynamic Streaming (HDS)</p> <p>You don't need to understand those protocols in detail. It is important to understand, though, that different streaming protocols support different video encodings and playback players.</p> <p>We need to choose the right streaming protocol to support our use-case.</p>"},{"location":"booknotes/system-design-interview/chapter15/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>In this part, we'll deep dive into the video uploading and video streaming flows.</p>"},{"location":"booknotes/system-design-interview/chapter15/#video-transcoding","title":"Video transcoding","text":"<p>Video transcoding is important for a few reasons:  * Raw video consumes a lot of storage space.  * Many browsers have constraints on the type of videos they can support. It is important to encode a video for compatibility reasons.  * To ensure good UX, you ought to serve HD videos to users with good network connection and lower-quality formats for the ones with slower connection.  * Network conditions can change, especially on mobile. It is important to be able to automatically switch video formats at runtime for smooth UX.</p> <p>Most transcoding formats consist of two parts:  * Container - the basket which contains the video file. Recognized by the file extension, eg .avi, .mov, .mp4  * Codecs - Compression and decompression algorithms, which reduce video size while preserving quality. Most popular ones - H.264, VP9, HEVC.</p>"},{"location":"booknotes/system-design-interview/chapter15/#directed-acyclic-graph-dag-model","title":"Directed Acyclic Graph (DAG) model","text":"<p>Transcoding video is computationally expensive and time-consuming.  In addition to that, different creators have different inputs - some provide thumbnails, others do not, some upload HD, others don't.</p> <p>In order to support video processing pipelines, dev customisations, high parallelism, we adopt a DAG model: </p> <p>Some of the tasks applied on a video file:  * Ensure video has good quality and is not malformed  * Video is encoded to support different resolutions, codecs, bitrates, etc.  * Thumbnail is automatically added if a user doesn't specify it.  * Watermark - image overlay on video if specified by creator </p>"},{"location":"booknotes/system-design-interview/chapter15/#video-transcoding-architecture","title":"Video transcoding architecture","text":""},{"location":"booknotes/system-design-interview/chapter15/#preprocessor","title":"Preprocessor","text":"<p>The preprocessor's responsibilities:  * Video splitting - video is split in group of pictures (GOP) alignment, ie arranged groups of chunks which can be played independently  * Cache - intermediary steps are stored in persistent storage in order to retry on failure.  * DAG generation - DAG is generated based on config files specified by programmers.</p> <p>Example DAG configuration with two steps: </p>"},{"location":"booknotes/system-design-interview/chapter15/#dag-scheduler","title":"DAG Scheduler","text":"<p>DAG scheduler splits a DAG into stages of tasks and puts them in a task queue, managed by a resource manager: </p> <p>In this example, a video is split into video, audio and metadata stages which are processed in parallel.</p>"},{"location":"booknotes/system-design-interview/chapter15/#resource-manager","title":"Resource manager","text":"<p>Resource manager is responsible for optimizing resource allocation.   * Task queue is a priority queue of tasks to be executed  * Worker queue is a queue of available workers and worker utilization info  * Running queue contains info about currently running tasks and which workers they're assigned to</p> <p>How it works:  * task scheduler gets highest-priority task from queue  * task scheduler gets optimal task worker to run the task  * task scheduler instructs worker to start working on the task  * task scheduler binds worker to task &amp; puts task/worker info in running queue  * task scheduler removes the job from the running queue once the job is done</p>"},{"location":"booknotes/system-design-interview/chapter15/#task-workers","title":"Task workers","text":"<p>The workers execute the tasks in the DAG. Different workers are responsible for different tasks and can be scaled independently. </p>"},{"location":"booknotes/system-design-interview/chapter15/#temporary-storage","title":"Temporary storage","text":"<p>Multiple storage systems are used for different types of data. Eg temporary images/video/audio is put in blob storage. Metadata is put in an in-memory cache as data size is small.</p> <p>Data is freed up once processing is complete.</p>"},{"location":"booknotes/system-design-interview/chapter15/#encoded-video","title":"Encoded video","text":"<p>Final output of the DAG. Example output - <code>funny_720p.mp4</code>.</p>"},{"location":"booknotes/system-design-interview/chapter15/#system-optimizations","title":"System Optimizations","text":"<p>Now it's time to introduce some optimizations for speed, safety, cost-saving.</p>"},{"location":"booknotes/system-design-interview/chapter15/#speed-optimization-parallelize-video-uploading","title":"Speed optimization - parallelize video uploading","text":"<p>We can split video uploading into separate units via GOP alignment: </p> <p>This enables fast resumable uploads if something goes wrong. Splitting the video file is done by the client.</p>"},{"location":"booknotes/system-design-interview/chapter15/#speed-optimization-place-upload-centers-close-to-users","title":"Speed optimization - place upload centers close to users","text":"<p>This can be achieved by leveraging CDNs.</p>"},{"location":"booknotes/system-design-interview/chapter15/#speed-optimization-parallelism-everywhere","title":"Speed optimization - parallelism everywhere","text":"<p>We can build a loosely coupled system and enable high parallelism.</p> <p>Currently, components rely on inputs from previous components in order to produce outputs: </p> <p>We can introduce message queues so that components can start doing their task independently of previous one once events are available: </p>"},{"location":"booknotes/system-design-interview/chapter15/#safety-optimization-pre-signed-upload-url","title":"Safety optimization - pre-signed upload URL","text":"<p>To avoid unauthorized users from uploading videos, we introduce pre-signed upload URLs: </p> <p>How it works:  * client makes request to API server to fetch upload URL  * API servers generate the URL and return it to the client  * Client uploads the video using the URL</p>"},{"location":"booknotes/system-design-interview/chapter15/#safety-optimization-protect-your-videos","title":"Safety optimization - protect your videos","text":"<p>To protect creators from having their original content stolen, we can introduce some safety options:  * Digital right management (DRM) systems - Apple FairPlay, Google Widevine, Microsoft PlayReady  * AES encryption - you can encrypt a video and configure an authorization policy. It is decrypted on playback.  * Visual watermarking - image overlay on top of video which contains your identifying information, eg company name.</p>"},{"location":"booknotes/system-design-interview/chapter15/#cost-saving-optimization","title":"Cost-saving optimization","text":"<p>CDN is expensive, as we've seen in our back of the envelope estimation.</p> <p>We can piggyback on the fact that video streams follow a long-tail distribution - ie a few popular videos are accessed frequently, but everything else is not.</p> <p>Hence, we can store popular videos in CDN and serve everything else from high capacity storage servers: </p> <p>Other cost-saving optimizations:  * We might not need to store many encoded versions for less popular videos. Short videos can be encoded on-demand.  * Some videos are only popular in certain regions. We can avoid distributing them in all regions.  * Build your own CDN. Can make sense for large streaming companies like Netflix.</p>"},{"location":"booknotes/system-design-interview/chapter15/#error-handling","title":"Error Handling","text":"<p>For a large-scale system, errors are unavoidable. To make a fault-tolerant system, we need to handle errors gracefully and recover from them.</p> <p>There are two types of errors:  * Recoverable error - can be mitigated by retrying a few times. If retrying fails, a proper error code is returned to the client.  * Non-recoverable error - system stops running related tasks and returns proper error code to the client.</p> <p>Other typical errors and their resolution:  * Upload error - retry a few times  * Split video error - entire video is passed to server if older clients don't support GOP alignment.  * Transcoding error - retry  * Preprocessor error - regenerate DAG  * DAG scheduler error - retry scheduling  * Resource manager queue down - use a replica  * Task worker down - retry task on different worker  * API server down - they're stateless so requests can be redirected to other servers  * Metadata db/cache server down - replicate data across multiple nodes  * Master is down - Promote one of the slaves to become master  * Slave is down - If slave goes down, you can use another slave for reads and bring up another slave instance</p>"},{"location":"booknotes/system-design-interview/chapter15/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Additional talking points:  * Scaling the API layer - easy to scale horizontally as API layer is stateless  * Scale the database - replication and sharding  * Live streaming - our system is not designed for live streams, but it shares some similarities, eg uploading, encoding, streaming. Notable differences:    * Live streaming has higher latency requirements so it might demand a different streaming protocol    * Lower requirement for parallelism as small chunks of data are already processed in real time    * different error handling, as there is a timeout after which we need to stop retrying    * Video takedowns - videos that violate copyrights, pornography, any other illegal acts need to be removed either during upload flow or based on user flagging.</p>"},{"location":"booknotes/system-design-interview/chapter16/","title":"Design Google Drive","text":"<p>Google Drive is a cloud file storage product, which helps you store documents, videos, etc from the cloud.</p> <p>You can access them from any device and share them with friends and family.</p>"},{"location":"booknotes/system-design-interview/chapter16/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<ul> <li>C: What are most important features?</li> <li>I: Upload/download files, file sync and notifications</li> <li>C: Mobile or web?</li> <li>I: Both</li> <li>C: What are the supported file formats?</li> <li>I: Any file type</li> <li>C: Do files need to be encrypted?</li> <li>I: Yes, files in storage need to be encrypted</li> <li>C: Is the a file size limit?</li> <li>I: Yes, files need to be 10 gb or smaller</li> <li>C: How many users does the app have?</li> <li>I: 10mil DAU</li> </ul> <p>Features we'll focus on:  * Adding files  * Downloading files  * Sync files across devices  * See file revisions  * Share file with friends  * Send a notification when file is edited/deleted/shared</p> <p>Features not discussed:  * Collaborative editing</p> <p>Non-functional requirements:  * Reliability - data loss is unacceptable  * Fast sync speed  * Bandwidth usage - users will get unhappy if app consumes too much network traffic or battery  * Scalability - we need to handle a lot of traffic  * High availability - users should be able to use the system even when some services are down</p>"},{"location":"booknotes/system-design-interview/chapter16/#back-of-the-envelope-estimation","title":"Back of the envelope estimation","text":"<ul> <li>Assume 50mil sign ups and 10mil DAU</li> <li>Users get 10 gb free space</li> <li>Users upload 2 files per day, average size is 500kb</li> <li>1:1 read-write ratio</li> <li>Total space allocated - 50mil * 10gb = 500pb</li> <li>QPS for upload API - 10mil * 2 uploads / 24h / 3600s = ~240</li> <li>Peak QPS = 480</li> </ul>"},{"location":"booknotes/system-design-interview/chapter16/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - propose high-level design and get buy-in","text":"<p>In this chapter, we'll use a different approach than other ones - we'll start building the design from a single server and scale out from there.</p> <p>We'll start from:  * A web server to upload and download files  * A database to keep track of metadata - user data, login info, files info, etc  * Storage system to store the files</p> <p>Example storage we could use: </p>"},{"location":"booknotes/system-design-interview/chapter16/#apis","title":"APIs","text":"<p>Upload file: <pre><code>https://api.example.com/files/upload?uploadType=resumable\n</code></pre></p> <p>This endpoint is used for uploading files with support for simple upload and resumable upload, which is used for large files. Resumable upload is achieved by retrieving an upload URL and uploading the file while monitoring upload state. If disturbed, resume the upload.</p> <p>Download file: <pre><code>https://api.example.com/files/download\n</code></pre></p> <p>The payload specifies which file to download: <pre><code>{\n    \"path\": \"/recipes/soup/best_soup.txt\"\n}\n</code></pre></p> <p>Get file revisions: <pre><code>https://api.example.com/files/list_revisions\n</code></pre></p> <p>params:  * path to file for which revision history is retrieved  * maximum number of revisions to return</p> <p>All the APIs require authentication and use HTTPS.</p>"},{"location":"booknotes/system-design-interview/chapter16/#move-away-from-single-server","title":"Move away from single server","text":"<p>As more files are uploaded, at some point, you reach your storage's capacity.</p> <p>One option to scale your storage server is by implementing sharing - each user's data is stored on separate servers: </p> <p>This solves your issue but you're still worried about potential data loss.</p> <p>A good option to address that is to use an off-the-shelf solution like Amazon S3 which offers replication (same-region/cross-region) out of the box: </p> <p>Other areas you could improve:  * Load balancing - this ensures evenly distributed network traffic to your web server replicas.  * More web servers - with the advent of a load balancer, you can easily scale your web server layer by adding more servers.  * Metadata database - move the database away from the server to avoid single points of failure. You can also setup replication and sharding to meet scalability requirements.  * File storage - Amazon S3 for storage. To ensure availability and durability, files are replicated in two separate geographical regions.</p> <p>Here's the updated design: </p>"},{"location":"booknotes/system-design-interview/chapter16/#sync-conflicts","title":"Sync conflicts","text":"<p>Once the user base grows sufficiently, sync conflicts are unavoidable.</p> <p>To address this, we can apply a strategy where the first who manages to modify a file first wins: </p> <p>What happens once you get a conflict? We generate a second version of the file which represents the alternative file version and it's up to the user to merge it: </p>"},{"location":"booknotes/system-design-interview/chapter16/#high-level-design","title":"High-level design","text":"<p>  * User uses the application through a browser or a mobile app  * Block servers upload files to cloud storage. Block storage is a technology which allows you to split a big file in blocks and store the blocks in a backing storage. Dropbox, for example, stores blocks of size 4mb.  * Cloud storage - a file split into multiple blocks is stored in cloud storage  * Cold storage - used for storing inactive files, infrequently accessed.  * Load balancer - evenly distributes requests among API servers.  * API servers - responsible for anything other than uploading files. Authentication, user profile management, updating file metadata, etc.  * Metadata database - stores metadata about files uploaded to cloud storage.  * Metadata cache - some of the metadata is cached for fast retrieval.  * Notification service - Publisher/subscriber system which notifies users when a file is updated/edited/removed so that they can pull the latest changes.  * Offline backup queue - used to queue file changes for users who are offline so that they can pull them once they come back online.</p>"},{"location":"booknotes/system-design-interview/chapter16/#step-3-design-deep-dive","title":"Step 3 - Design deep dive","text":"<p>Let's explore:  * block servers  * metadata database  * upload/download flow  * notification service  * saving storage space  * failure handling</p>"},{"location":"booknotes/system-design-interview/chapter16/#block-servers","title":"Block servers","text":"<p>For large files, it's infeasible to send the whole file on each update as it consumes a lot of bandwidth.</p> <p>Two optimizations we're going to explore:  * Delta sync - once a file is modified, only modified blocks are sent to the block servers instead of the whole file.  * Compression - applying compression on blocks can significantly reduce data size. Different algorithms are suitable for different file types, eg for text files, we'll use gzip/bzip2.</p> <p>Apart from splitting files in blocks, the block servers also apply encryption prior to storing files in file storage: </p> <p>Example delta sync: </p>"},{"location":"booknotes/system-design-interview/chapter16/#high-consistency-requirement","title":"High consistency requirement","text":"<p>Our system requires strong consistency as it's unacceptable to show different versions of a file to different people.</p> <p>This is mainly problematic when we use caches, in particular the metadata cache in our example.  To sustain strong consistency, we need to:  * keep cache master and replicas consistent  * invalidate caches on database write</p> <p>For the database, strong consistency is guaranteed as long as we use a relational database, which supports ACID (all typically do).</p>"},{"location":"booknotes/system-design-interview/chapter16/#metadata-database","title":"Metadata database","text":"<p>Here's a simplified table schema for the metadata db (only interesting fields are shown):   * User table contains basic information about the user such as username, email, profile photo, etc.  * Device table stores device info. Push_id is used for sending push notifications. Users can have multiple devices.  * Namespace - root directory of a user  * File table stores everything related to a file  * File_version stores the version history of a file. Existing fields are read-only to sustain file integrity.  * Block - stores everything related to a file block. A file version can be reconstructed by joining all blocks in the correct version.</p>"},{"location":"booknotes/system-design-interview/chapter16/#upload-flow","title":"Upload flow","text":"<p>In the above flow, two requests are sent in parallel - updating file metadata and uploading the file to cloud storage.</p> <p>Add file metadata:  * Client 1 sends request to update file metadata  * New file metadata is stored and upload status is set to \"pending\"  * Notify the notification service that a new file is being added.  * Notification service notifies relevant clients about the file upload.</p> <p>Upload files to cloud storage:  * Client 1 uploads file contents to block servers  * Block servers chunk the file in blocks, compresses, encrypts them and uploads to cloud storage  * Once file is uploaded, upload completion callback is triggered. Request is sent to API servers.  * File status is changed to \"uploaded\" in Metadata DB.  * Notification service is notified of file uploaded event and client 2 is notified about the new file.</p>"},{"location":"booknotes/system-design-interview/chapter16/#download-flow","title":"Download flow","text":"<p>Download flow is triggered when file is added or edited elsewhere. Client is notified via:  * Notification if online  * New changes are cached until user comes online if offline at the moment</p> <p>Once a client is notified of the changes, it requests the file metadata and then downloads the blocks to reconstruct the file:   * Notification service informs client 2 of file changes  * Client 2 fetches metadata from API servers  * API servers fetch metadata from metadata DB  * Client 2 gets the metadata  * Once client receives the metadata, it sends requests to block servers to download blocks  * Block servers download blocks from cloud storage and forwards them to the client</p>"},{"location":"booknotes/system-design-interview/chapter16/#notification-service","title":"Notification service","text":"<p>The notification service enables file changes to be communicated to clients as they happen.</p> <p>Clients can communicate with the notification service via:  * long polling (eg Dropbox uses this approach)  * Web sockets - communication is persistent and bidirectional</p> <p>Both options work well but we opt for long polling because:  * Communication for notification service is not bi-directional. Server sends information to clients, not vice versa.  * WebSocket is meant for real-time bidirectional communication. For google drive, notifications are sent infrequently.</p> <p>With long polling, the client sends a request to the server which stays open until a change is received or timeout is reached.  After that, a subsequent request is sent for next couple of changes.</p>"},{"location":"booknotes/system-design-interview/chapter16/#save-storage-space","title":"Save storage space","text":"<p>To support file version history and ensure reliability, multiple versions of a file are stored across multiple data centers.</p> <p>Storage space can be filled up quickly. Three techniques can be applied to save storage space:  * De-duplicate data blocks - if two blocks have the same hash, we can only store them once.  * Adopt an intelligent backup strategy - set a limit on max version history and aggregate frequent edits into a single version.  * Move infrequently accessed data to cold storage - eg Amazon S3 glacier is a good option for this, which is much cheaper than Amazon S3.</p>"},{"location":"booknotes/system-design-interview/chapter16/#failure-handling","title":"Failure handling","text":"<p>Some typical failures and how you could resolve them:  * Load balancer failure - If a load balancer fails, a secondary becomes active and picks up the traffic.  * Block server failure - If a block server fails, other replicas pick up the traffic and finish the job.  * Cloud storage failure - S3 buckets are replicated across regions. If one region fails, traffic is redirected to the other one.  * API server failure - Traffic is redirected to other service instances by the load balancer.  * Metadata cache failure - Metadata cache servers are replicated multiple times. If one goes down, other nodes are still available.  * Metadata DB failure - if master is down, promote one of the slaves to be master. If slave is down, use another one for read operations.  * Notification service failure - If long polling connections are lost, clients reconnect to a different service replica, but reconnection of millions of clients will take some time.  * Offline backup queue failure - Queues are replicated multiple times. If one queue fails, consumers need to resubscribe to the backup queue.</p>"},{"location":"booknotes/system-design-interview/chapter16/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Properties of our Google Drive system design in a nutshell:  * Strongly consistent  * Low network bandwidth  * Fast sync</p> <p>Our design contains two flows - file upload and file sync.</p> <p>If time permits, you could discuss alternative design approaches as there is no perfect design. For example, we can upload blocks directly to cloud storage instead of going through block servers. </p> <p>This is faster than our approach but has drawbacks:  * Chunking, compression, encryption need to be implemented on different platforms (Android, iOS, Web).   * Client can be hacked so implementing encryption client-side is not ideal.</p> <p>Another interesting discussion is moving online/offline logic to separate service so that other services can reuse it to implement interesting functionality.</p>"},{"location":"booknotes/system-design-interview/chapter17/","title":"Proximity Service","text":"<p>A proximity service enables you to discover nearby places such as restaurants, hotels, theatres, etc.</p>"},{"location":"booknotes/system-design-interview/chapter17/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>Sample questions to understand the problem better:  * C: Can a user specify a search radius? What if there are not enough businesses within the search area?  * I: We only care about businesses within a certain area. If time permits, we can discuss enhancing the functionality.  * C: What's the max radius allowed? Can I assume it's 20km?  * I: Yes, that is a reasonable assumption  * C: Can a user change the search radius via the UI?  * I: Yes, let's say we have the options - 0.5km, 1km, 2km, 5km, 20km  * C: How is business information modified? Do we need to reflect changes in real-time?  * I: Business owners can add/delete/update a business. Assume changes are going to be propagated on the next day.  * C: How do we handle search results while the user is moving?  * I: Let's assume we don't need to constantly update the page since users are moving slowly.</p>"},{"location":"booknotes/system-design-interview/chapter17/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Return all businesses based on user's location</li> <li>Business owners can add/delete/update a business. Information is not reflected in real-time.</li> <li>Customers can view detailed information about a business</li> </ul>"},{"location":"booknotes/system-design-interview/chapter17/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Low latency - users should be able to see nearby businesses quickly</li> <li>Data privacy - Location info is sensitive data and we should take this into consideration in order to comply with regulations</li> <li>High availability and scalability requirements - We should ensure system can handle spike in traffic during peak hours in densely populated areas</li> </ul>"},{"location":"booknotes/system-design-interview/chapter17/#back-of-the-envelope-calculation","title":"Back-of-the-envelope calculation","text":"<ul> <li>Assuming 100mil daily active users and 200mil businesses</li> <li>Search QPS == 100mil * 5 (average searches per day) / 10^5 (seconds in day) == 5000</li> </ul>"},{"location":"booknotes/system-design-interview/chapter17/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and get Buy-In","text":""},{"location":"booknotes/system-design-interview/chapter17/#api-design","title":"API Design","text":"<p>We'll use a RESTful API convention to design a simplified version of the APIs. <pre><code>GET /v1/search/nearby\n</code></pre></p> <p>This endpoint returns businesses based on search criteria, paginated.</p> <p>Request parameters - latitude, longitude, radius</p> <p>Example response: <pre><code>{\n  \"total\": 10,\n  \"businesses\":[{business object}]\n}\n</code></pre></p> <p>The endpoint returns everything required to render a search results page, but a user might require additional details about a particular business, fetched via other endpoints.</p> <p>Here's some other business APIs we'll need:  * <code>GET /v1/businesses/{:id}</code> - return business detailed info  * <code>POST /v1/businesses</code> - create a new business  * <code>PUT /v1/businesses/{:id}</code> - update business details  * <code>DELETE /v1/businesses/{:id}</code> - delete a business</p>"},{"location":"booknotes/system-design-interview/chapter17/#data-model","title":"Data model","text":"<p>In this problem, the read volume is high because these features are commonly used:  * Search for nearby businesses  * View the detailed information of a business</p> <p>On the other hand, write volume is low because we rarely change business information. Hence for a read-heavy workflow, a relational database such as MySQL is ideal.</p> <p>In terms of schema, we'll need one main <code>business</code> table which holds information about a business: </p> <p>We'll also need a geo-index table so that we efficiently process spatial operations. This table will be discussed later when we introduce the concept of geohashes.</p>"},{"location":"booknotes/system-design-interview/chapter17/#high-level-design","title":"High-level design","text":"<p>Here's a high-level overview of the system:   * The load balancer automatically distributes incoming traffic across multiple services. A company typically provides a single DNS entry point and internally routes API calls to appropriate services based on URL paths.  * Location-based service (LBS) - read-heavy, stateless service, responsible for serving read requests for nearby businesses  * Business service - supports CRUD operations on businesses.  * Database cluster - stores business information and replicates it in order to scale reads. This leads to some inconsistency for LBS to read business information, which is not an issue for our use-case  * Scalability of business service and LBS - since both services are stateless, we can easily scale them horizontally</p>"},{"location":"booknotes/system-design-interview/chapter17/#algorithms-to-fetch-nearby-businesses","title":"Algorithms to fetch nearby businesses","text":"<p>In real life, one might use a geospatial database, such as Geohash in Redis or Postgres with PostGIS extension.</p> <p>Let's explore how these databases work and what other alternative algorithms there are for this type of problem.</p>"},{"location":"booknotes/system-design-interview/chapter17/#two-dimensional-search","title":"Two-dimensional search","text":"<p>The most intuitive and naive approach to solving this problem is to draw a circle around the person and fetch all businesses within the circle's radius: </p> <p>This can easily be translated to a SQL query: <pre><code>SELECT business_id, latitude, longitude,\nFROM business\nWHERE (latitude BETWEEN {:my_lat} - radius AND {:my_lat} + radius) AND\n      (longitude BETWEEN {:my_long} - radius AND {:my_long} + radius)\n</code></pre></p> <p>This query is not efficient because we need to query the whole table. An alternative is to build an index on the longitude and latitude columns but that won't improve performance by much.</p> <p>This is because we still need to subsequently filter a lot of data regardless of whether we index by long or lat: </p> <p>We can, however, build 2D indexes and there are different approaches to that: </p> <p>We'll discuss the ones highlighted in purple - geohash, quadtree and google S2 are the most popular approaches.</p>"},{"location":"booknotes/system-design-interview/chapter17/#evenly-divided-grid","title":"Evenly divided grid","text":"<p>Another option is to divide the world in small grids: </p> <p>The major flaw with this approach is that business distribution is uneven as there are a lot of businesses concentrated in new york and close to zero in the sahara desert.</p>"},{"location":"booknotes/system-design-interview/chapter17/#geohash","title":"Geohash","text":"<p>Geohash works similarly to the previous approach, but it recursively divides the world into smaller and smaller grids, where each two bits correspond to a single quadrant: </p> <p>Geohashes are typically represented in base32. Here's the example geohash of google headquarters: <pre><code>1001 10110 01001 10000 11011 11010 (base32 in binary) \u2192 9q9hvu (base32)\n</code></pre></p> <p>It supports 12 levels of precision, but we only need up to 6 levels for our use-case: </p> <p>Geohashes enable us to quickly locate neighboring regions based on a substring of the geohash: </p> <p>However, one issue \\w geohashes is that there can be places which are very close to each other which don't share any prefix, because they're on different sides of the equator or meridian: </p> <p>Another issue is that two businesses can be very close but not share a common prefix because they're in different quadrants: </p> <p>This can be mitigated by fetching neighboring geohashes, not just the geohash of the user.</p> <p>A benefit of using geohashes is that we can use them to easily implement the bonus problem of increasing search radius in case insufficient businesses are fetched via query: </p> <p>This can be done by removing the last letter of the target geohash to increase radius.</p>"},{"location":"booknotes/system-design-interview/chapter17/#quadtree","title":"Quadtree","text":"<p>A quadtree is a data structure, which recursively subdivides quadrants as deep as it needs to, based on business needs: </p> <p>This is an in-memory solution which can't easily be implemented in a database.</p> <p>Here's how it might look conceptually: </p> <p>Example pseudocode to build a quadtree: <pre><code>public void buildQuadtree(TreeNode node) {\n    if (countNumberOfBusinessesInCurrentGrid(node) &gt; 100) {\n        node.subdivide();\n        for (TreeNode child : node.getChildren()) {\n            buildQuadtree(child);\n        }\n    }\n}\n</code></pre></p> <p>In a leaf node, we store:  * Top-left, bottom-right coordinates to identify the quadrant dimensions  * List of business IDs in the grid</p> <p>In an internalt node we store:  * Top-left, bottom-right coordinates of quadrant dimensions  * 4 pointers to children</p> <p>The total memory to represent the quadtree is calculated as ~1.7GB in the book if we assume that we operate with 200mil businesses.</p> <p>Hence, a quadtree can be stored in a single server, in-memory, although we can of course replicate it for redundancy and load balancing purposes.</p> <p>One consideration to take into consideration if this approach is adopted - startup time of server can be a couple of minutes while the quadtree is being built.</p> <p>Hence, this should be taken into account during the deployment process. Eg a healthcheck endpoint can be exposed and queried to signal when the quadtree build is finished.</p> <p>Another consideration is how to update the quadtree. Given our requirements, a good option would be to update it every night using a nightly job due to our commitment of reflecting changes at start of next day.</p> <p>It is nevertheless possible to update the quadtree on the fly, but that would complicate the implementation significantly.</p> <p>Example quadtree of Denver: </p>"},{"location":"booknotes/system-design-interview/chapter17/#google-s2","title":"Google S2","text":"<p>Google S2 is a geometry library, which supports mapping 2D points on a 1D plane using hilbert curves. Objects close to each other on the 2D plane are close on the hilbert curve as well: </p> <p>This library is great for geofencing, which supports covering arbitrary areas vs. confining yourself to specific quadrants. </p> <p>This functionality can be used to support more advanced use-cases than nearby businesses.</p> <p>Another benefit of Google S2 is its region cover algorithm, which enables us to define more granular precision levels, than those provided by geohashes.</p>"},{"location":"booknotes/system-design-interview/chapter17/#recommendation","title":"Recommendation","text":"<p>There is no perfect solution, different companies adopt different solutions: </p> <p>Author suggest choosing geohashes or quadtree in an interview as those are easier to explain than Google S2.</p> <p>Here's a quick summary of geohashes:  * Easy to use and implement, no need to build a tree  * supports returning businesses within a specified radius  * Geohash precision is fixed. More complex logic is required if a more granular precision is needed  * Updating the index is easy</p> <p>Here's a quick summary of quadtrees:  * Slightly harder to implement as it requires us to build a tree  * Supports fetching k-nearest neighbors instead of businesses within radius, which can be a good use-case for certain features  * Grid size can be dynamically adjusted based on population density  * Updating the index is more complicated than updating the geohash variant. All problems with updating and balancing trees are present when working with quad trees.</p>"},{"location":"booknotes/system-design-interview/chapter17/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's dive deeper into some areas of the design.</p>"},{"location":"booknotes/system-design-interview/chapter17/#scale-the-database","title":"Scale the database","text":"<p>The business table can be scaled by sharding it in case it doesn't fit in a single server instance.</p> <p>The geohash table can be represented by two columns: </p> <p>We don't need to shard the geohash table as we don't have that much data. We calculated that it takes ~1.7gb to build a quad tree and geohash space usage is similar.</p> <p>We can, however, replicate the table to scale the read load.</p>"},{"location":"booknotes/system-design-interview/chapter17/#caching","title":"Caching","text":"<p>Before using caching, we should ask ourselves if it is really necessary. In our case, the workflow is read-heavy and data can fit into a single server, so this kind of data is ripe for caching.</p> <p>We should be careful when choosing the cache key. Location coordinates are not a good cache key as they often change and can be inaccurate.</p> <p>Using the geohash is a more suitable key candidate.</p> <p>Here's how we could query all businesses in a geohash: <pre><code>SELECT business_id FROM geohash_index WHERE geohash LIKE `{:geohash}%`\n</code></pre></p> <p>Here's example code to cache the data in redis: <pre><code>public List&lt;String&gt; getNearbyBusinessIds(String geohash) {\n    String cacheKey = hash(geohash);\n    List&lt;string&gt; listOfBusinessIds = Redis.get(cacheKey);\n    if (listOfBusinessIDs  == null) {\n        listOfBusinessIds = Run the select SQL query above;\n        Cache.set(cacheKey, listOfBusinessIds, \"1d\");\n    }\n    return listOfBusinessIds;\n}\n</code></pre></p> <p>We can cache the data on all precisions we support, which are not a lot, ie <code>geohash_4, geohash_5, geohash_6</code>.</p> <p>As we already discussed, the storage requirements are not high and can fit into a single redis server, but we could replicate it for redundancy purposes as well as to scale reads.</p> <p>We can even deploy multiple redis replicas across different data centers.</p> <p>We could also cache <code>business_id -&gt; business_data</code> as users could often query the details of the same popular restaurant.</p>"},{"location":"booknotes/system-design-interview/chapter17/#region-and-availability-zones","title":"Region and availability zones","text":"<p>We can deploy multiple LBS service instances across the globe so that users query the instance closest to them. This leads to reduced latency; </p> <p>It also enables us to spread traffic evenly across the globe. This could also be required in order to comply with certain data privacy laws.</p>"},{"location":"booknotes/system-design-interview/chapter17/#follow-up-question-filter-businesses-by-type-or-time","title":"Follow-up question - filter businesses by type or time","text":"<p>Once businesses are filtered, the result set is going to be small, hence, it is acceptable to filter the data in-memory.</p>"},{"location":"booknotes/system-design-interview/chapter17/#final-design-diagram","title":"Final design diagram","text":"<p>  * Client tries to locate restaurants within 500meters of their location  * Load balancer forwards the request to the LBS  * LBS maps the radius to geohash with length 6  * LBS calculates neighboring geohashes and adds them to the list  * For each geohash, LBS calls the redis server to fetch corresponding business IDs. This can be done in parallel.  * Finally, LBS hydrates the business ids, filters the result and returns it to the user  * Business-related APIs are separated from the LBS into the business service, which checks the cache first for any read requests before consulting the database  * Business updates are handled via a nightly job, which updates the geohash store</p>"},{"location":"booknotes/system-design-interview/chapter17/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Summary of some of the more interesting topics we covered:  * Discussed several indexing options - 2d search, evenly divided grid, geohash, quadtree, google S2  * Discussed caching, replication, sharding, cross-DC deployments in the deep dive section</p>"},{"location":"booknotes/system-design-interview/chapter18/","title":"Nearby Friends","text":"<p>This chapter focuses on designing a scalable backend for an application which enables user to share their location and discover friends who are nearby.</p> <p>The major difference with the proximity chapter is that in this problem, locations constantly change, whereas in that one, business addresses more or less stay the same.</p>"},{"location":"booknotes/system-design-interview/chapter18/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>Some questions to drive the interview:  * C: How geographically close is considered to be \"nearby\"?  * I: 5 miles, this number should be configurable  * C: Is distance calculated as straight-line distance vs. taking into consideration eg a river in-between friends  * I: Yes, that is a reasonable assumption  * C: How many users does the app have?  * I: 1bil users and 10% of them use the nearby friends feature  * C: Do we need to store location history?  * I: Yes, it can be valuable for eg machine learning  * C: Can we assume inactive friends will disappear from the feature in 10min  * I: Yes  * C: Do we need to worry about GDPR, etc?  * I: No, for simlicity's sake</p>"},{"location":"booknotes/system-design-interview/chapter18/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Users should be able to see nearby friends on their mobile app. Each friend has a distance and timestamp, indicating when the location was updated</li> <li>Nearby friends list should be updated every few seconds</li> </ul>"},{"location":"booknotes/system-design-interview/chapter18/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Low latency - it's important to receive location updates without too much delay</li> <li>Reliability - Occassional data point loss is acceptable, but system should be generally available</li> <li>Eventual consistency - Location data store doesn't need strong consistency. Few seconds delay in receiving location data in different replicas is acceptable</li> </ul>"},{"location":"booknotes/system-design-interview/chapter18/#back-of-the-envelope","title":"Back-of-the-envelope","text":"<p>Some estimations to determine potential scale:  * Nearby friends are friends within 5mile radius  * Location refresh interval is 30s. Human walking speed is slow, hence, no need to update location too frequently.  * On average, 100mil users use the feature every day \\w 10% concurrent users, ie 10mil  * On average, a user has 400 friends, all of them use the nearby friends feature  * App displays 20 nearby friends per page  * Location Update QPS = 10mil / 30 == ~334k updates per second</p>"},{"location":"booknotes/system-design-interview/chapter18/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>Before exploring API and data model design, we'll study the communication protocol we'll use as it's less ubiquitous than traditional request-response communication model.</p>"},{"location":"booknotes/system-design-interview/chapter18/#high-level-design","title":"High-level design","text":"<p>At a high-level we'd want to establish effective message passing between peers. This can be done via a peer-to-peer protocol, but that's not practical for a mobile app with flaky connection and tight power consumption constraints.</p> <p>A more practical approach is to use a shared backend as a fan-out mechanism towards friends you want to reach: </p> <p>What does the backend do?  * Receives location updates from all active users  * For each location update, find all active users which should receive it and forward it to them  * Do not forward location data if distance between friends is beyond the configured threshold</p> <p>This sounds simple but the challenge is to design the system for the scale we're operating with.</p> <p>We'll start with a simpler design at first and discuss a more advanced approach in the deep dive:   * The load balancer spreads traffic across rest API servers as well as bidirectional web socket servers  * The rest API servers handles auxiliary tasks such as managing friends, updating profiles, etc  * The websocket servers are stateful servers, which forward location update requests to respective clients. It also manages seeding the mobile client with nearby friends locations at initialization (discussed in detail later).  * Redis location cache is used to store most recent location data for each active user. There is a TTL set on each entry in the cache. When the TTL expires, user is no longer active and their data is removed from the cache.  * User database stores user and friendship data. Either a relational or NoSQL database can be used for this purpose.  * Location history database stores a history of user location data, not necessarily used directly within nearby friends feature, but instead used to track historical data for analytical purposes  * Redis pubsub is used as a lightweight message bus which enables different topics for each user channel for location updates. </p> <p>In the above example, websocket servers subscribe to channels for the users which are connected to them &amp; forward location updates whenever they receive them to appropriate users.</p>"},{"location":"booknotes/system-design-interview/chapter18/#periodic-location-update","title":"Periodic location update","text":"<p>Here's how the periodic location update flow works:   * Mobile client sends a location update to the load balancer  * Load balancer forwards location update to the websocket server's persistent connection for that client  * Websocket server saves location data to location history database  * Location data is updated in location cache. Websocket server also saves location data in-memory for subsequent distance calculations for that user  * Websocket server publishes location data in user's channel via redis pub sub  * Redis pubsub broadcasts location update to all subscribers for that user channel, ie servers responsible for the friends of that user  * Subscribed web socket servers receive location update, calculate which users the update should be sent to and sends it</p> <p>Here's a more detailed version of the same flow: </p> <p>On average, there's going to be 40 location updates to forward as a user has 400 friends on average and 10% of them are online at a time.</p>"},{"location":"booknotes/system-design-interview/chapter18/#api-design","title":"API Design","text":"<p>Websocket Routines we'll need to support:  * periodic location update - user sends location data to websocket server  * client receives location update - server sends friend location data and timestamp  * websocket client initialization - client sends user location, server sends back nearby friends location data  * Subscribe to a new friend - websocket server sends a friend ID mobile client is supposed to track eg when friend appears online for the first time  * Unsubscribe a friend - websocket server sends a friend ID, mobile client is supposed to unsubscribe from due to eg friend going offline</p> <p>HTTP API - traditional request/response payloads for auxiliary responsibilities.</p>"},{"location":"booknotes/system-design-interview/chapter18/#data-model","title":"Data model","text":"<ul> <li>The location cache will store a mapping between <code>user_id</code> and <code>lat,long,timestamp</code>. Redis is a great choice for this cache as we only care about current location and it supports TTL eviction which we need for our use-case.</li> <li>Location history table stores the same data but in a relational table \\w the four columns stated above. Cassandra can be used for this data as it is optimized for write-heavy loads.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter18/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's discuss how we scale the high-level design so that it works at the scale we're targetting.</p>"},{"location":"booknotes/system-design-interview/chapter18/#how-well-does-each-component-scale","title":"How well does each component scale?","text":"<ul> <li>API servers - can be easily scaled via autoscaling groups and replicating server instances</li> <li>Websocket servers - we can easily scale out the ws servers, but we need to ensure we gracefully shutdown existing connections when tearing down a server. Eg we can mark a server as \"draining\" in the load balancer and stop sending connections to it, prior to being finally removed from the server pool</li> <li>Client initialization - when a client first connects to a server, it fetches the user's friends, subscribes to their channels on redis pubsub, fetches their location from cache and finally forwards to client</li> <li>User database - We can shard the database based on user_id. It might also make sense to expose user/friends data via a dedicated service and API, managed by a dedicated team</li> <li>Location cache - We can shard the cache easily by spinning up several redis nodes. Also, the TTL puts a limit on the max memory we could have taken up at a time. But we still want to handle the large write load</li> <li>Redis pub/sub server - we leverage the fact that no memory is consumed if there are channels initialized but are not in use. Hence, we can pre-allocate channels for all users who use the nearby friends feature to avoid having to deal with eg bringing up a new channel when a user comes online and notifying active websocket servers</li> </ul>"},{"location":"booknotes/system-design-interview/chapter18/#scaling-deep-dive-on-redis-pubsub-component","title":"Scaling deep-dive on redis pub/sub component","text":"<p>We will need around 200gb of memory to maintain all pub/sub channels. This can be achieved by using 2 redis servers with 100gb each.</p> <p>Given that we need to push ~14mil location updates per second, we will however need at least 140 redis servers to handle that amount of load, assuming that a single server can handle ~100k pushes per second.</p> <p>Hence, we'll need a distributed redis server cluster to handle the intense CPU load.</p> <p>In order to support a distributed redis cluster, we'll need to utilize a service discovery component, such as zookeeper or etcd, to keep track of which servers are alive.</p> <p>What we need to encode in the service discovery component is this data: </p> <p>Web socket servers use that encoded data, fetched from zookeeper to determine where a particular channel lives. For efficiency, the hash ring data can be cached in-memory on each websocket server.</p> <p>In terms of scaling the server cluster up or down, we can setup a daily job to scale the cluster as needed based on historical traffic data. We can also overprovision the cluster to handle spikes in loads.</p> <p>The redis cluster can be treated as a stateful storage server as there is some state maintained for the channels and there is a need for coordination with subscribers so that they hand-off to newly provisioned nodes in the cluster.</p> <p>We have to be mindful of some potential issues during scaling operations:  * There will be a lot of resubscription requests from the web socket servers due to channels being moved around  * Some location updates might be missed from clients during the operation, which is acceptable for this problem, but we should still minimize it from happening. Consider doing such operation when traffic is at lowest point of the day.  * We can leverage consistent hashing to minimize amount of channels moved in the event of adding/removing servers </p>"},{"location":"booknotes/system-design-interview/chapter18/#addingremoving-friends","title":"Adding/removing friends","text":"<p>Whenever a friend is added/removed, websocket server responsible for affected user needs to subscribe/unsubscribe from the friend's channel.</p> <p>Since the \"nearby friends\" feature is part of a larger app, we can assume that a callback on the mobile client side can be registered whenever any of the events occur and the client will send a message to the websocket server to do the appropriate action.</p>"},{"location":"booknotes/system-design-interview/chapter18/#users-with-many-friends","title":"Users with many friends","text":"<p>We can put a cap on the total number of friends one can have, eg facebook has a cap of 5000 max friends.</p> <p>The websocket server handling the \"whale\" user might have a higher load on its end, but as long as we have enough web socket servers, we should be okay.</p>"},{"location":"booknotes/system-design-interview/chapter18/#nearby-random-person","title":"Nearby random person","text":"<p>What if the interviewer wants to update the design to include a feature where we can occasionally see a random person pop up on our nearby friends map?</p> <p>One way to handle this is to define a pool of pubsub channels, based on geohash: </p> <p>Anyone within the geohash subscribes to the appropriate channel to receive location updates for random users: </p> <p>We could also subscribe to several geohashes to handle cases where someone is close but in a bordering geohash: </p>"},{"location":"booknotes/system-design-interview/chapter18/#alternative-to-redis-pubsub","title":"Alternative to Redis pub/sub","text":"<p>An alternative to using Redis for pub/sub is to leverage Erlang - a general programming language, optimized for distributed computing applications.</p> <p>With it, we can spawn millions of small, erland processes which communicate with each other. We can handle both websocket connections and pub/sub channels within the distributed erlang application.</p> <p>A challenge with using Erlang, though, is that it's a niche programming language and it could be hard to source strong erlang developers.</p>"},{"location":"booknotes/system-design-interview/chapter18/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>We successfully designed a system, supporting the nearby friends features.</p> <p>Core components:  * Web socket servers - real-time comms between client and server  * Redis - fast read and write of location data + pub/sub channels</p> <p>We also explored how to scale restful api servers, websocket servers, data layer, redis pub/sub servers and we also explored an alternative to using Redis Pub/Sub. We also explored a \"random nearby person\" feature.</p>"},{"location":"booknotes/system-design-interview/chapter19/","title":"Google Maps","text":"<p>We'll design a simple version of Google Maps.</p> <p>Some facts about google maps:  * Started in 2005  * Provides various services - satellite imagery, street maps, real-time traffic conditions, route planning  * By 2021, had 1bil daily active users, 99% coverage of the world, 25mil updates daily of real-time location info</p>"},{"location":"booknotes/system-design-interview/chapter19/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>Sample Q&amp;A between candidate and interviewer:  * C: How many daily active users are we dealing with?  * I: 1bil DAU  * C: What features should we focus on?  * I: Location update, navigation, ETA, map rendering  * C: How large is road data? Do we have access to it?  * I: We obtained road data from various sources, it's TBs of raw data  * C: Should we take traffic conditions into consideration?  * I: Yes, we should for accurate time estimations  * C: How about different travel modes - by foot, biking, driving?  * I: We should support those  * C: How about multi-stop directions?  * I: Let's not focus on that for scope of interview  * C: Business places and photos?  * I: Good question, but no need to consider those</p> <p>We'll focus on three key features - user location update, navigation service including ETA, map rendering.</p>"},{"location":"booknotes/system-design-interview/chapter19/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Accuracy - user shouldn't get wrong directions</li> <li>Smooth navigation - Users should experience smooth map rendering</li> <li>Data and battery usage - Client should use as little data and battery as possible. Important for mobile devices.</li> <li>General availability and scalability requirements</li> </ul>"},{"location":"booknotes/system-design-interview/chapter19/#map-101","title":"Map 101","text":"<p>Before jumping into the design, there are some map-related concepts we should understand.</p>"},{"location":"booknotes/system-design-interview/chapter19/#positioning-system","title":"Positioning system","text":"<p>World is a sphere, rotating on its axis. Positiions are defined by latitude (how far north/south you are) and longitude (how far east/west you are): </p>"},{"location":"booknotes/system-design-interview/chapter19/#going-from-3d-to-2d","title":"Going from 3D to 2D","text":"<p>The process of translating points from 3D to 2D plane is called \"map projection\".</p> <p>There are different ways to do it and each comes with its pros and cons. Almost all distort the actual geometry. </p> <p>Google maps selected a modified version of Mercator projection called \"Web Mercator\".</p>"},{"location":"booknotes/system-design-interview/chapter19/#geocoding","title":"Geocoding","text":"<p>Geocoding is the process of converting addresses to geographic coordinates. </p> <p>The reverse process is called \"reverse geocoding\".</p> <p>One way to achieve this is to use interpolation - leveraging data from different sources (eg GIS-es) where street network is mapped to geo coordinate space.</p>"},{"location":"booknotes/system-design-interview/chapter19/#geohashing","title":"Geohashing","text":"<p>Geohashing is an encoding system which encodes a geographic area into a string of letters and digits.</p> <p>It depicts the world as a flattened surface and recursively sub-divides it into four quadrants: </p>"},{"location":"booknotes/system-design-interview/chapter19/#map-rendering","title":"Map rendering","text":"<p>Map rendering happens via tiling. Instead of rendering entire map as one big custom image, world is broken up into smaller tiles.</p> <p>Client only downloads relevant tiles and renders them like stitching together a mosaic.</p> <p>There are different tiles for different zoom levels. Client chooses appropriate tiles based on the client's zoom level.</p> <p>Eg, zooming out the entire world would download only a single 256x256 tile, representing the whole world.</p>"},{"location":"booknotes/system-design-interview/chapter19/#road-data-processing-for-navigation-algorithms","title":"Road data processing for navigation algorithms","text":"<p>In most routing algorithms, intersections are represented as nodes and roads are represented as edges: </p> <p>Most navigation algorithms use a modified version of Djikstra or A* algorithms.</p> <p>Pathfinding performance is sensitive to the size of the graph. To work at scale, we can't represent the whole world as a graph and run the algorithm on it.</p> <p>Instead, we use a technique similar to tiling - we subdivide the world into smaller and smaller graphs.</p> <p>Routing tiles hold references to neighboring tiles and algorithms can stitch together a bigger road graph as it traverses interconnected tiles: </p> <p>This technique enables us to significantly reduce memory bandwidth and only load the tiles we need for the given source/destination pair.</p> <p>However, for larger routes, stitching together small, detailed routing tiles would still be time/memory consuming. Instead, there are routing tiles with different level of detail and the algorithm uses the appropriately-detailed tiles, based on the destination we're headed for: </p>"},{"location":"booknotes/system-design-interview/chapter19/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>For storage, we need to store:  * map of the world - estimated as ~70pb based on all the tiles we need to store, but factoring in compression of very similar tiles (eg vast desert)  * metadata - negligible in size, so we can skip it from calculation  * Road info - stored as routing tiles</p> <p>Estimated QPS for navigation requests - 1bil DAU at 35min of usage per week -&gt; 5bil minutes per day.  Assuming gps update requests are batched, we arrive at 200k QPS and 1mil QPS at peak load</p>"},{"location":"booknotes/system-design-interview/chapter19/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"booknotes/system-design-interview/chapter19/#location-service","title":"Location service","text":"<p>It is responsible for recording a user's location updates:  * location updates are sent every <code>t</code> seconds  * location data streams can be used to improve the service over time, eg provide more accurate ETAs, monitor traffic data, detect closed roads, analyze user behavior, etc</p> <p>Instead of sending location updates to the server all the time, we can batch the updates on the client-side and send batches instead: </p> <p>Despite this optimization, for a system of Google Maps scale, load will still be significant. Therefore, we can leverage a database, optimized for heavy writes such as Cassandra.</p> <p>We can also leverage Kafka for efficient stream processing of location updates, meant for further analysis.</p> <p>Example location update request payload: <pre><code>POST /v1/locations\nParameters\n  locs: JSON encoded array of (latitude, longitude, timestamp) tuples.\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter19/#navigation-service","title":"Navigation service","text":"<p>This component is responsible for finding fast routes between A and B in a reasonable time (a little bit of latency is okay). Route need not be the fastest, but accuracy is important.</p> <p>Example request payload: <pre><code>GET /v1/nav?origin=1355+market+street,SF&amp;destination=Disneyland\n</code></pre></p> <p>Example response: <pre><code>{\n  \"distance\": {\"text\":\"0.2 mi\", \"value\": 259},\n  \"duration\": {\"text\": \"1 min\", \"value\": 83},\n  \"end_location\": {\"lat\": 37.4038943, \"Ing\": -121.9410454},\n  \"html_instructions\": \"Head &lt;b&gt;northeast&lt;/b&gt; on &lt;b&gt;Brandon St&lt;/b&gt; toward &lt;b&gt;Lumin Way&lt;/b&gt;&lt;div style=\\\"font-size:0.9em\\\"&gt;Restricted usage road&lt;/div&gt;\",\n  \"polyline\": {\"points\": \"_fhcFjbhgVuAwDsCal\"},\n  \"start_location\": {\"lat\": 37.4027165, \"lng\": -121.9435809},\n  \"geocoded_waypoints\": [\n    {\n       \"geocoder_status\" : \"OK\",\n       \"partial_match\" : true,\n       \"place_id\" : \"ChIJwZNMti1fawwRO2aVVVX2yKg\",\n       \"types\" : [ \"locality\", \"political\" ]\n    },\n    {\n       \"geocoder_status\" : \"OK\",\n       \"partial_match\" : true,\n       \"place_id\" : \"ChIJ3aPgQGtXawwRLYeiBMUi7bM\",\n       \"types\" : [ \"locality\", \"political\" ]\n    }\n  ],\n  \"travel_mode\": \"DRIVING\"\n}\n</code></pre></p> <p>Traffic changes and reroutes are not taken into consideration yet, those will be tackled in the deep dive section.</p>"},{"location":"booknotes/system-design-interview/chapter19/#map-rendering_1","title":"Map rendering","text":"<p>Holding the entire data set of mapping tiles on the client-side is not feasible as it's petabytes in size.</p> <p>They need to be fetched on-demand from the server, based on the client's location and zoom level.</p> <p>When should new tiles be fetched - while user is zooming in/out and during navigation, while they're going towards a new tile.</p> <p>How should the map tiles be served to the client?  * They can be built dynamically, but that puts a huge load on the server and also makes caching hard  * Map tiles are served statically, based on their geohash, which a client can calculate. They can be statically stored &amp; served from a CDN </p> <p>CDNs enable users to fetch map tiles from point-of-presence servers (POP) which are closest to users in order to minimize latency: </p> <p>Options to consider for determining map tiles:  * geohash for map tile can be calculated on the client-side. If that's the case, we should be careful that we commit to this type of map tile calculation for the long-term as forcing clients to update is hard  * alternatively, we can have simple API which calculates the map tile URLs on behalf of the clients at the cost of additional API call </p>"},{"location":"booknotes/system-design-interview/chapter19/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":""},{"location":"booknotes/system-design-interview/chapter19/#data-model","title":"Data model","text":"<p>Let's discuss how we store the different types of data we're dealing with.</p>"},{"location":"booknotes/system-design-interview/chapter19/#routing-tiles","title":"Routing tiles","text":"<p>Initial road data set is obtained from different sources. It is improved over time based on location updates data.</p> <p>The road data is unstructured. We have a periodic offline processing pipeline, which transforms this raw data into the graph-based routing tiles our app needs.</p> <p>Instead of storing these tiles in a database as we don't need any database features. We can store them in S3 object storage, while caching them agressively.</p> <p>We can also leverage libraries to compress adjacency lists into binary files efficiently.</p>"},{"location":"booknotes/system-design-interview/chapter19/#user-location-data","title":"User location data","text":"<p>User location data is very useful for updaring traffic conditions and doing all sorts of other analysis.</p> <p>We can use Cassandra for storing this kind of data as its nature is to be write-heavy.</p> <p>Example row: </p>"},{"location":"booknotes/system-design-interview/chapter19/#geocoding-database","title":"Geocoding database","text":"<p>This database stores a key-value pair of lat/long pairs and places.</p> <p>We can use Redis for its fast read access speed, as we have frequent read and infrequent writes.</p>"},{"location":"booknotes/system-design-interview/chapter19/#precomputed-images-of-the-world-map","title":"Precomputed images of the world map","text":"<p>As we discussed, we will precompute map tiling images and store them in CDN. </p>"},{"location":"booknotes/system-design-interview/chapter19/#services","title":"Services","text":""},{"location":"booknotes/system-design-interview/chapter19/#location-service_1","title":"Location service","text":"<p>Let's focus on the database design and how user location is stored in detail for this service. </p> <p>We can use a NoSQL database to facilitate the heavy write load we have on location updates. We prioritize availability over consistency as user location data often changes and becomes stale as new updates arrive.</p> <p>We'll choose Cassandra as our database choice as it nicely fits all our requirements.</p> <p>Example row we're going to store:   * <code>user_id</code> is the partition key in order to quickly access all location updates for a particular user  * <code>timestamp</code> is the clustering key in order to store the data sorted by the time a location update is received</p> <p>We also leverage Kafka to stream location updates to various other service which need the location updates for various purposes: </p>"},{"location":"booknotes/system-design-interview/chapter19/#rendering-map","title":"Rendering map","text":"<p>Map tiles are stored at various zoom levels. At the lowest zoom level, the entire world is represented by a single 256x256 tile.</p> <p>As zoom levels increase, the number of map tiles quadruples: </p> <p>One optimization we can use is to not send the entire image information over the network, but instead represent tiles as vectors (paths &amp; polygons) and let the client render the tiles dynamically.</p> <p>This will have substantial bandwidth savings.</p>"},{"location":"booknotes/system-design-interview/chapter19/#navigation-service_1","title":"Navigation service","text":"<p>This service is responsible for finding the fastest routes: </p> <p>Let's go through each component in this sub-system.</p> <p>First, we have the geocoding service which resolves an address to a location of lat/long pair.</p> <p>Example request: <pre><code>https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA\n</code></pre></p> <p>Example response: <pre><code>{\n   \"results\" : [\n      {\n         \"formatted_address\" : \"1600 Amphitheatre Parkway, Mountain View, CA 94043, USA\",\n         \"geometry\" : {\n            \"location\" : {\n               \"lat\" : 37.4224764,\n               \"lng\" : -122.0842499\n            },\n            \"location_type\" : \"ROOFTOP\",\n            \"viewport\" : {\n               \"northeast\" : {\n                  \"lat\" : 37.4238253802915,\n                  \"lng\" : -122.0829009197085\n               },\n               \"southwest\" : {\n                  \"lat\" : 37.4211274197085,\n                  \"lng\" : -122.0855988802915\n               }\n            }\n         },\n         \"place_id\" : \"ChIJ2eUgeAK6j4ARbn5u_wAGqWA\",\n         \"plus_code\": {\n            \"compound_code\": \"CWC8+W5 Mountain View, California, United States\",\n            \"global_code\": \"849VCWC8+W5\"\n         },\n         \"types\" : [ \"street_address\" ]\n      }\n   ],\n   \"status\" : \"OK\"\n}\n</code></pre></p> <p>The route planner service computes a suggested route, optimized for travel time according to current traffic conditions.</p> <p>The shortest-path service runs a variation of the A* algorithm against the routing tiles in object storage to compute an optimal path:  * It receives the source/destination pairs, converts them to lat/long pairs and derives the geohashes from those pairs to derive the routing tiles  * The algorithm starts from the initial routing tile and starts traversing it until a good enough path is found to the destination tile </p> <p>The ETA service is called by the route planner to get estimated time based on machine learning algorithms, predicting ETA based on traffic data.</p> <p>The ranker service is responsible to rank different possible paths based on filters, passed by the user, ie flags to avoid toll roads or freeways.</p> <p>The updater service asynchronously update some of the important databases to keep them up-to-date.</p>"},{"location":"booknotes/system-design-interview/chapter19/#improvement-adaptive-eta-and-rerouting","title":"Improvement - adaptive ETA and rerouting","text":"<p>One improvement we can do is to adaptively update in-flight routes based on newly available traffic data.</p> <p>One way to implement this is to store users who are currently navigating through a route in the database by storing all the tiles they're supposed to go through.</p> <p>Data might look like this: <pre><code>user_1: r_1, r_2, r_3, \u2026, r_k\nuser_2: r_4, r_6, r_9, \u2026, r_n\nuser_3: r_2, r_8, r_9, \u2026, r_m\n...\nuser_n: r_2, r_10, r21, ..., r_l\n</code></pre></p> <p>If a traffic accident happens on some tile, we can identify all users whose path goes through that tile and re-route them.</p> <p>To reduce the amount of tiles we store in the database, we can instead store the origin routing tile and several routing tiles in different resolution levels until the destination tile is also included: <pre><code>user_1, r_1, super(r_1), super(super(r_1)), ...\n</code></pre></p> <p></p> <p>Using this, we only need to check if the final tile of a user includes the traffic accident tile to see if user is impacted.</p> <p>We can also keep track of all possible routes for a navigating user and notify them if a faster re-route is available.</p>"},{"location":"booknotes/system-design-interview/chapter19/#delivery-protocols","title":"Delivery protocols","text":"<p>We have several options, which enable us to proactively push data to clients from the server:  * Mobile push notifications don't work because payload is limited and it's not available for web apps  * WebSocket is generally a better option than long-polling as it has less compute footprint on servers  * We can also use server-sent events (SSE) but lean towards web sockets as they support bi-directional communication which can come in handy for eg a last-mile delivery feature</p>"},{"location":"booknotes/system-design-interview/chapter19/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>This is our final design: </p> <p>One additional feature we could provide is multi-stop navigation which can be sold to enterprise customers such as Uber or Lyft in order to determine optimal path for visiting a set of locations.</p>"},{"location":"booknotes/system-design-interview/chapter20/","title":"Distributed Message Queue","text":"<p>We'll be designing a distributed message queue in this chapter.</p> <p>Benefits of message queues:  * Decoupling - Eliminates tight coupling between components. Let them update separately.  * Improved scalability - Producers and consumers can be scaled independently based on traffic.  * Increased availability - If one part of the system goes down, other parts continue interacting with the queue.  * Better performance - Producers can produce messages without waiting for consumer confirmation.</p> <p>Some popular message queue implementations - Kafka, RabbitMQ, RocketMQ, Apache Pulsar, ActiveMQ, ZeroMQ.</p> <p>Strictly speaking, Kafka and Pulsar are not message queues. They are event streaming platforms. There is however a convergence of features which blurs the distinction between message queues and event streaming platforms.</p> <p>In this chapter, we'll be building a message queue with support for more advanced features such as long data retention, repeated message consumption, etc.</p>"},{"location":"booknotes/system-design-interview/chapter20/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the problem and establish design scope","text":"<p>Message queues ought to support few basic features - producers produce messages and consumers consume them. There are, however, different considerations with regards to performance, message delivery, data retention, etc.</p> <p>Here's a set of potential questions between Candidate and Interviewer:  * C: What's the format and average message size? Is it text only?  * I: Messages are text-only and usually a few KBs  * C: Can messages be repeatedly consumed?  * I: Yes, messages can be repeatedly consumed by different consumers. This is an added requirement, which traditional message queues don't support.  * C: Are messages consumed in the same order they were produced?  * I: Yes, order guarantee should be preserved. This is an added requirement, traditional message queues don't support this.  * C: What are the data retention requirements?  * I: Messages need to have a retention of two weeks. This is an added requirement.  * C: How many producers and consumers do we want to support?  * I: The more, the better.  * C: What data delivery semantic do we want to support? At-most-once, at-least-once, exactly-once?  * I: We definitely want to support at-least-once. Ideally, we can support all and make them configurable.  * C: What's the target throughput for end-to-end latency?  * I: It should support high throughput for use cases like log aggregation and low throughput for more traditional use cases.</p> <p>Functional requirements:  * Producers send messages to a message queue  * Consumers consume messages from the queue  * Messages can be consumed once or repeatedly  * Historical data can be truncated  * Message size is in the KB range  * Order of messages needs to be preserved  * Data delivery semantics is configurable - at-most-once/at-least-once/exactly-once.</p> <p>Non-functional requirements:  * High throughput or low latency. Configurable based on use-case  * Scalable - system should be distributed and support a sudden surge in message volume  * Persistent and durable - data should be persisted on disk and replicated among nodes</p> <p>Traditional message queues typically don't support data retention and don't provide ordering guarantees. This greatly simplifies the design and we'll discuss it.</p>"},{"location":"booknotes/system-design-interview/chapter20/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose high-level design and get buy-in","text":"<p>Key components of a message queue:   * Producer sends messages to a queue  * Consumer subscribes to a queue and consumes the subscribed messages  * Message queue is a service in the middle which decouples producers from consumers, letting them scale independently.  * Producer and consumer are both clients, while the message queue is the server.</p>"},{"location":"booknotes/system-design-interview/chapter20/#messaging-models","title":"Messaging models","text":"<p>The first type of messaging model is point-to-point and it's commonly found in traditional message queues:   * A message is sent to a queue and it's consumed by exactly one consumer.  * There can be multiple consumers, but a message is consumed only once.  * Once message is acknowledged as consumed, it is removed from the queue.  * There is no data retention in the point-to-point model, but there is such in our design.</p> <p>On the other hand, the publish-subscribe model is more common for event streaming platforms:   * In this model, messages are associated to a topic.  * Consumers are subscribed to a topic and they receive all messages sent to this topic.</p>"},{"location":"booknotes/system-design-interview/chapter20/#topics-partitions-and-brokers","title":"Topics, partitions and brokers","text":"<p>What if the data volume for a topic is too large? One way to scale is by splitting a topic into partitions (aka sharding):   * Messages sent to a topic are evenly distributed across partitions  * The servers that host partitions are called brokers  * Each topic operates like a queue using FIFO for message processing. Message order is preserved within a partition.  * The position of a message within the partition is called an offset.  * Each message produced is sent to a specific partition. A partition key specifies which partition a message should land in.     * Eg a <code>user_id</code> can be used as a partition key to guarantee order of messages for the same user.  * Each consumer subscribes to one or more partitions. When there are multiple consumers for the same messages, they form a consumer group.</p>"},{"location":"booknotes/system-design-interview/chapter20/#consumer-groups","title":"Consumer groups","text":"<p>Consumer groups are a set of consumers working together to consume messages from a topic:   * Messages are replicated per consumer group (not per consumer).  * Each consumer group maintains its own offset.  * Reading messages in parallel by a consumer group improves throughput but hampers the ordering guarantee.  * This can be mitigated by only allowing one consumer from a group to be subscribed to a partition.   * This means that we can't have more consumers in a group than there are partitions.</p>"},{"location":"booknotes/system-design-interview/chapter20/#high-level-architecture","title":"High-level architecture","text":"<p>  * Clients are producer and consumer. Producer pushes messages to a designated topic. Consumer group subscribes to messages from a topic.  * Brokers hold multiple partitions. A partition holds a subset of messages for a topic.  * Data storage stores messages in partitions.  * State storage keeps the consumer states.  * Metadata storage stores configuration and topic properties  * The coordination service is responsible for service discovery (which brokers are alive) and leader election (which broker is leader, responsible for assigning partitions).</p>"},{"location":"booknotes/system-design-interview/chapter20/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>In order to achieve high throughput and preserve the high data retention requirement, we made some important design choices:  * We chose an on-disk data structure which takes advantage of the properties of modern HDD and disk caching strategies of modern OS-es.  * The message data structure is immutable to avoid extra copying, which we want to avoid in a high volume/high traffic system.  * We designed our writes around batching as small I/O is an enemy of high throughput.</p>"},{"location":"booknotes/system-design-interview/chapter20/#data-storage","title":"Data storage","text":"<p>In order to find the best data store for messages, we must examine a message's properties:  * Write-heavy, read-heavy  * No update/delete operations. In traditional message queues, there is a \"delete\" operation as messages are not retained.  * Predominantly sequential read/write access pattern.</p> <p>What are our options:  * Database - not ideal as typical databases don't support well both write and read heavy systems.  * Write-ahead log (WAL) - a plain text file which only supports appending to it and is very HDD-friendly.     * We split partitions into segments to avoid maintaining a very large file.    * Old segments are read-only. Writes are accepted by latest segment only. </p> <p>WAL files are extremely efficient when used with traditional HDDs. </p> <p>There is a misconception that HDD acces is slow, but that hugely depends on the access pattern. When the access pattern is sequential (as in our case), HDDs can achieve several MB/s write/read speed which is sufficient for our needs. We also piggyback on the fact that the OS caches disk data in memory aggressively.</p>"},{"location":"booknotes/system-design-interview/chapter20/#message-data-structure","title":"Message data structure","text":"<p>It is important that the message schema is compliant between producer, queue and consumer to avoid extra copying. This allows much more efficient processing.</p> <p>Example message structure: </p> <p>The key of the message specifies which partition a message belongs to. An example mapping is <code>hash(key) % numPartitions</code>. For more flexibility, the producer can override default keys in order to control which partitions messages are distributed to.</p> <p>The message value is the payload of a message. It can be plaintext or a compressed binary block.</p> <p>Note: Message keys, unlike traditional KV stores, need not be unique. It is acceptable to have duplicate keys and for it to even be missing.</p> <p>Other message files:  * Topic - topic the message belongs to  * Partition - The ID of the partition a message belongs to  * Offset - The position of the message in a partition. A message can be located via <code>topic</code>, <code>partition</code>, <code>offset</code>.  * Timestamp - When the message is stored  * Size - the size of this message  * CRC - checksum to ensure message integrity</p> <p>Additional features such as filtering can be supported by adding additional fields.</p>"},{"location":"booknotes/system-design-interview/chapter20/#batching","title":"Batching","text":"<p>Batching is critical for the performance of our system. We apply it in the producer, consumer and message queue.</p> <p>It is critical because:  * It allows the operating system to group messages together, amortizing the cost of expensive network round trips  * Messages are written to the WAL in groups sequentially, which leads to a lot of sequential writes and disk caching.</p> <p>There is a trade-off between latency and throughput:  * High batching leads to high throughput and higher latency.   * Less batching leads to lower throughput and lower latency.</p> <p>If we need to support lower latency since the system is deployed as a traditional message queue, the system could be tuned to use a smaller batch size.</p> <p>If tuned for throughput, we might need more partitions per topic to compensate for the slower sequential disk write throughput.</p>"},{"location":"booknotes/system-design-interview/chapter20/#producer-flow","title":"Producer flow","text":"<p>If a producer wants to send a message to a partition, which broker should it connect to?</p> <p>One option is to introduce a routing layer, which route messages to the correct broker. If replication is enabled, the correct broker is the leader replica:   * Routing layer reads the replication plan from the metadata store and caches it locally.  * Producer sends a message to the routing layer.  * Message is forwarded to broker 1 who is the leader of the given partition  * Follower replicas pull the new message from the leader. Once enough confirmations are received, the leader commits the data and responds to the producer.</p> <p>The reason for having replicas is to enable fault tolerance.</p> <p>This approach works but has some drawbacks:  * Additional network hops due to the extra component  * The design doesn't enable batching messages</p> <p>To mitigate these issues, we can embed the routing layer into the producer:   * Fewer network hops lead to lower latency  * Producers can control which partition a message is routed to  * The buffer allows us to batch messages in-memory and send out larger batches in a single request, which increases throughput.</p> <p>The batch size choice is a classical trade-off between throughput and latency.    * Larger batch size leads to longer wait time before batch is committed.   * Smaller batch size leads to request being sent sooner and having lower latency but lower throughput.</p>"},{"location":"booknotes/system-design-interview/chapter20/#consumer-flow","title":"Consumer flow","text":"<p>The consumer specifies its offset in a partition and receives a chunk of messages, beginning from that offset: </p> <p>One important consideration when designing the consumer is whether to use a push or a pull model:  * Push model leads to lower latency as broker pushes messages to consumer as it receives them.    * However, if rate of consumption falls behind the rate of production, the consumer can be overwhelmed.    * It is challenging to deal with consumers with varying processing power as the broker controls the rate of consumption.  * Pull model leads to the consumer controlling the consumption rate.     * If rate of consumption is slow, consumer will not be overwhelmed and we can scale it to catch up.    * The pull model is more suitable for batch processing, because with the push model, the broker can't know how many messages a consumer can handle.     * With the pull model, on the other hand, consumers can aggressively fetch large message batches.    * The down side is the higher latency and extra network calls when there are no new messages. Latter issue can be mitigated using long polling.</p> <p>Hence, most message queues (and us) choose the pull model.   * A new consumer subscribes to topic A and joins group 1.  * The correct broker node is found by hashing the group name. This way, all consumers in a group connect to the same broker.  * Note that this consumer group coordinator is different from the coordination service (ZooKeeper).  * Coordinator confirms that the consumer has joined the group and assigns partition 2 to that consumer.  * There are different partition assignment strategies - round-robin, range, etc.  * Consumer fetches latest messages from the last offset. The state storage keeps the consumer offsets.  * Consumer processes messages and commits the offset to the broker. The order of those operations affects the message delivery semantics.</p>"},{"location":"booknotes/system-design-interview/chapter20/#consumer-rebalancing","title":"Consumer rebalancing","text":"<p>Consumer rebalancing is responsible for deciding which consumers are responsible for which partition.</p> <p>This process occurs when a consumer joins/leaves or a partition is added/removed.</p> <p>The broker, acting as a coordinator plays a huge role in orchestrating the rebalancing workflow.   * All consumers from the same group are connected to the same coordinator. The coordinator is found by hashing the group name.  * When the consumer list changes, the coordinator chooses a new leader of the group.  * The leader of the group calculates a new partition dispatch plan and reports it back to the coordinator, which broadcasts it to the other consumers.</p> <p>When the coordinator stops receiving heartbeats from the consumers in a group, a rebalancing is triggered: </p> <p>Let's explore what happens when a consumer joins a group:   * Initially, only consumer A is in the group and it consumes all partitions.  * Consumer B sends a request to join the group.  * The coordinator notifies all group members that it's time to rebalance passively - as a response to the heartbeat.  * Once all consumers rejoin the group, the coordinator chooses a leader and notifies the rest about the election result.  * The leader generates the partition dispatch plan and sends it to the coordinator. Others wait for the dispatch plan.  * Consumers start consuming from the newly assigned partitions.</p> <p>Here's what happens when a consumer leaves the group:   * Consumer A and B are in the same group  * Consumer B asks to leave the group  * When coordinator receives A's heartbeat, it informs them that it's time to rebalance.  * The rest of the steps are the same.</p> <p>The process is similar when a consumer doesn't send a heartbeat for a long time: </p>"},{"location":"booknotes/system-design-interview/chapter20/#state-storage","title":"State storage","text":"<p>The state storage stores mapping between partitions and consumers, as well as the last consumed offsets for a partition. </p> <p>Group 1's offset is at 6, meaning all previous messages are consumed. If a consumer crashes, the new consumer will continue from that message on wards.</p> <p>Data access patterns for consumer states:  * Frequent read/write operations, but low volume  * Data is updated frequently, but rarely deleted  * Random read/write  * Data consistency is important</p> <p>Given these requirements, a fast KV storage like Zookeeper is ideal.</p>"},{"location":"booknotes/system-design-interview/chapter20/#metadata-storage","title":"Metadata storage","text":"<p>The metadata storage stores configuration and topic properties - partition number, retention period, replica distribution.</p> <p>Metadata doesn't change often and volume is small, but there is a high consistency requirement. Zookeeper is a good choice for this storage.</p>"},{"location":"booknotes/system-design-interview/chapter20/#zookeeper","title":"ZooKeeper","text":"<p>Zookeeper is essential for building distributed message queues.</p> <p>It is a hierarchical key-value store, commonly used for a distributed configuration, synchronization service and naming registry (ie service discovery). </p> <p>With this change, the broker only needs to maintain data for the messages. Metadata and state storage is in Zookeeper.</p> <p>Zookeeper also helps with leader election of the broker replicas.</p>"},{"location":"booknotes/system-design-interview/chapter20/#replication","title":"Replication","text":"<p>In distributed systems, hardware issues are inevitable. We can tackle this via replication to achieve high availability.   * Each partition is replicated across multiple brokers, but there is only one leader replica.  * Producers send messages to leader replicas  * Followers pull the replicated messages from the leader  * Once enough replicas are synchronized, the leader returns acknowledgment to the producer  * Distribution of replicas for each partition is called the replica distribution plan.  * The leader for a given partition creates the replica distribution plan and saves it in Zookeeper</p>"},{"location":"booknotes/system-design-interview/chapter20/#in-sync-replicas","title":"In-sync replicas","text":"<p>One problem we need to tackle is keeping messages in-sync between the leader and the followers for a given partition.</p> <p>In-sync replicas (ISR) are replicas for a partition that stay in-sync with the leader.</p> <p>The <code>replica.lag.max.messages</code> defines how many messages can a replica be lagging behind the leader to be considered in-sync.</p> <p>  * Committed offset is 13  * Two new messages are written to the leader, but not committed yet.  * A message is committed once all replicas in the ISR have synchronized that message  * Replica 2 and 3 have fully caught up with leader, hence, they are in ISR  * Replica 4 has lagged behind, hence, is removed from ISR for now</p> <p>ISR reflects a trade-off between performance and durability.  * In order for producers not to lose messages, all replicas should be in sync before sending an acknowledgment  * But a slow replica will cause the whole partition to become unavailable</p> <p>Acknowledgment handling is configurable.</p> <p><code>ACK=all</code> means that all replicas in ISR have to sync a message. Message sending is slow, but message durability is highest. </p> <p><code>ACK=1</code> means that producer receives acknowledgment once leader receives the message. Message sending is fast, but message durability is low. </p> <p><code>ACK=0</code> means that producer sends messages without waiting for any acknowledgment from leader. Message sending is fastest, message durability is lowest. </p> <p>On the consumer side, we can connect all consumers to the leader for a partition and let them read messages from it:  * This makes for the simplest design and easiest operation  * Messages in a partition are sent to only one consumer in a group, which limits the connections to the leader replica  * The number of connections to leader replica is typically not high as long as the topic is not super hot  * We can scale a hot topic by increasing the number of partitions and consumers  * In certain scenarios, it might make sense to let a consumer lead from an ISR, eg if they're located in a separate DC</p> <p>The ISR list is maintained by the leader who tracks the lag between itself and each replica.</p>"},{"location":"booknotes/system-design-interview/chapter20/#scalability","title":"Scalability","text":"<p>Let's evaluate how we can scale different parts of the system.</p>"},{"location":"booknotes/system-design-interview/chapter20/#producer","title":"Producer","text":"<p>The producer is much smaller than the consumer. Its scalability can easily be achieved by adding/removing new producer instances.</p>"},{"location":"booknotes/system-design-interview/chapter20/#consumer","title":"Consumer","text":"<p>Consumer groups are isolated from each other. It is easy to add/remove consumer groups at will.</p> <p>Rebalancing help handle the case when consumers are added/removed from a group gracefully.</p> <p>Consumer groups are rebalancing help us achieve scalability and fault tolerance.</p>"},{"location":"booknotes/system-design-interview/chapter20/#broker","title":"Broker","text":"<p>How do brokers handle failure?   * Once a broker fails, there are still enough replicas to avoid partition data loss  * A new leader is elected and the broker coordinator redistributes partitions which were at the failed broker to existing replicas  * Existing replicas pick up the new partitions and act as followers until they're caught up with the leader and become ISR</p> <p>Additional considerations to make the broker fault-tolerant:  * The minimum number of ISRs balances latency and safety. You can fine-tune it to meet your needs.  * If all replicas of a partition are in the same node, then it's a waste of resources. Replicas should be across different brokers.  * If all replicas of a partition crash, then the data is lost forever. Spreading replicas across data centers can help, but it adds up a lot of latency. One option is to use data mirroring as a work around.</p> <p>How do we handle redistribution of replicas when a new broker is added?   * We can temporarily allow more replicas than configured, until new broker catches up  * Once it does, we can remove the partition replica which is no longer needed</p>"},{"location":"booknotes/system-design-interview/chapter20/#partition","title":"Partition","text":"<p>Whenever a new partition is added, the producer is notified and consumer rebalancing is triggered.</p> <p>In terms of data storage, we can only store new messages to the new partition vs. trying to copy all old ones: </p> <p>Decreasing the number of partitions is more involved:   * Once a partition is decommissioned, new messages are only received by remaining partitions  * The decommissioned partition isn't removed immediately as messages can still be consumed from it  * Once a pre-configured retention period passes, do we truncate the data and storage space is freed up  * During the transitional period, producers only send messages to active partitions, but consumers read from all  * Once retention period expires, consumers are rebalanced</p>"},{"location":"booknotes/system-design-interview/chapter20/#data-delivery-semantics","title":"Data delivery semantics","text":"<p>Let's discuss different delivery semantics.</p>"},{"location":"booknotes/system-design-interview/chapter20/#at-most-once","title":"At-most once","text":"<p>With this guarantee, messages are delivered not more than once and could not be delivered at all.   * Producer sends a message asynchronously to a topic. If message delivery fails, there is no retry.  * Consumer fetches message and immediately commits offset. If consumer crashes before processing the message, the message will not be processed.</p>"},{"location":"booknotes/system-design-interview/chapter20/#at-least-once","title":"At-least once","text":"<p>A message can be sent more than once and no message should be left unprocessed.   * Producer sends message with <code>ack=1</code> or <code>ack=all</code>. If there is any issue, it will keep retrying.  * Consumer fetches the message and consumes the offset only after it's done processing it.  * It is possible for a message to be delivered more than once if eg consumer crashes before committing offset but after processing it.  * This is why, this is good for use-cases where data duplication is acceptable or deduplication is possible.</p>"},{"location":"booknotes/system-design-interview/chapter20/#exactly-once","title":"Exactly once","text":"<p>Extremely costly to implement for the system, albeit it's the friendliest guarantee to users: </p>"},{"location":"booknotes/system-design-interview/chapter20/#advanced-features","title":"Advanced features","text":"<p>Let's discuss some advanced features, we might discuss in the interview.</p>"},{"location":"booknotes/system-design-interview/chapter20/#message-filtering","title":"Message filtering","text":"<p>Some consumers might want to only consume messages of a certain type within a partition.</p> <p>This can be achieved by building separate topics for each subset of messages, but this can be costly if systems have too many differing use-cases.  * It is a waste of resources to store the same message on different topics  * Producer is now tightly coupled to consumers as it changes with each new consumer requirement</p> <p>We can resolve this using message filtering.  * A naive approach would be to do the filtering on the consumer-side, but that introduces unnecessary consumer traffic  * Alternatively, messages can have tags attached to them and consumers can specify which tags they're subscribed to  * Filtering could also be done via the message payloads but that can be challenging and unsafe for encrypted/serialized messages  * For more complex mathematical formulaes, the broker could implement a grammar parser or script executor, but that can be heavyweight for the message queue </p>"},{"location":"booknotes/system-design-interview/chapter20/#delayed-messages-scheduled-messages","title":"Delayed messages &amp; scheduled messages","text":"<p>For some use-cases, we might want to delay or schedule message delivery.  For example, we might submit a payment verification check for 30m from now, which triggers the consumer to see if a payment was successful.</p> <p>This can be achieved by sending messages to temporary storage in the broker and moving the message to the partition at the right time:   * The temporary storage can be one or more special message topics  * The timing function can be achieved using dedicated delay queues or a hierarchical time wheel</p>"},{"location":"booknotes/system-design-interview/chapter20/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Additional talking points:  * Protocol of communication. Important considerations - support all use-cases and high data volume, as well as verify message integrity. Popular protocols - AMQP and Kafka protocol.  * Retry consumption - if we can't process a message immediately, we could send it to a dedicated retry topic to be attempted later.  * Historical data archive - old messages can be backed up in high-capacity storages such as HDFS or object storage (eg S3).</p>"},{"location":"booknotes/system-design-interview/chapter21/","title":"Metrics Monitoring and Alerting System","text":"<p>This chapter focuses on designing a highly scalable metrics monitoring and alerting system, which is critical for ensuring high availability and reliability.</p>"},{"location":"booknotes/system-design-interview/chapter21/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>A metrics monitoring system can mean a lot of different things - eg you don't want to design a logs aggregation system, when the interviewer is interested in infra metrics only.</p> <p>Let's try to understand the problem first:  * C: Who are we building the system for? An in-house monitoring system for a big tech company or a SaaS like DataDog?  * I: We are building for internal use only.  * C: Which metrics do we want to collect?  * I: Operational system metrics - CPU load, Memory, Data disk space. But also high-level metrics like requests per second. Business metrics are not in scope.  * C: What is the scale of the infrastructure we're monitoring?  * I: 100mil daily active users, 1000 server pools, 100 machines per pool  * C: How long should we keep the data?  * I: Let's assume 1y retention.  * C: May we reduce metrics data resolution for long-term storage?  * I: Keep newly received metrics for 7 days. Roll them up to 1m resolution for next 30 days. Further roll them up to 1h resolution after 30 days.  * C: What are the supported alert channels?  * I: Email, phone, PagerDuty or webhooks.  * C: Do we need to collect logs such as error or access logs?  * I: No  * C: Do we need to support distributed system tracing?  * I: No</p>"},{"location":"booknotes/system-design-interview/chapter21/#high-level-requirements-and-assumptions","title":"High-level requirements and assumptions","text":"<p>The infrastructure being monitored is large-scale:  * 100mil DAU  * 1000 server pools * 100 machines * ~100 metrics per machine -&gt; ~10mil metrics  * 1-year data retention  * Data retention policy - raw for 7d, 1-minute resolution for 30d, 1h resolution for 1y</p> <p>A variety of metrics can be monitored:  * CPU load  * Request count  * Memory usage  * Message count in message queues</p>"},{"location":"booknotes/system-design-interview/chapter21/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Scalability - System should be scalable to accommodate more metrics and alerts</li> <li>Low latency - System needs to have low query latency for dashboards and alerts</li> <li>Reliability - System should be highly reliable to avoid missing critical alerts</li> <li>Flexibility - System should be able to easily integrate new technologies in the future</li> </ul> <p>What requirements are out of scope?  * Log monitoring - the ELK stack is very popular for this use-case  * Distributed system tracing - this refers to collecting data about a request lifecycle as it flows through multiple services within the system</p>"},{"location":"booknotes/system-design-interview/chapter21/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"booknotes/system-design-interview/chapter21/#fundamentals","title":"Fundamentals","text":"<p>There are five core components involved in a metrics monitoring and alerting system:   * Data collection - collect metrics data from different sources  * Data transmission - transfer data from sources to the metrics monitoring system  * Data storage - organize and store incoming data  * Alerting - Analyze incoming data, detect anomalies and generate alerts  * Visualization - Present data in graphs, charts, etc</p>"},{"location":"booknotes/system-design-interview/chapter21/#data-model","title":"Data model","text":"<p>Metrics data is usually recorded as a time-series, which contains a set of values with timestamps. The series can be identified by name and an optional set of tags.</p> <p>Example 1 - What is the CPU load on production server instance i631 at 20:00? </p> <p>The data can be identified by the following table: </p> <p>The time series is identified by the metric name, labels and a single point in at a specific time.</p> <p>Example 2 - What is the average CPU load across all web servers in the us-west region for the last 10min? <pre><code>CPU.load host=webserver01,region=us-west 1613707265 50\n\nCPU.load host=webserver01,region=us-west 1613707265 62\n\nCPU.load host=webserver02,region=us-west 1613707265 43\n\nCPU.load host=webserver02,region=us-west 1613707265 53\n\n...\n\nCPU.load host=webserver01,region=us-west 1613707265 76\n\nCPU.load host=webserver01,region=us-west 1613707265 83\n</code></pre></p> <p>This is an example data we might pull from storage to answer that question. The average CPU load can be calculated by averaging the values in the last column of the rows.</p> <p>The format shown above is called the line protocol and is used by many popular monitoring software in the market - eg Prometheus, OpenTSDB.</p> <p>What every time series consists of: </p> <p>A good way to visualize how data looks like:   * The x axis is the time  * the y axis is the dimension you're querying - eg metric name, tag, etc.</p> <p>The data access pattern is write-heavy and spiky reads as we collect a lot of metrics, but they are infrequently accessed, although in bursts when eg there are ongoing incidents.</p> <p>The data storage system is the heart of this design.   * It is not recommended to use a general-purpose database for this problem, although you could achieve good scale \\w expert-level tuning.  * Using a NoSQL database can work in theory, but it is hard to devise a scalable schema for effectively storing and querying time-series data.</p> <p>There are many databases, specifically tailored for storing time-series data. Many of them support custom query interfaces which allow for effective querying of time-series data.  * OpenTSDB is a distributed time-series database, but it is based on Hadoop and HBase. If you don't have that infrastructure provisioned, it would be hard to use this tech.  * Twitter uses MetricsDB, while Amazon offers Timestream.  * The two most popular time-series databases are InfluxDB and Prometheus.   * They are designed to store large volumes of time-series data. Both of them are based on in-memory cache + on-disk storage.</p> <p>Example scale of InfluxDB - more than 250k writes per second when provisioned with 8 cores and 32gb RAM: </p> <p>It is not expected for you to understand the internals of a metrics database as it is niche knowledge. You might be asked only if you've mentioned it on your resume.</p> <p>For the purposes of the interview, it is sufficient to understand that metrics are time-series data and to be aware of popular time-series databases, like InfluxDB.</p> <p>One nice feature of time-series databases is the efficient aggregation and analysis of large amounts of time-series data by labels. InfluxDB, for example, builds indexes for each label.</p> <p>It is critical, however, to keep the cardinality of labels low - ie, not using too many unique labels.</p>"},{"location":"booknotes/system-design-interview/chapter21/#high-level-design","title":"High-level Design","text":"<p>  * Metrics source - can be application servers, SQL databases, message queues, etc.  * Metrics collector - Gathers metrics data and writes to time-series database  * Time-series database - stores metrics as time-series. Provides a custom query interface for analyzing large amounts of metrics.  * Query service - Makes it easy to query and retrieve data from the time-series DB. Could be replaced entirely by the DB's interface if it's sufficiently powerful.  * Alerting system - Sends alert notifications to various alerting destinations.  * Visualization system - Shows metrics in the form of graphs/charts.</p>"},{"location":"booknotes/system-design-interview/chapter21/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's deep dive into several of the more interesting parts of the system.</p>"},{"location":"booknotes/system-design-interview/chapter21/#metrics-collection","title":"Metrics collection","text":"<p>For metrics collection, occasional data loss is not critical. It's acceptable for clients to fire and forget. </p> <p>There are two ways to implement metrics collection - pull or push.</p> <p>Here's how the pull model might look like: </p> <p>For this solution, the metrics collector needs to maintain an up-to-date list of services and metrics endpoints. We can use Zookeeper or etcd for that purpose - service discovery.</p> <p>Service discovery contains contains configuration rules about when and where to collect metrics from: </p> <p>Here's a detailed explanation of the metrics collection flow:   * Metrics collector fetches configuration metadata from service discovery. This includes pulling interval, IP addresses, timeout &amp; retry params.  * Metrics collector pulls metrics data via a pre-defined http endpoint (eg <code>/metrics</code>). This is typically done by a client library.  * Alternatively, the metrics collector can register a change event notification with the service discovery to be notified once the service endpoint changes.  * Another option is for the metrics collector to periodically poll for metrics endpoint configuration changes.</p> <p>At our scale, a single metrics collector is not enough. There must be multiple instances.  However, there must also be some kind of synchronization among them so that two collectors don't collect the same metrics twice.</p> <p>One solution for this is to position collectors and servers on a consistent hash ring and associate a set of servers with a single collector only: </p> <p>With the push model, on the other hand, services push their metrics to the metrics collector proactively: </p> <p>In this approach, typically a collection agent is installed alongside service instances.  The agent collects metrics from the server and pushes them to the metrics collector. </p> <p>With this model, we can potentially aggregate metrics before sending them to the collector, which reduces the volume of data processed by the collector.</p> <p>On the flip side, metrics collector can reject push requests as it can't handle the load.  It is important, hence, to add the collector to an auto-scaling group behind a load balancer.</p> <p>so which one is better? There are trade-offs between both approaches and different systems use different approaches:  * Prometheus uses a pull architecture  * Amazon Cloud Watch and Graphite use a push architecture</p> <p>Here are some of the main differences between push and pull: |                                        | Pull                                                                                                                                                                                                    | Push                                                                                                                                                                                                                                    | |----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Easy debugging                         | The /metrics endpoint on application servers used for pulling metrics can be used to view metrics at any time. You can even do this on your laptop. Pull wins.                                          | If the metrics collector doesn\u2019t receive metrics, the problem might be caused by network issues.                                                                                                                                        | | Health check                           | If an application server doesn\u2019t respond to the pull, you can quickly figure out if an application server is down. Pull wins.                                                                           | If the metrics collector doesn\u2019t receive metrics, the problem might be caused by network issues.                                                                                                                                        | | Short-lived jobs                       |                                                                                                                                                                                                         | Some of the batch jobs might be short-lived and don\u2019t last long enough to be pulled. Push wins. This can be fixed by introducing push gateways for the pull model [22].                                                                 | | Firewall or complicated network setups | Having servers pulling metrics requires all metric endpoints to be reachable. This is potentially problematic in multiple data center setups. It might require a more elaborate network infrastructure. | If the metrics collector is set up with a load balancer and an auto-scaling group, it is possible to receive data from anywhere. Push wins.                                                                                             | | Performance                            | Pull methods typically use TCP.                                                                                                                                                                         | Push methods typically use UDP. This means the push method provides lower-latency transports of metrics. The counterargument here is that the effort of establishing a TCP connection is small compared to sending the metrics payload. | | Data authenticity                      | Application servers to collect metrics from are defined in config files in advance. Metrics gathered from those servers are guaranteed to be authentic.                                                 | Any kind of client can push metrics to the metrics collector. This can be fixed by whitelisting servers from which to accept metrics, or by requiring authentication.                                                                   |</p> <p>There is no clear winner. A large organization probably needs to support both. There might not be a way to install a push agent in the first place.</p>"},{"location":"booknotes/system-design-interview/chapter21/#scale-the-metrics-transmission-pipeline","title":"Scale the metrics transmission pipeline","text":"<p>The metrics collector is provisioned in an auto-scaling group, regardless if we use the push or pull model.</p> <p>There is a chance of data loss if the time-series DB is down, however. To mitigate this, we'll provision a queuing mechanism:   * Metrics collectors push metrics data into kafka  * Consumers or stream processing services such as Apache Storm, Flink or Spark process the data and push it to the time-series DB</p> <p>This approach has several advantages:  * Kafka is used as a highly-reliable and scalable distributed message platform  * It decouples data collection and data processing from one another  * It can prevent data loss by retaining the data in Kafka</p> <p>Kafka can be configured with one partition per metric name, so that consumers can aggregate data by metric names. To scale this, we can further partition by tags/labels and categorize/prioritize metrics to be collected first. </p> <p>The main downside of using Kafka for this problem is the maintenance/operation overhead. An alternative is to use a large-scale ingestion system like Gorilla. It can be argued that using that would be as scalable as using Kafka for queuing.</p>"},{"location":"booknotes/system-design-interview/chapter21/#where-aggregations-can-happen","title":"Where aggregations can happen","text":"<p>Metrics can be aggregated at several places. There are trade-offs between different choices:  * Collection agent - client-side collection agent only supports simple aggregation logic. Eg collect a counter for 1m and send it to the metrics collector.  * Ingestion pipeline - To aggregate data before writing to the DB, we need a stream processing engine like Flink. This reduces write volume, but we lose data precision as we don't store raw data.  * Query side - We can aggregate data when we run queries via our visualization system. There is no data loss, but queries can be slow due to a lot of data processing.</p>"},{"location":"booknotes/system-design-interview/chapter21/#query-service","title":"Query Service","text":"<p>Having a separate query service from the time-series DB decouples the visualization and alerting system from the database, which enables us to decouple the DB from clients and change it at will.</p> <p>We can add a Cache layer here to reduce the load to the time-series database: </p> <p>We can also avoid adding a query service altogether as most visualization and alerting systems have powerful plugins to integrate with most time-series databases. With a well-chosen time-series DB, we might not need to introduce our own caching layer as well.</p> <p>Most time-series DBs don't support SQL simply because it is ineffective for querying time-series data. Here's an example SQL query for computing an exponential moving average: <pre><code>select id,\n       temp,\n       avg(temp) over (partition by group_nr order by time_read) as rolling_avg\nfrom (\n  select id,\n         temp,\n         time_read,\n         interval_group,\n         id - row_number() over (partition by interval_group order by time_read) as group_nr\n  from (\n    select id,\n    time_read,\n    \"epoch\"::timestamp + \"900 seconds\"::interval * (extract(epoch from time_read)::int4 / 900) as interval_group,\n    temp\n    from readings\n  ) t1\n) t2\norder by time_read;\n</code></pre></p> <p>Here's the same query in Flux - query language used in InfluxDB: <pre><code>from(db:\"telegraf\")\n  |&gt; range(start:-1h)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"foo\")\n  |&gt; exponentialMovingAverage(size:-10s)\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter21/#storage-layer","title":"Storage layer","text":"<p>It is important to choose the time-series database carefully.</p> <p>According to research published by Facebook, ~85% of queries to the operational store were for data from the past 26h.</p> <p>If we choose a database, which harnesses this property, it could have significant impact on system performance. InfluxDB is one such option.</p> <p>Regardless of the database we choose, there are some optimizations we might employ.</p> <p>Data encoding and compression can significantly reduce the size of data. Those features are usually built into a good time-series database. </p> <p>In the above example, instead of storing full timestamps, we can store timestamp deltas.</p> <p>Another technique we can employ is down-sampling - converting high-resolution data to low-resolution in order to reduce disk usage.</p> <p>We can use that for old data and make the rules configurable by data scientists, eg:  * 7d - no down-sampling  * 30d - down-sample to 1min  * 1y - down-sample to 1h</p> <p>For example, here's a 10-second resolution metrics table: | metric | timestamp            | hostname | Metric_value | |--------|----------------------|----------|--------------| | cpu    | 2021-10-24T19:00:00Z | host-a   | 10           | | cpu    | 2021-10-24T19:00:10Z | host-a   | 16           | | cpu    | 2021-10-24T19:00:20Z | host-a   | 20           | | cpu    | 2021-10-24T19:00:30Z | host-a   | 30           | | cpu    | 2021-10-24T19:00:40Z | host-a   | 20           | | cpu    | 2021-10-24T19:00:50Z | host-a   | 30           |</p> <p>down-sampled to 30-second resolution: | metric | timestamp            | hostname | Metric_value (avg) | |--------|----------------------|----------|--------------------| | cpu    | 2021-10-24T19:00:00Z | host-a   | 19                 | | cpu    | 2021-10-24T19:00:30Z | host-a   | 25                 |</p> <p>Finally, we can also use cold storage to use old data, which is no longer used. The financial cost for cold storage is much lower.</p>"},{"location":"booknotes/system-design-interview/chapter21/#alerting-system","title":"Alerting system","text":"<p>Configuration is loaded to cache servers. Rules are typically defined in YAML format. Here's an example: <pre><code>- name: instance_down\n  rules:\n\n  # Alert for any instance that is unreachable for &gt;5 minutes.\n  - alert: instance_down\n    expr: up == 0\n    for: 5m\n    labels:\n      severity: page\n</code></pre></p> <p>The alert manager fetches alert configurations from cache. Based on configuration rules, it also calls the query service at a predefined interval. If a rule is met, an alert event is created.</p> <p>Other responsibilities of the alert manager are:  * Filtering, merging and deduplicating alerts. Eg if an alert of a single instance is triggered multiple times, only one alert event is generated.  * Access control - it is important to restrict alert-management operations to certain individuals only  * Retry - the manager ensures that the alert is propagated at least once.</p> <p>The alert store is a key-value database, like Cassandra, which keeps the state of all alerts. It ensures a notification is sent at least once. Once an alert is triggered, it is published to Kafka.</p> <p>Finally, alert consumers pull alerts data from Kafka and send notifications over to different channels - Email, text message, PagerDuty, webhooks.</p> <p>In the real-world, there are many off-the-shelf solutions for alerting systems. It is difficult to justify building your own system in-house.</p>"},{"location":"booknotes/system-design-interview/chapter21/#visualization-system","title":"Visualization system","text":"<p>The visualization system shows metrics and alerts over a time period. Here's an dashboard built with Grafana: </p> <p>A high-quality visualization system is very hard to build. It is hard to justify not using an off-the-shelf solution like Grafana.</p>"},{"location":"booknotes/system-design-interview/chapter21/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Here's our final design: </p>"},{"location":"booknotes/system-design-interview/chapter22/","title":"Ad Click Event Aggregation","text":"<p>Digital advertising is a big industry with the rise of Facebook, YouTube, TikTok, etc.</p> <p>Hence, tracking ad click events is important. In this chapter, we explore how to design an ad click event aggregation system at Facebook/Google scale.</p> <p>Digital advertising has a process called real-time bidding (RTB), where digital advertising inventory is bought and sold: </p> <p>Speed of RTB is important as it usually occurs within a second. Data accuracy is also very important as it impacts how much money advertisers pay.</p> <p>Based on ad click event aggregations, advertisers can make decisions such as adjust target audience and keywords.</p>"},{"location":"booknotes/system-design-interview/chapter22/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: What is the format of the input data?</li> <li>I: 1bil ad clicks per day and 2mil ads in total. Number of ad-click events grows 30% year-over-year.</li> <li>C: What are some of the most important queries our system needs to support?</li> <li>I: Top queries to take into consideration:</li> <li>Return number of click events for ad X in last Y minutes</li> <li>Return top 100 most clicked ads in the past 1min. Both parameters should be configurable. Aggregation occurs every minute.</li> <li>Support data filtering by <code>ip</code>, <code>user_id</code>, <code>country</code> for the above queries</li> <li>C: Do we need to worry about edge cases? Some of the ones I can think of:</li> <li>There might be events that arrive later than expected</li> <li>There might be duplicate events</li> <li>Different parts of the system might be down, so we need to consider system recovery</li> <li>I: That's a good list, take those into consideration</li> <li>C: What is the latency requirement?</li> <li>I: A few minutes of e2e latency for ad click aggregation. For RTB, it is less than a second. It is ok to have that latency for ad click aggregation as those are usually used for billing and reporting.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter22/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Aggregate the number of clicks of <code>ad_id</code> in the last Y minutes</li> <li>Return top 100 most clicked <code>ad_id</code> every minute</li> <li>Support aggregation filtering by different attributes</li> <li>Dataset volume is at Facebook or Google scale</li> </ul>"},{"location":"booknotes/system-design-interview/chapter22/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Correctness of the aggregation result is important as it's used for RTB and ads billing</li> <li>Properly handle delayed or duplicate events</li> <li>Robustness - system should be resilient to partial failures</li> <li>Latency - a few minutes of e2e latency at most</li> </ul>"},{"location":"booknotes/system-design-interview/chapter22/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>1bil DAU</li> <li>Assuming user clicks 1 ad per day -&gt; 1bil ad clicks per day</li> <li>Ad click QPS = 10,000</li> <li>Peak QPS is 5 times the number = 50,000</li> <li>A single ad click occupies 0.1KB storage. Daily storage requirement is 100gb</li> <li>Monthly storage = 3tb</li> </ul>"},{"location":"booknotes/system-design-interview/chapter22/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>In this section, we discuss query API design, data model and high-level design.</p>"},{"location":"booknotes/system-design-interview/chapter22/#query-api-design","title":"Query API Design","text":"<p>The API is a contract between the client and the server. In our case, the client is the dashboard user - data scientist/analyst, advertiser, etc.</p> <p>Here's our functional requirements:  * Aggregate the number of clicks of <code>ad_id</code> in the last Y minutes  * Return top N most clicked <code>ad_id</code> in the last M minutes  * Support aggregation filtering by different attributes</p> <p>We need two endpoints to achieve those requirements. Filtering can be done via query parameters on one of them.</p> <p>Aggregate number of clicks of ad_id in the last M minutes: <pre><code>GET /v1/ads/{:ad_id}/aggregated_count\n</code></pre></p> <p>Query parameters:  * from - start minute. Default is now - 1 min  * to - end minute. Default is now  * filter - identifier for different filtering strategies. Eg 001 means \"non-US clicks\".</p> <p>Response:  * ad_id - ad identifier  * count - aggregated count between start and end minutes</p> <p>Return top N most clicked ad_ids in the last M minutes <pre><code>GET /v1/ads/popular_ads\n</code></pre></p> <p>Query parameters:  * count - top N most clicked ads  * window - aggregation window size in minutes  * filter - identifier for different filtering strategies</p> <p>Response:  * list of ad_ids</p>"},{"location":"booknotes/system-design-interview/chapter22/#data-model","title":"Data model","text":"<p>In our system, we have raw and aggregated data.</p> <p>Raw data looks like this: <pre><code>[AdClickEvent] ad001, 2021-01-01 00:00:01, user 1, 207.148.22.22, USA\n</code></pre></p> <p>Here's an example in a structured format: | ad_id | click_timestamp     | user  | ip            | country | |-------|---------------------|-------|---------------|---------| | ad001 | 2021-01-01 00:00:01 | user1 | 207.148.22.22 | USA     | | ad001 | 2021-01-01 00:00:02 | user1 | 207.148.22.22 | USA     | | ad002 | 2021-01-01 00:00:02 | user2 | 209.153.56.11 | USA     |</p> <p>Here's the aggregated version: | ad_id | click_minute | filter_id | count | |-------|--------------|-----------|-------| | ad001 | 202101010000 | 0012      | 2     | | ad001 | 202101010000 | 0023      | 3     | | ad001 | 202101010001 | 0012      | 1     | | ad001 | 202101010001 | 0023      | 6     |</p> <p>The <code>filter_id</code> helps us achieve our filtering requirements. | filter_id | region | IP        | user_id | |-----------|--------|-----------|---------| | 0012      | US     | *         | *       | | 0013      | *      | 123.1.2.3 | *       |</p> <p>To support quickly returning top N most clicked ads in the last M minutes, we'll also maintain this structure: | most_clicked_ads   |           |                                                  | |--------------------|-----------|--------------------------------------------------| | window_size        | integer   | The aggregation window size (M) in minutes       | | update_time_minute | timestamp | Last updated timestamp (in 1-minute granularity) | | most_clicked_ads   | array     | List of ad IDs in JSON format.                   |</p> <p>What are some pros and cons between storing raw data and storing aggregated data?  * Raw data enables using the full data set and supports data filtering and recalculation  * On the other hand, aggregated data allows us to have a smaller data set and a faster query  * Raw data means having a larger data store and a slower query  * Aggregated data, however, is derived data, hence there is some data loss.</p> <p>In our design, we'll use a combination of both approaches:  * It's a good idea to keep the raw data around for debugging. If there is some bug in aggregation, we can discover the bug and backfill.  * Aggregated data should be stored as well for faster query performance.  * Raw data can be stored in cold storage to avoid extra storage costs.</p> <p>When it comes to the database, there are several factors to take into consideration:  * What does the data look like? Is it relational, document or blob?  * Is the workload read-heavy, write-heavy or both?  * Are transactions needed?  * Do the queries rely on OLAP functions like SUM and COUNT?</p> <p>For the raw data, we can see that the average QPS is 10k and peak QPS is 50k, so the system is write-heavy. On the other hand, read traffic is low as raw data is mostly used as backup if anything goes wrong.</p> <p>Relational databases can do the job, but it can be challenging to scale the writes.  Alternatively, we can use Cassandra or InfluxDB which have better native support for heavy write loads.</p> <p>Another option is to use Amazon S3 with a columnar data format like ORC, Parquet or AVRO. Since this setup is unfamiliar, we'll stick to Cassandra.</p> <p>For aggregated data, the workload is both read and write heavy as aggregated data is constantly queried for dashboards and alerts. It is also write-heavy as data is aggregated and written every minute by the aggregation service.  Hence, we'll use the same data store (Cassandra) here as well.</p>"},{"location":"booknotes/system-design-interview/chapter22/#high-level-design","title":"High-level design","text":"<p>Here's how our system looks like: </p> <p>Data flows as an unbounded data stream on both inputs and outputs.</p> <p>In order to avoid having a synchronous sink, where a consumer crashing can cause the whole system to stall,  we'll leverage asynchronous processing using message queues (Kafka) to decouple consumers and producers. </p> <p>The first message queue stores ad click event data: | ad_id | click_timestamp | user_id | ip | country | |-------|-----------------|---------|----|---------|</p> <p>The second message queue contains ad click counts, aggregated per-minute: | ad_id | click_minute | count | |-------|--------------|-------|</p> <p>As well as top N clicked ads aggregated per minute: | update_time_minute | most_clicked_ads | |--------------------|------------------|</p> <p>The second message queue is there in order to achieve end to end exactly-once atomic commit semantics: </p> <p>For the aggregation service, using the MapReduce framework is a good option:  </p> <p>Each node is responsible for one single task and it sends the processing result to the downstream node.</p> <p>The map node is responsible for reading from the data source, then filtering and transforming the data.</p> <p>For example, the map node can allocate data across different aggregation nodes based on the <code>ad_id</code>: </p> <p>Alternatively, we can distribute ads across Kafka partitions and let the aggregation nodes subscribe directly within a consumer group. However, the mapping node enables us to sanitize or transform the data before subsequent processing.</p> <p>Another reason might be that we don't have control over how data is produced,  so events related to the same <code>ad_id</code> might go on different partitions.</p> <p>The aggregate node counts ad click events by <code>ad_id</code> in-memory every minute.</p> <p>The reduce node collects aggregated results from aggregate node and produces the final result: </p> <p>This DAG model uses the MapReduce paradigm. It takes big data and leverages parallel distributed computing to turn it into regular-sized data.</p> <p>In the DAG model, intermediate data is stored in-memory and different nodes communicate with each other using TCP or shared memory.</p> <p>Let's explore how this model can now help us to achieve our various use-cases.</p> <p>Use-case 1 - aggregate the number of clicks:   * Ads are partitioned using <code>ad_id % 3</code></p> <p>Use-case 2 - return top N most clicked ads:   * In this case, we're aggregating the top 3 ads, but this can be extended to top N ads easily  * Each node maintains a heap data structure for fast retrieval of top N ads</p> <p>Use-case 3 - data filtering: To support fast data filtering, we can predefine filtering criterias and pre-aggregate based on it: | ad_id | click_minute | country | count | |-------|--------------|---------|-------| | ad001 | 202101010001 | USA     | 100   | | ad001 | 202101010001 | GPB     | 200   | | ad001 | 202101010001 | others  | 3000  | | ad002 | 202101010001 | USA     | 10    | | ad002 | 202101010001 | GPB     | 25    | | ad002 | 202101010001 | others  | 12    |</p> <p>This technique is called the star schema and is widely used in data warehouses. The filtering fields are called dimensions.</p> <p>This approach has the following benefits:  * Simple to undertand and build  * Current aggregation service can be reused to create more dimensions in the star schema.  * Accessing data based on filtering criteria is fast as results are pre-calculated</p> <p>A limitation of this approach is that it creates many more buckets and records, especially when we have lots of filtering criterias.</p>"},{"location":"booknotes/system-design-interview/chapter22/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's dive deeper into some of the more interesting topics.</p>"},{"location":"booknotes/system-design-interview/chapter22/#streaming-vs-batching","title":"Streaming vs. Batching","text":"<p>The high-level architecture we proposed is a type of stream processing system.  Here's a comparison between three types of systems: |                         | Services (Online system)      | Batch system (offline system)                          | Streaming system (near real-time system)     | |-------------------------|-------------------------------|--------------------------------------------------------|----------------------------------------------| | Responsiveness          | Respond to the client quickly | No response to the client needed                       | No response to the client needed             | | Input                   | User requests                 | Bounded input with finite size. A large amount of data | Input has no boundary (infinite streams)     | | Output                  | Responses to clients          | Materialized views, aggregated metrics, etc.           | Materialized views, aggregated metrics, etc. | | Performance measurement | Availability, latency         | Throughput                                             | Throughput, latency                          | | Example                 | Online shopping               | MapReduce                                              | Flink [13]                                   |</p> <p>In our design, we used a mixture of batching and streaming. </p> <p>We used streaming for processing data as it arrives and generates aggregated results in near real-time. We used batching, on the other hand, for historical data backup.</p> <p>A system which contains two processing paths - batch and streaming, simultaneously, this architecture is called lambda. A disadvantage is that you have two processing paths with two different codebases to maintain.</p> <p>Kappa is an alternative architecture, which combines batch and stream processing in one processing path. The key idea is to use a single stream processing engine.</p> <p>Lambda architecture: </p> <p>Kappa architecture: </p> <p>Our high-level design uses Kappa architecture as reprocessing of historical data also goes through the aggregation service.</p> <p>Whenever we have to recalculate aggregated data due to eg a major bug in aggregation logic, we can recalculate the aggregation from the raw data we store.  * Recalculation service retrieves data from raw storage. This is a batch job.  * Retrieved data is sent to a dedicated aggregation service, so that the real-time processing aggregation service is not impacted.  * Aggregated results are sent to the second message queue, after which we update the results in the aggregation database. </p>"},{"location":"booknotes/system-design-interview/chapter22/#time","title":"Time","text":"<p>We need a timestamp to perform aggregation. It can be generated in two places:  * event time - when ad click occurs  * Processing time - system time when the server processes the event</p> <p>Due to the usage of async processing (message queues) and network delays, there can be significant difference between event time and processing time.  * If we use processing time, aggregation results can be inaccurate  * If we use event time, we have to deal with delayed events</p> <p>There is no perfect solution, we need to consider trade-offs: |                 | Pros                                  | Cons                                                                                 | |-----------------|---------------------------------------|--------------------------------------------------------------------------------------| | Event time      | Aggregation results are more accurate | Clients might have the wrong time or timestamp might be generated by malicious users | | Processing time | Server timestamp is more reliable     | The timestamp is not accurate if event is late                                       |</p> <p>Since data accuracy is important, we'll use the event time for aggregation.</p> <p>To mitigate the issue of delayed events, a technique called \"watermark\" can be leveraged.</p> <p>In the example below, event 2 misses the window where it needs to be aggregated: </p> <p>However, if we purposefully extend the aggregation window, we can reduce the likelihood of missed events. The extended part of a window is called a \"watermark\":   * Short watermark increases likelihood of missed events, but reduces latency  * Longer watermark reduces likelihood of missed events, but increases latency</p> <p>There is always likelihood of missed events, regardless of the watermark's size. But there is no use in optimizing for such low-probability events.</p> <p>We can instead resolve such inconsistencies by doing end-of-day reconciliation.</p>"},{"location":"booknotes/system-design-interview/chapter22/#aggregation-window","title":"Aggregation window","text":"<p>There are four types of window functions:  * Tumbling (fixed) window  * Hopping window  * Sliding window  * Session window</p> <p>In our design, we leverage a tumbling window for ad click aggregations: </p> <p>As well as a sliding window for the top N clicked ads in M minutes aggregation: </p>"},{"location":"booknotes/system-design-interview/chapter22/#delivery-guarantees","title":"Delivery guarantees","text":"<p>Since the data we're aggregating is going to be used for billing, data accuracy is a priority.</p> <p>Hence, we need to discuss:  * How to avoid processing duplicate events  * How to ensure all events are processed</p> <p>There are three delivery guarantees we can use - at-most-once, at-least-once and exactly once.</p> <p>In most circumstances, at-least-once is sufficient when a small amount of duplicates is acceptable. This is not the case for our system, though, as a difference in small percent can result in millions of dollars of discrepancy. Hence, we'll need to use exactly-once delivery semantics.</p>"},{"location":"booknotes/system-design-interview/chapter22/#data-deduplication","title":"Data deduplication","text":"<p>One of the most common data quality issues is duplicated data.</p> <p>It can come from a wide range of sources:  * Client-side - a client might resend the same event multiple times. Duplicated events sent with malicious intent are best handled by a risk engine.  * Server outage - An aggregation service node goes down in the middle of aggregation and the upstream service hasn't received an acknowledgment so event is resent.</p> <p>Here's an example of data duplication occurring due to failure to acknowledge an event on the last hop: </p> <p>In this example, offset 100 will be processed and sent downstream multiple times.</p> <p>One option to try and mitigate this is to store the last seen offset in HDFS/S3, but this risks the result never reaching downstream: </p> <p>Finally, we can store the offset while interacting with downstream atomically. To achieve this, we need to implement a distributed transaction: </p> <p>Personal side-note: Alternatively, if the downstream system handles the aggregation result idempotently, there is no need for a distributed transaction.</p>"},{"location":"booknotes/system-design-interview/chapter22/#scale-the-system","title":"Scale the system","text":"<p>Let's discuss how we scale the system as it grows.</p> <p>We have three independent components - message queue, aggregation service and database. Since they are decoupled, we can scale them independently.</p> <p>How do we scale the message queue:  * We don't put a limit on producers, so they can be scaled easily  * Consumers can be scaled by assigning them to consumer groups and increasing the number of consumers.  * For this to work, we also need to ensure there are enough partitions created preemptively  * Also, consumer rebalancing can take a while when there are thousands of consumers so it is recommended to do it off peak hours  * We could also consider partitioning the topic by geography, eg <code>topic_na</code>, <code>topic_eu</code>, etc. </p> <p>How do we scale the aggregation service:   * The map-reduce nodes can easily be scaled by adding more nodes  * The throughput of the aggregation service can be scaled by by utilising multi-threading  * Alternatively, we can leverage resource providers such as Apache YARN to utilize multi-processing  * Option 1 is easier, but option 2 is more widely used in practice as it's more scalable  * Here's the multi-threading example: </p> <p>How do we scale the database:  * If we use Cassandra, it natively supports horizontal scaling utilizing consistent hashing  * If a new node is added to the cluster, data automatically gets rebalanced across all (virtual) nodes  * With this approach, no manual (re)sharding is required </p> <p>Another scalability issue to consider is the hotspot issue - what if an ad is more popular and gets more attention than others?   * In the above example, aggregation service nodes can apply for extra resources via the resource manager  * The resource manager allocates more resources, so the original node isn't overloaded  * The original node splits the events into 3 groups and each of the aggregation nodes handles 100 events  * Result is written back to the original aggregation node</p> <p>Alternative, more sophisticated ways to handle the hotspot problem:  * Global-Local Aggregation  * Split Distinct Aggregation</p>"},{"location":"booknotes/system-design-interview/chapter22/#fault-tolerance","title":"Fault Tolerance","text":"<p>Within the aggregation nodes, we are processing data in-memory. If a node goes down, the processed data is lost.</p> <p>We can leverage consumer offsets in kafka to continue from where we left off once another node picks up the slack. However, there is additional intermediary state we need to maintain, as we're aggregating the top N ads in M minutes.</p> <p>We can make snapshots at a particular minute for the on-going aggregation: </p> <p>If a node goes down, the new node can read the latest committed consumer offset, as well as the latest snapshot to continue the job: </p>"},{"location":"booknotes/system-design-interview/chapter22/#data-monitoring-and-correctness","title":"Data monitoring and correctness","text":"<p>As the data we're aggregating is critical as it's used for billing, it is very important to have rigorous monitoring in place in order to ensure correctness.</p> <p>Some metrics we might want to monitor:  * Latency - Timestamps of different events can be tracked in order to understand the e2e latency of the system  * Message queue size - If there is a sudden increase in queue size, we need to add more aggregation nodes. As Kafka is implemented via a distributed commit log, we need to keep track of records-lag metrics instead.  * System resources on aggregation nodes - CPU, disk, JVM, etc.</p> <p>We also need to implement a reconciliation flow which is a batch job, running at the end of the day.  It calculates the aggregated results from the raw data and compares them against the actual data stored in the aggregation database: </p>"},{"location":"booknotes/system-design-interview/chapter22/#alternative-design","title":"Alternative design","text":"<p>In a generalist system design interview, you are not expected to know the internals of specialized software used in big data processing.</p> <p>Explaining the thought process and discussing trade-offs is more important than knowing specific tools, which is why the chapter covers a generic solution.</p> <p>An alternative design, which leverages off-the-shelf tooling, is to store ad click data in Hive with an ElasticSearch layer on top built for faster queries.</p> <p>Aggregation is typically done in OLAP databases such as ClickHouse or Druid. </p>"},{"location":"booknotes/system-design-interview/chapter22/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Things we covered:  * Data model and API Design  * Using MapReduce to aggregate ad click events  * Scaling the message queue, aggregation service and database  * Mitigating the hotspot issue  * Monitoring the system continuously  * Using reconciliation to ensure correctness  * Fault tolerance</p> <p>The ad click event aggregation is a typical big data processing system.</p> <p>It would be easier to understand and design it if you have prior knowledge of related technologies:  * Apache Kafka  * Apache Spark  * Apache Flink</p>"},{"location":"booknotes/system-design-interview/chapter23/","title":"Hotel Reservation System","text":"<p>In this chapter, we're designing a hotel reservation system, similar to Marriott International.</p> <p>Applicable to other types of systems as well - Airbnb, flight reservation, movie ticket booking.</p>"},{"location":"booknotes/system-design-interview/chapter23/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<p>Before diving into designing the system, we should ask the interviewer questions to clarify the scope:  * C: What is the scale of the system?  * I: We're building a website for a hotel chain \\w 5000 hotels and 1mil rooms  * C: Do customers pay when they make a reservation or when they arrive at the hotel?  * I: They pay in full when making reservations.  * C: Do customers book hotel rooms through the website only? Do we have to support other reservation options such as phone calls?  * I: They make bookings through the website or app only.  * C: Can customers cancel reservations?  * I: Yes  * C: Other things to consider?  * I: Yes, we allow overbooking by 10%. Hotel will sell more rooms than there actually are. Hotels do this in anticipation that clients will cancel bookings.  * C: Since not much time, we'll focus on - show hotel-related page, hotel-room details page, reserve a room, admin panel, support overbooking.  * I: Sounds good.  * I: One more thing - hotel prices change all the time. Assume a hotel room's price changes every day.  * C: OK.</p>"},{"location":"booknotes/system-design-interview/chapter23/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Support high concurrency - there might be a lot of customers trying to book the same hotel during peak season.</li> <li>Moderate latency - it's ideal to have low latency when a user makes a reservation, but it's acceptable if the system takes a few seconds to process it.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter23/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>5000 hotels and 1mil rooms in total</li> <li>Assume 70% of rooms are occupied and average stay duration is 3 days</li> <li>Estimated daily reservations - 1mil * 0.7 / 3 = ~240k reservations per day</li> <li>Reservations per second - 240k / 10^5 seconds in a day = ~3. Average reservation TPS is low.</li> </ul> <p>Let's estimate the QPS. If we assume that there are three steps to reach the reservation page and there is a 10% conversion rate per page, we can estimate that if there are 3 reservations, then there must be 30 views of reservation page and 300 views of hotel room detail page. </p>"},{"location":"booknotes/system-design-interview/chapter23/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>We'll explore - API Design, Data model, high-level design.</p>"},{"location":"booknotes/system-design-interview/chapter23/#api-design","title":"API Design","text":"<p>This API Design focuses on the core endpoints (using RESTful practices), we'll need in order to support a hotel reservation system.</p> <p>A fully-fledged system would require a more extensive API with support for searching for rooms based on lots of criteria, but we won't be focusing on that in this section. Reason is that they aren't technically challenging, so they're out of scope.</p> <p>Hotel-related API  * <code>GET /v1/hotels/{id}</code> - get detailed info about a hotel  * <code>POST /v1/hotels</code> - add a new hotel. Only available to ops  * <code>PUT /v1/hotels/{id}</code> - update hotel info. Only available to ops  * <code>DELETE /v1/hotels/{id}</code> - delete a hotel. API is only available to ops</p> <p>Room-related API  * <code>GET /v1/hotels/{id}/rooms/{id}</code> - get detailed information about a room  * <code>POST /v1/hotels/{id}/rooms</code> - Add a room. Only available to ops  * <code>PUT /v1/hotels/{id}/rooms/{id}</code> - Update room info. Only available to ops  * <code>DELETE /v1/hotels/{id}/rooms/{id}</code> - Delete a room. Only available to ops</p> <p>Reservation-related API  * <code>GET /v1/reservations</code> - get reservation history of current user  * <code>GET /v1/reservations/{id}</code> - get detailed info about a reservation  * <code>POST /v1/reservations</code> - make a new reservation  * <code>DELETE /v1/reservations/{id}</code> - cancel a reservation</p> <p>Here's an example request to make a reservation: <pre><code>{\n  \"startDate\":\"2021-04-28\",\n  \"endDate\":\"2021-04-30\",\n  \"hotelID\":\"245\",\n  \"roomID\":\"U12354673389\",\n  \"reservationID\":\"13422445\"\n}\n</code></pre></p> <p>Note that the <code>reservationID</code> is an idempotency key to avoid double booking. Details explained in concurrency section</p>"},{"location":"booknotes/system-design-interview/chapter23/#data-model","title":"Data model","text":"<p>Before we choose what database to use, let's consider our access patterns.</p> <p>We need to support the following queries:  * View detailed info about a hotel  * Find available types of rooms given a date range  * Record a reservation  * Look up a reservation or past history of reservations</p> <p>From our estimations, we know the scale of the system is not large, but we need to prepare for traffic surges.</p> <p>Given this knowledge, we'll choose a relational database because:  * Relational DBs work well with read-heavy and less write-heavy systems.  * NoSQL databases are normally optimized for writes, but we know we won't have many as only a fraction of users who visit the site make a reservation.  * Relational DBs provide ACID guarantees. These are important for such a system as without them, we won't be able to prevent problems such as negative balance, double charge, etc.  * Relational DBs can easily model the data as the structure is very clear.</p> <p>Here is our schema design: </p> <p>Most fields are self-explanatory. Only field worth mentioning is the <code>status</code> field which represents the state machine of a given room: </p> <p>This data model works well for a system like Airbnb, but not for hotels where users don't reserve a particular room but a room type. They reserve a type of room and a room number is chosen at the point of reservation.</p> <p>This shortcoming will be addressed in the Improved Data Model section.</p>"},{"location":"booknotes/system-design-interview/chapter23/#high-level-design","title":"High-level Design","text":"<p>We've chosen a microservice architecture for this design. It has gained great popularity in recent years:   * Users book a hotel room on their phone or computer  * Admin perform administrative functions such as refunding/cancelling a payment, etc  * CDN caches static resources such as JS bundles, images, videos, etc  * Public API Gateway - fully-managed service which supports rate limiting, authentication, etc.  * Internal APIs - only visible to authorized personnel. Usually protected by a VPN.  * Hotel service - provides detailed information about hotels and rooms. Hotel and room data is static, so it can be cached aggressively.  * Rate service - provides room rates for different future dates. An interesting note about this domain is that prices depend on how full a hotel is at a given day.  * Reservation service - receives reservation requests and reserves hotel rooms. Also tracks room inventory as reservations are made/cancelled.  * Payment service - processes payments and updates reservation statuses on success.  * Hotel management service - available to authorized personnel only. Allows certain administrative functions for managing and viewing reservations, hotels, etc.</p> <p>Inter-service communication can be facilitated via a RPC framework, such as gRPC.</p>"},{"location":"booknotes/system-design-interview/chapter23/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's dive deeper into:  * Improved data model  * Concurrency issues  * Scalability  * Resolving data inconsistency in microservices</p>"},{"location":"booknotes/system-design-interview/chapter23/#improved-data-model","title":"Improved data model","text":"<p>As mentioned in a previous section, we need to amend our API and schema to enable reserving a type of room vs. a particular one.</p> <p>For the reservation API, we no longer reserve a <code>roomID</code>, but we reserve a <code>roomTypeID</code>: <pre><code>POST /v1/reservations\n{\n  \"startDate\":\"2021-04-28\",\n  \"endDate\":\"2021-04-30\",\n  \"hotelID\":\"245\",\n  \"roomTypeID\":\"12354673389\",\n  \"roomCount\":\"3\",\n  \"reservationID\":\"13422445\"\n}\n</code></pre></p> <p>Here's the updated schema:   * room - contains information about a room  * room_type_rate - contains information about prices for a given room type  * reservation - records guest reservation data  * room_type_inventory - stores inventory data about hotel rooms. </p> <p>Let's take a look at the <code>room_type_inventory</code> columns as that table is more interesting:  * hotel_id - id of hotel  * room_type_id - id of a room type  * date - a single date  * total_inventory - total number of rooms minus those that are temporarily taken off the inventory.  * total_reserved - total number of rooms booked for given (hotel_id, room_type_id, date)</p> <p>There are alternative ways to design this table, but having one room per (hotel_id, room_type_id, date) enables easy  reservation management and easier queries.</p> <p>The rows in the table are pre-populated using a daily CRON job.</p> <p>Sample data: | hotel_id | room_type_id | date       | total_inventory | total_reserved | |----------|--------------|------------|-----------------|----------------| | 211      | 1001         | 2021-06-01 | 100             | 80             | | 211      | 1001         | 2021-06-02 | 100             | 82             | | 211      | 1001         | 2021-06-03 | 100             | 86             | | 211      | 1001         | ...        | ...             |                | | 211      | 1001         | 2023-05-31 | 100             | 0              | | 211      | 1002         | 2021-06-01 | 200             | 16             | | 2210     | 101          | 2021-06-01 | 30              | 23             | | 2210     | 101          | 2021-06-02 | 30              | 25             |</p> <p>Sample SQL query to check the availability of a type of room: <pre><code>SELECT date, total_inventory, total_reserved\nFROM room_type_inventory\nWHERE room_type_id = ${roomTypeId} AND hotel_id = ${hotelId}\nAND date between ${startDate} and ${endDate}\n</code></pre></p> <p>How to check availability for a specified number of rooms using that data (note that we support overbooking): <pre><code>if (total_reserved + ${numberOfRoomsToReserve}) &lt;= 110% * total_inventory\n</code></pre></p> <p>Now let's do some estimation about the storage volume.  * We have 5000 hotels.  * Each hotel has 20 types of rooms.  * 5000 * 20 * 2 (years) * 365 (days) = 73mil rows</p> <p>73 million rows is not a lot of data and a single database server can handle it. It makes sense, however, to setup read replication (potentially across different zones) to enable high availability.</p> <p>Follow-up question - if reservation data is too large for a single database, what would you do?  * Store only current and future reservation data. Reservation history can be moved to cold storage.  * Database sharding - we can shard our data by <code>hash(hotel_id) % servers_cnt</code> as we always select the <code>hotel_id</code> in our queries.</p>"},{"location":"booknotes/system-design-interview/chapter23/#concurrency-issues","title":"Concurrency issues","text":"<p>Another important problem to address is double booking.</p> <p>There are two issues to address:  * Same user clicks on \"book\" twice  * Multiple users try to book a room at the same time</p> <p>Here's a visualization of the first problem: </p> <p>There are two approaches to solving this problem:  * Client-side handling - front-end can disable the book button once clicked. If a user disabled javascript, however, they won't see the button becoming grayed out.  * Idemptent API - Add an idempotency key to the API, which enables a user to execute an action once, regardless of how many times the endpoint is invoked: </p> <p>Here's how this flow works:  * A reservation order is generated once you're in the process of filling in your details and making a booking. The reservation order is generated using a globally unique identifier.  * Submit reservation 1 using the <code>reservation_id</code> generated in the previous step.  * If \"complete booking\" is clicked a second time, the same <code>reservation_id</code> is sent and the backend detects that this is a duplicate reservation.  * The duplication is avoided by making the <code>reservation_id</code> column have a unique constraint, preventing multiple records with that id being stored in the DB. </p> <p>What if there are multiple users making the same reservation?   * Let's assume the transaction isolation level is not serializable  * User 1 and 2 attempt to book the same room at the same time.  * Transaction 1 checks if there are enough rooms - there are  * Transaction 2 check if there are enough rooms - there are  * Transaction 2 reserves the room and updates the inventory  * Transaction 1 also reserves the room as it still sees there are 99 <code>total_reserved</code> rooms out of 100.  * Both transactions successfully commit the changes</p> <p>This problem can be solved using some form of locking mechanism:  * Pessimistic locking  * Optimistic locking  * Database constraints</p> <p>Here's the SQL we use to reserve a room: <pre><code># step 1: check room inventory\nSELECT date, total_inventory, total_reserved\nFROM room_type_inventory\nWHERE room_type_id = ${roomTypeId} AND hotel_id = ${hotelId}\nAND date between ${startDate} and ${endDate}\n\n# For every entry returned from step 1\nif((total_reserved + ${numberOfRoomsToReserve}) &gt; 110% * total_inventory) {\n  Rollback\n}\n\n# step 2: reserve rooms\nUPDATE room_type_inventory\nSET total_reserved = total_reserved + ${numberOfRoomsToReserve}\nWHERE room_type_id = ${roomTypeId}\nAND date between ${startDate} and ${endDate}\n\nCommit\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter23/#option-1-pessimistic-locking","title":"Option 1: Pessimistic locking","text":"<p>Pessimistic locking prevents simultaneous updates by putting a lock on a record while it's being updated.</p> <p>This can be done in MySQL by using the <code>SELECT... FOR UPDATE</code> query, which locks the rows selected by the query until the transaction is committed. </p> <p>Pros:  * Prevents applications from updating data that is being changed  * Easy to implement and avoids conflict by serializing updates. Useful when there is heavy data contention.</p> <p>Cons:  * Deadlocks may occur when multiple resources are locked.  * This approach is not scalable - if transaction is locked for too long, this has impact on all other transactions trying to access the resource.  * The impact is severe when the query selects a lot of resources and the transaction is long-lived.</p> <p>The author doesn't recommend this approach due to its scalability issues.</p>"},{"location":"booknotes/system-design-interview/chapter23/#option-2-optimistic-locking","title":"Option 2: Optimistic locking","text":"<p>Optimistic locking allows multiple users to attempt to update a record at the same time.</p> <p>There are two common ways to implement it - version numbers and timestamps. Version numbers are recommended as server clocks can be inaccurate.   * A new <code>version</code> column is added to the database table  * Before a user modifies a database row, the version number is read  * When the user updates the row, the version number is increased by 1 and written back to the database  * Database validation prevents the insert if the new version number doesn't exceed the previous one</p> <p>Optimistic locking is usually faster than pessimistic locking as we're not locking the database.  Its performance tends to degrade when concurrency is high, however, as that leads to a lot of rollbacks.</p> <p>Pros:  * It prevents applications from editing stale data  * We don't need to acquire a lock in the database  * Preferred option when data contention is low, ie rarely are there update conflicts</p> <p>Cons:  * Performance is poor when data contention is high</p> <p>Optimistic locking is a good option for our system as reservation QPS is not extremely high.</p>"},{"location":"booknotes/system-design-interview/chapter23/#option-3-database-constraints","title":"Option 3: Database constraints","text":"<p>This approach is very similar to optimistic locking, but the guardrails are implemented using a database constraint: <pre><code>CONSTRAINT `check_room_count` CHECK((`total_inventory - total_reserved` &gt;= 0))\n</code></pre> </p> <p>Pros:  * Easy to implement  * Works well when data contention is small</p> <p>Cons:  * Similar to optimistic locking, performs poorly when data contention is high  * Database constraints cannot be easily version-controlled like application code  * Not all databases support constraints</p> <p>This is another good option for a hotel reservation system due to its ease of implementation.</p>"},{"location":"booknotes/system-design-interview/chapter23/#scalability","title":"Scalability","text":"<p>Usually, the load of a hotel reservation system is not high. </p> <p>However, the interviewer might ask you how you'd handle a situation where the system gets adopted for a larger, popular travel site such as booking.com In that case, QPS can be 1000 times larger.</p> <p>When there is such a situation, it is important to understand where our bottlenecks are. All the services are stateless, so they can be easily scaled via replication.</p> <p>The database, however, is stateful and it's not as obvious how it can get scaled.</p> <p>One way to scale it is by implementing database sharding - we can split the data across multiple databases, where each of them contain a portion of the data.</p> <p>We can shard based on <code>hotel_id</code> as all queries filter based on it.  Assuming, QPS is 30,000, after sharding the database in 16 shards, each shard handles 1875 QPS, which is within a single MySQL cluster's load capacity. </p> <p>We can also utilize caching for room inventory and reservations via Redis. We can set TTL so that old data can expire for days which are past. </p> <p>The way we store an inventory is based on the <code>hotel_id</code>, <code>room_type_id</code> and <code>date</code>: <pre><code>key: hotelID_roomTypeID_{date}\nvalue: the number of available rooms for the given hotel ID, room type ID and date.\n</code></pre></p> <p>Data consistency happens async and is managed by using a CDC streaming mechanism - database changes are read and applied to a separate system. Debezium is a popular option for synchronizing database changes with Redis.</p> <p>Using such a mechanism, there is a possibility that the cache and database are inconsistent for some time. This is fine in our case because the database will prevent us from making an invalid reservation.</p> <p>This will cause some issue on the UI as a user would have to refresh the page to see that \"there are no more rooms left\",  but that is something which can happen regardless of this issue if eg a person hesitates a lot before making a reservation.</p> <p>Caching pros:  * Reduced database load  * High performance, as Redis manages data in-memory</p> <p>Caching cons:  * Maintaining data consistency between cache and DB is hard. We need to consider how the inconsistency impacts user experience.</p>"},{"location":"booknotes/system-design-interview/chapter23/#data-consistency-among-services","title":"Data consistency among services","text":"<p>A monolithic application enables us to use a shared relational database for ensuring data consistency.</p> <p>In our microservice design, we chose a hybrid approach where some services are separate,  but the reservation and inventory APIs are handled by the same servicefor the reservation and inventory APIs.</p> <p>This is done because we want to leverage the relational database's ACID guarantees to ensure consistency.</p> <p>However, the interviewer might challenge this approach as it's not a pure microservice architecture, where each service has a dedicated database: </p> <p>This can lead to consistency issues. In a monolithic server, we can leverage a relational DBs transaction capabilities to implement atomic operations: </p> <p>It's more challenging, however, to guarantee this atomicity when the operation spans across multiple services: </p> <p>There are some well-known techniques to handle these data inconsistencies:  * Two-phase commit - a database protocol which guarantees atomic transaction commit across multiple nodes.     It's not performant, though, since a single node lag leads to all nodes blocking the operation.  * Saga - a sequence of local transactions, where compensating transactions are triggered if any of the steps in a workflow fail. This is an eventually consistent approach.</p> <p>It's worth noting that addressing data inconsistencies across microservices is a challenging problem, which raise the system complexity. It is good to consider whether the cost is worth it, given our more pragmatic approach of encapsulating dependent operations within the same relational database.</p>"},{"location":"booknotes/system-design-interview/chapter23/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>We presented a design for a hotel reservation system.</p> <p>These are the steps we went through:  * Gathering requirements and doing back-of-the-envelope calculations to understand the system's scale  * We presented the API Design, Data Model and system architecture in the high-level design  * In the deep dive, we explored alternative database schema designs as requirements changed  * We discussed race conditions and proposed solutions - pessimistic/optimistic locking, database constraints  * Ways to scale the system via database sharding and caching  * Finally we addressed how to handle data consistency issues across multiple microservices</p>"},{"location":"booknotes/system-design-interview/chapter24/","title":"Distributed Email Service","text":"<p>We'll design a distributed email service, similar to gmail in this chapter.</p> <p>In 2020, gmail had 1.8bil active users, while Outlook had 400mil users worldwide.</p>"},{"location":"booknotes/system-design-interview/chapter24/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: How many users use the system?</li> <li>I: 1bil users</li> <li>C: I think following features are important - auth, send/receive email, fetch email, filter emails, search email, anti-spam protection.</li> <li>I: Good list. Don't worry about auth for now.</li> <li>C: How do users connect \\w email servers?</li> <li>I: Typically, email clients connect via SMTP, POP, IMAP, but we'll use HTTP for this problem.</li> <li>C: Can emails have attachments?</li> <li>I: Yes</li> </ul>"},{"location":"booknotes/system-design-interview/chapter24/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Reliability - we shouldn't lose data</li> <li>Availability - We should use replication to prevent single points of failure. We should also tolerate partial system failures.</li> <li>Scalability - As userbase grows, our system should be able to handle them.</li> <li>Flexibility and extensibility - system should be flexible and easy to extend with new features. One of the reasons we chose HTTP over SMTP/other mail protocols.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter24/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>1bil users</li> <li>Assuming one person sends 10 emails per day -&gt; 100k emails per second.</li> <li>Assuming one person receives 40 emails per day and each email on average has 50kb metadata -&gt; 730pb storage per year.</li> <li>Assuming 20% of emails have storage attachments and average size is 500kb -&gt; 1,460pb per year.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter24/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"booknotes/system-design-interview/chapter24/#email-knowledge-101","title":"Email knowledge 101","text":"<p>There are various protocols used for sending and receiving emails:  * SMTP - standard protocol for sending emails from one server to another.  * POP - standard protocol for receiving and downloading emails from a remote mail server to a local client. Once retrieved, emails are deleted from remote server.  * IMAP - similar to POP, it is used for receiving and downloading emails from a remote server, but it keeps the emails on the server-side.  * HTTPS - not technically an email protocol, but it can be used for web-based email clients.</p> <p>Apart from the mailing protocol, there are some DNS records we need to configure for our email server - the MX records: </p> <p>Email attachments are sent base64-encoded and there is usually a size limit of 25mb on most mail services. This is configurable and varies from individual to corporate accounts.</p>"},{"location":"booknotes/system-design-interview/chapter24/#traditional-mail-servers","title":"Traditional mail servers","text":"<p>Traditional mail servers work well when there are a limited number of users, connected to a single server.   * Alice logs into her Outlook email and presses \"send\". Email is sent to Outlook mail server. Communication is via SMTP.  * Outlook server queries DNS to find MX record for gmail.com and transfers the email to their servers. Communication is via SMTP.  * Bob fetches emails from his gmail server via IMAP/POP.</p> <p>In traditional mail servers, emails were stored on the local file system. Every email was a separate file. </p> <p>As the scale grew, disk I/O became a bottleneck. Also, it doesn't satisfy our high availability and reliability requirements. Disks can be damaged and server can go down.</p>"},{"location":"booknotes/system-design-interview/chapter24/#distributed-mail-servers","title":"Distributed mail servers","text":"<p>Distributed mail servers are designed to support modern use-cases and solve modern scalability issues.</p> <p>These servers can still support IMAP/POP for native email clients and SMTP for mail exchange across servers.</p> <p>But for rich web-based mail clients, a RESTful API over HTTP is typically used.</p> <p>Example APIs:  * <code>POST /v1/messages</code> - sends a message to recipients in To, Cc, Bcc headers.  * <code>GET /v1/folders</code> - returns all folders of an email account Example response: <pre><code>[{id: string        Unique folder identifier.\n  name: string      Name of the folder.\n                    According to RFC6154 [9], the default folders can be one of\n                    the following: All, Archive, Drafts, Flagged, Junk, Sent,\n                    and Trash.\n  user_id: string   Reference to the account owner\n}]\n</code></pre>  * <code>GET /v1/folders/{:folder_id}/messages</code> - returns all messages under a folder \\w pagination  * <code>GET /v1/messages/{:message_id}</code> - get all information about a particular message Example response: <pre><code>{\n  user_id: string                      // Reference to the account owner.\n  from: {name: string, email: string}  // &lt;name, email&gt; pair of the sender.\n  to: [{name: string, email: string}]  // A list of &lt;name, email&gt; paris\n  subject: string                      // Subject of an email\n  body: string                         //  Message body\n  is_read: boolean                     //  Indicate if a message is read or not.\n}\n</code></pre></p> <p>Here's the high-level design of the distributed mail server:   * Webmail - users use web browsers to send/receive emails  * Web servers - public-facing request/response services used to manage login, signup, user profile, etc.  * Real-time servers - Used for pushing new email updates to clients in real-time. We use websockets for real-time communication but fallback to long-polling for older browsers that don't support them.  * Metadata db - stores email metadata such as subject, body, from, to, etc.  * Attachment store - Object store (eg Amazon S3), suitable for storing large files.  * Distributed cache - We can cache recent emails in Redis to improve UX.  * Search store - distributed document store, used for supporting full-text searches.</p> <p>Here's what the email sending flow looks like:   * User writes an email and presses \"send\". Email is sent to load balancer.  * Load balancer rate limits excessive mail sends and routes to one of the web servers.  * Web servers do basic email validation (eg email size) and short-circuits outbound flow if domain is same as sender. But does spam check first.  * If basic validation passes, email is sent to message queue (attachment is referenced from object store)  * If basic validation fails, email is sent to error queue  * SMTP outgoing workers pull messages from outgoing queue, do spam/virus checks and route to destination mail server.  * Email is stored in the \"Sent Emails\" folder</p> <p>We need to also monitor size of outgoing message queue. Growing too large might indicate a problem:  * Recipient's mail server is unavailable. We can retry sending the email at a later time using exponential backoff.  * Not enough consumers to handle the load, we might have to scale the consumers.</p> <p>Here's the email receiving flow:   * Incoming emails arrive at the SMTP load balancer. Mails are distributed to SMTP servers, where mail acceptance policy is done (eg invalid emails are directly discarded).  * If attachment of email is too large, we can put it in object store (s3).  * Mail processing workers do preliminary checks, after which mails are forwarded to storage, cache, object store and real-time servers.  * Offline users get their new emails once they come back online via HTTP API.</p>"},{"location":"booknotes/system-design-interview/chapter24/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>Let's now go deeper into some of the components.</p>"},{"location":"booknotes/system-design-interview/chapter24/#metadata-database","title":"Metadata database","text":"<p>Here are some of the characteristics of email metadata:  * headers are usually small and frequently accessed  * Body size ranges from small to big, but is typically read once  * Most mail operations are isolated to a single user - eg fetching email, marking as read, searching.  * Data recency impacts data usage. Users typically read only recent emails  * Data has high-reliability requirements. Data loss is unacceptable.</p> <p>At gmail/outlook scale, the database is typically custom made to reduce input/output operations per second (IOPS).</p> <p>Let's consider what database options we have:  * Relational database - we can build indexes for headers and body, but these DBs are typically optimized for small chunks of data.  * Distributed object store - this can be a good option for backup storage, but can't efficiently support searching/marking as read/etc.  * NoSQL - Google BigTable is used by gmail, but it's not open-sourced.</p> <p>Based on above analysis, very few existing solutions seems to fit our needs perfectly. In an interview setting, it's infeasible to design a new distributed database solution, but important to mention characteristics:  * Single column can be a single-digit MB  * Strong data consistency  * Designed to reduce disk I/O  * Highly available and fault tolerant  * Should be easy to create incremental backups</p> <p>In order to partition the data, we can use the <code>user_id</code> as a partition key, so that one user's data is stored on a single shard. This prohibits us from sharing an email with multiple users, but this is not a requirement for this interview.</p> <p>Let's define the tables:  * Primary key consists of partition key (data distribution) and clustering key (sorting data)  * Queries we need to support - get all folders for a user, display all emails for a folder, create/get/delete an email, fetch read/unread email, get conversation threads (bonus)</p> <p>Legend for tables to follow: </p> <p>Here is the folders table: </p> <p>emails table:   * email_id is timeuuid which allows sorting based on timestamp when email was created</p> <p>Attachments are stored in a separate table, identified by filename: </p> <p>Supporting fetchin read/unread emails is easy in a traditional relational database, but not in Cassandra, since filtering on non-partition/clustering key is prohibited. One workaround to fetch all emails in a folder and filter in-memory, but that doesn't work well for a big-enough application.</p> <p>What we can do is denormalize the emails table into read/unread emails tables: </p> <p>In order to support conversation threads, we can include some headers, which mail clients interpret and use to reconstruct a conversation thread: <pre><code>{\n  \"headers\" {\n     \"Message-Id\": \"&lt;7BA04B2A-430C-4D12-8B57-862103C34501@gmail.com&gt;\",\n     \"In-Reply-To\": \"&lt;CAEWTXuPfN=LzECjDJtgY9Vu03kgFvJnJUSHTt6TW@gmail.com&gt;\",\n     \"References\": [\"&lt;7BA04B2A-430C-4D12-8B57-862103C34501@gmail.com&gt;\"]\n  }\n}\n</code></pre></p> <p>Finally, we'll trade availability for consistency for our distributed database, since it is a hard requirement for this problem.</p> <p>Hence, in the event of a failover or network parititon, sync/update actions will be briefly unavailable to impacted users.</p>"},{"location":"booknotes/system-design-interview/chapter24/#email-deliverability","title":"Email deliverability","text":"<p>It is easy to setup a server to send emails, but getting the email to a receiver's inbox is hard, due to spam-protection algorithms.</p> <p>If we just setup a new mail server and start sending mails through it, our emails will probably end up in the spam folder.</p> <p>Here's what we can do to prevent that:  * Dedicated IPs - use dedicated IPs for sending emails, otherwise, recipient servers will not trust you.  * Classify emails - avoid sending marketing emails from the same servers to prevent more important email to be classified as spam  * Warm up your IP address slowly to build a good reputation with big email providers. It takes 2 to 6 weeks to warm up a new IP  * Ban spammers quickly to not deteriorate your reputation  * Feedback processing - setup a feedback loop with ISPs to keep track of complaint rate and ban spam accounts quickly.  * Email authentication - use common techniques to combat phishing such as Sender Policy Framework, DomainKeys Identified Mail, etc.</p> <p>You don't need to remember all of this. Just know that building a good mail server requires a lot of domain knowledge.</p>"},{"location":"booknotes/system-design-interview/chapter24/#search","title":"Search","text":"<p>Searching includes doing a full-text search based on email contents or more advanced queries based on from, to, subject, unread, etc filters.</p> <p>One characteristic of email search is that it is local to the user and it has more writes than reads, because we need to re-index it on each operation, but users rarely use the search tab.</p> <p>Let's compare google search with email search: |               | Scope                | Sorting                               | Accuracy                                          | |---------------|----------------------|---------------------------------------|---------------------------------------------------| | Google search | The whole internet   | Sort by relevance                     | Indexing takes some time, so not instant results. | | Email search  | User\u2019s own email box | Sort by attributes eg time, date, etc | Indexing should be quick and results accurate.    |</p> <p>To achieve this search functionality, one option is to use an Elasticsearch cluster. We can use <code>user_id</code> as the partition key to group data under the same node: </p> <p>Mutating operations are async via Kafka in order to decouple services from the reindexing flow. Actually searching for data happens synchronously.</p> <p>Elasticsearch is one of the most popular search-engine databases and supports full-text search for emails very well.</p> <p>Alternatively, we can attempt to develop our own custom search solution to meet our specific requirements.</p> <p>Designing such a system is out of scope. One of the core challenges when building it is to optimize it for write-heavy workloads.</p> <p>To achieve that, we can use Log-Structured Merge-Trees (LSM) to structure the index data on disk. Write path is optimized for sequential writes only. This technique is used in Cassandra, BigTable and RocksDB.</p> <p>Its core idea is to store data in-memory until a predefined threshold is reached, after which it is merged in the next layer (disk): </p> <p>Main trade-offs between the two approaches:  * Elasticsearch scales to some extent, whereas a custom search engine can be fine-tuned for the email use-case, allowing it to scale further.  * Elasticsearch is a separate service we need to maintain, alongside the metadata store. A custom solution can be the datastore itself.  * Elasticsearch is an off-the-shelf solution, whereas the custom search engine would require significant engineering effort to build.</p>"},{"location":"booknotes/system-design-interview/chapter24/#scalability-and-availability","title":"Scalability and availability","text":"<p>Since individual user operations don't collide with other users, most components can be independently scaled.</p> <p>To ensure high availability, we can also use a multi-DC setup with leader-folower failover in case of failures: </p>"},{"location":"booknotes/system-design-interview/chapter24/#step-4-wrap-up","title":"Step 4 - Wrap up","text":"<p>Additional talking points:  * Fault tolerance - Many parts of the system could fail. It is worthwhile how we'd handle node failures.  * Compliance - PII needs to be stored in a reasonable way, given Europe's GDPR laws.  * Security - email encryption, phishing protection, safe browsing, etc.  * Optimizations - eg preventing duplication of the same attachments, sent multiple times by different users.</p>"},{"location":"booknotes/system-design-interview/chapter25/","title":"S3-like Object Storage","text":"<p>In this chapter, we'll be designing an object storage service, similar to Amazon S3.</p> <p>Storage systems fall into three broad categories:  * Block storage  * File storage  * Object storage</p> <p>Block storage are devices, which came out in 1960s. HDDs and SSDs are such examples. These devices are typically physically attached to a server, although they can also be network-attached via high-speed network protocols. Servers can format the raw blocks and use them as a file system or it can hand control of them to servers directly.</p> <p>File storage is built on top of block storage. It provides a higher level of abstraction, making it easier to manage folders and files.</p> <p>Object storage sacrifices performance for high durability, vast scale and low cost. It targets \"cold\" data and is mainly used for archival and backup. There is no hierarchical directory structure, all data is stored as objects in a flat structure. It is relatively slow compared to other storage types. Most cloud providers have an object storage offering - Amazon S3, Google GCS, etc. </p> Block Storage File Storage Object Storage Mutable Content Y Y N (has object versioning\uff09 Cost High Medium to high Low Performance Medium to high, very high Medium to high Low to medium Consistency Strong consistency Strong consistency Strong consistency [5] Data access SAS/iSCSI/FC Standard file access, CIFS/SMB, and NFS RESTful API Scalability Medium scalability High scalability Vast scalability Good for Virtual machines (VM), databases General-purpose file system access Binary data, unstructured data <p>Some terminology, related to object storage:  * Bucket - logical container for objects. Name is globally unique.  * Object - An individual piece of data, stored in a bucket. Contains object data and metadata.  * Versioning - A feature keeping multiple variants of an object in the same bucket.  * Uniform Resource Identifier (URI) - each resource is uniquely identified by a URI.  * Service-level Agreement (SLA) - contract between service provider and client. </p> <p>Amazon S3 Standard-Infrequent Access storage class SLAs:  * Durability of 99.999999999% across multiple Availability Zones  * Data is resilient in the event of entire Availability Zone being destroyed  * Designed for 99.9% availability</p>"},{"location":"booknotes/system-design-interview/chapter25/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: Which features should be included?</li> <li>I: Bucket creation, Object upload/download, versioning, Listing objects in a bucket</li> <li>C: What is the typical data size?</li> <li>I: We need to store both massive objects and small objects efficiently</li> <li>C: How much data do we store in a year?</li> <li>I: 100 petabytes</li> <li>C: Can we assume 6 nines of data durbility (99.9999%) and service availability of 4 nines (99.99%)?</li> <li>I: Yes, sounds reasonable</li> </ul>"},{"location":"booknotes/system-design-interview/chapter25/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>100 PB of data</li> <li>6 nines of data durability</li> <li>4 nines of service availability</li> <li>Storage efficiency. Reduce storage cost while maintaining high reliability and performance</li> </ul>"},{"location":"booknotes/system-design-interview/chapter25/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>Object storage is likely to have bottlenecks in disk capacity or IO per second (IOPS).</p> <p>Assumptions:  * we have 20% small (less than 1mb), 60% mid-size (1-64mb) and 20% large objects (greater than 64mb),  * One hard disk (SATA, 7200rpm) is capable of doing 100-150 random seeks per second (100-150 IOPS)</p> <p>Given the assumptions, we can estimate the total number of objects the system can persist.  * Let's use median size per object type to simplify calculation - 0.5mb for small, 32mb for medium, 200mb for large.  * Given 100PB of storage (10^11 MB) and 40% of storage usage results in 0.68bil objects  * If we assume metadata is 1kb, then we need 0.68tb space to store metadata info</p>"},{"location":"booknotes/system-design-interview/chapter25/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>Let's explore some interesting properties of object storage before diving into the design:  * Object immutability - objects in object storage are immutable (not the case in other storage systems). We may delete them or replace them, but no update.  * Key-value store - an object URI is its key and we can get its contents by making an HTTP call  * Write once, read many times - data access pattern is writing once and reading many times. According to some Linkedin research, 95% of operations are reads  * Support both small and large objects</p> <p>Design philosophy of object storage is similar to UNIX - when we save a file, it creates the filename in a data structure, called inode and file data is stored in different disk locations. The inode contains a list of file block pointers, which point to different locations on disk. </p> <p>When accessing a file, we first fetch its metadata from the inode, prior to fetching the file contents.</p> <p>Object storage works similarly - metadata store is used for file information, but contents are stored on disk: </p> <p>By separating metadata from file contents, we can scale the different stores independently: </p>"},{"location":"booknotes/system-design-interview/chapter25/#high-level-design","title":"High-level design","text":"<p>  * Load balancer - distributes API requests across service replicas  * API service - Stateless server, orchestrating calls to metadata and object store, as well as IAM service.  * Identity and access management (IAM) - central place for auth, authz, access control.  * Data store - stores and retrieves actual data. Operations are based on object ID (UUID).  * Metadata store - stores object metadata</p>"},{"location":"booknotes/system-design-interview/chapter25/#uploading-an-object","title":"Uploading an object","text":"<p>  * Create a bucket named \"bucket-to-share\" via HTTP PUT request  * API service calls IAM to ensure user is authorized and has write permissions  * API service calls metadata store to create a bucket entry. Once created, success response is returned.  * After bucket is created, HTTP PUT is sent to create an object named \"script.txt\"  * API service verifies user identity and ensures user has write permissions  * Once validation passes, object payload is sent via HTTP PUT to the data store. Data store persists it and returns a UUID.  * API service calls metadata store to create a new entry with object_id, bucket_id and bucket_name, among other metadata.</p> <p>Example object upload request: <pre><code>PUT /bucket-to-share/script.txt HTTP/1.1\nHost: foo.s3example.org\nDate: Sun, 12 Sept 2021 17:51:00 GMT\nAuthorization: authorization string\nContent-Type: text/plain\nContent-Length: 4567\nx-amz-meta-author: Alex\n\n[4567 bytes of object data]\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter25/#downloading-an-object","title":"Downloading an object","text":"<p>Buckets have no directory hierarchy, buy we can create a logical hierarchy by concatenating bucket name and object name to simulate a folder structure.</p> <p>Example GET request for fetching an object: <pre><code>GET /bucket-to-share/script.txt HTTP/1.1\nHost: foo.s3example.org\nDate: Sun, 12 Sept 2021 18:30:01 GMT\nAuthorization: authorization string\n</code></pre></p> <p>  * Client sends an HTTP GET request to the load balancer, ie <code>GET /bucket-to-share/script.txt</code>  * API service queries IAM to verify the user has correct permissions to read the bucket  * Once validated, UUID of object is retrieved from metadata store  * Object payload is retrieved from data store based on UUID and returned to the client</p> <p>// sprint 1</p>"},{"location":"booknotes/system-design-interview/chapter25/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":""},{"location":"booknotes/system-design-interview/chapter25/#data-store","title":"Data store","text":"<p>Here's how the API service interacts with the data store: </p> <p>The data store's main components: </p> <p>The data routing service provides a RESTful or gRPC API to access the data node cluster. It is a stateless service, which scales by adding more servers.</p> <p>It's main responsibilities are:  * querying the placement service to get the best data node to store data  * reading data from data nodes and returning it to the API service  * Writing data to data nodes</p> <p>The placement service determines which data nodes should store an object. It maintains a virtual cluster map, which determines the physical topology of a cluster. </p> <p>The service also sends heartbeats to all data nodes to determine if they should be removed from the virtual cluster.</p> <p>Since this is a critical service, it is recommended to maintain a cluster of 5 or 7 replicas, synchronized via Paxos or Raft consensus algorithms. Eg a 7 node cluster can tolerate 3 nodes failing.</p> <p>Data nodes store the actual object data. Reliability and durability is ensured by replicating data to multiple data nodes.</p> <p>Each data node has a daemon running, which sends heartbeats to the placement service.</p> <p>The heartbeat includes:  * How many disk drives (HDD or SSD) does the data node manage?  * How much data is stored on each drive?</p>"},{"location":"booknotes/system-design-interview/chapter25/#data-persistence-flow","title":"Data persistence flow","text":"<p>  * API service forwards the object data to data store  * Data routing service sends the data to the primary data node  * Primary data node saves the data locally and replicates it to two secondary data nodes. Response is sent after successful replication.  * The UUID of the object is returned to the API service.</p> <p>Caveats:  * Given an object UUID, it's replication group is deterministically chosen by using consistent hashing  * In step 4, the primary data node replicates the object data before returning a response. This favors strong consistency over higher latency. </p>"},{"location":"booknotes/system-design-interview/chapter25/#how-data-is-organized","title":"How data is organized","text":"<p>One simple approach to managing data is to store each object in a separate file. </p> <p>This works, but is not performant with many small files in a file system:  * Data blocks on HDD are wasted, because every file uses the whole block size. Typical block size is 4kb.  * Many files means many inodes. Operating systems don't deal well with too many inodes and there is also a max inode limit.</p> <p>These issues can be addressed by merging many small files into bigger ones via a write-ahead log (WAL). Once the file reaches its capacity (typically a few GB), a new file is created: </p> <p>The downside of this approach is that write access to the file needs to be serialized. Multiple cores accessing the same file must wait for each other. To fix this, we can confine files to specific cores to avoid lock contention.</p>"},{"location":"booknotes/system-design-interview/chapter25/#object-lookup","title":"Object lookup","text":"<p>To support storing multiple objects in the same file, we need to maintain a table, which tells the data node:  * <code>object_id</code>  * <code>filename</code> where object is stored  * <code>file_offset</code> where object starts  * <code>object_size</code></p> <p>We can deploy this table in a file-based db like RocksDB or a traditional relational database. Since the access pattern is low write+high read, a relational database works better.</p> <p>How should we deploy it? We could deploy the db and scale it separately in a cluster, accessed by all data nodes.</p> <p>Downsides:  * we'd need to aggressively scale the cluster to serve all requests  * there's additional network latency between data node and db cluster</p> <p>An alternative is to take advantage of the fact that data nodes are only interested to data related to them,  so we can deploy the relational db within the data node itself. </p> <p>SQLite is a good option as it's a lightweight file-based relational database.</p>"},{"location":"booknotes/system-design-interview/chapter25/#updated-data-persistence-flow","title":"Updated data persistence flow","text":"<p>  * API Service sends a request to save a new object  * Data node service appends the new object at the end of a file, named \"/data/c\"  * A new record for the object is inserted into the object mapping table</p>"},{"location":"booknotes/system-design-interview/chapter25/#durability","title":"Durability","text":"<p>Data durability is an important requirement in our design. In order to achieve 6 nines of durability, every failure case needs to be properly examined.</p> <p>First problem to address is hardware failures. We can achieve that by replicating data nodes to minimize probability of failure. But in addition to that, we also ought to replicate across different failure domains (cross-rack, cross-dc, separate networks, etc).  A critical event can cause multiple hardware failures within the same domain: </p> <p>Assuming annual failure rate of a typical HDD is 0.81%, making three copies gives us 6 nines of durability.</p> <p>Replicating the data nodes like that grants us the durability we want, but we could also leverage erasure coding to reduce storage costs.</p> <p>Erasure coding enables us to use parity bits, which allow us to reconstruct lost bits in the event of a failure: </p> <p>Imagine those bits are data nodes. If two of them go down, they can be recovered using the remaining four ones.</p> <p>There are different erasure coding schemes. In our case, we could use 8+4 erasure coding, split across different failure domains to maximize reliability: </p> <p>Erasure coding enables us to achieve a much lower storage cost (50% improvement) at the expense of access speed due to the data routing service having to collect data from multiple locations: </p> <p>Other caveats:  * Replication requires 200% storage overhead (in case of 3 replicas) vs. 50% via erasure coding  * Erasure coding gives us 11 nines of durability vs 6 nines via replication  * Erasure coding requires more computation to calculate and store parities</p> <p>In sum, replication is more useful for latency-sensitive applications, whereas erasure coding is attractive for storage cost efficiency and durability. Erasure coding is also much harder to implement.</p>"},{"location":"booknotes/system-design-interview/chapter25/#correctness-verification","title":"Correctness verification","text":"<p>If a disk fails entirely, then the failure is easy to detect. This is less straightforward in the event part of the disk memory gets corrupted.</p> <p>To detect this, we can use checksums - a hash of the file contents, which can be used to verify the file's integrity.</p> <p>In our case, we'll store checksums for each file and each object: </p> <p>In the case of erasure coding (8+4), we'll need to fetch each of the 8 pieces of data separately and verify each of their checksums.</p> <p>// sprint 2</p>"},{"location":"booknotes/system-design-interview/chapter25/#metadata-data-model","title":"Metadata data model","text":"<p>Table schemas: </p> <p>Queries we need to support:  * Find an object ID by name  * Insert/delete object based on name  * List objects in a bucket sharing the same prefix</p> <p>There is usually a limit on the number of buckets a user can create, hence, the size of the buckets table is small and can fit into a single db server. But we still need to scale the server for read throughput.</p> <p>The object table will probably not fit into a single database server, though. Hence, we can scale the table via sharding:  * Sharding by bucket_id will lead to hotspot issues as a bucket can have billions of objects  * Sharding by bucket_id makes the load more evenly distributed, but our queries will be slow  * We choose sharding by <code>hash(bucket_name, object_name)</code> since most queries are based on the object/bucket name.</p> <p>Even with this sharding scheme, though, listing objects in a bucket will be slow.</p>"},{"location":"booknotes/system-design-interview/chapter25/#listing-objects-in-a-bucket","title":"Listing objects in a bucket","text":"<p>In a single database, listing an object based on its prefix (looks like a directory) works like this: <pre><code>SELECT * FROM object WHERE bucket_id = \"123\" AND object_name LIKE `abc/%`\n</code></pre></p> <p>This is challenging to fulfill when the database is sharded. To achieve it, we can run the query on every shard and aggregate the results in-memory. This makes pagination challenging though, since different shards contain a different result size and we need to maintain separate limit/offset for each.</p> <p>We can leverage the fact that typically object stores are not optimized for listing objects, so we can sacrifice listing performance. We can also create a denormalized table for listing objects, sharded by bucket ID.  That would make our listing query sufficiently fast as it's isolated to a single database instance.</p>"},{"location":"booknotes/system-design-interview/chapter25/#object-versioning","title":"Object versioning","text":"<p>Versioning works by having another <code>object_version</code> column which is of type TIMEUUID, enabling us to sort records based on it.</p> <p>Each new version produces a new <code>object_id</code>: </p> <p>Deleting an object creates a new version with a special <code>object_id</code> indicating that the object was deleted. Queries for it return 404: </p>"},{"location":"booknotes/system-design-interview/chapter25/#optimizing-uploads-of-large-files","title":"Optimizing uploads of large files","text":"<p>Uploading large files can be optimized by using multipart uploads - splitting a big file into several chunks, uploaded independently:   * Client calls service to initiate a multipart upload  * Data store returns an upload ID which uniquely identifies the upload  * Client splits the large file into several chunks, uploaded independently using the upload id  * When a chunk is uploaded, the data store returns an etag, which is a md5 checksum, identifying that upload chunk  * After all parts are uploaded, client sends a complete multipart upload request, which includes upload_id, part numbers and all etags  * Data store reassembles the object from its parts. The process can take a few minutes. After that, success response is returned to the client.</p> <p>Old parts, which are no longer useful can be removed at this point. We can introduce a garbage collector to deal with it.</p>"},{"location":"booknotes/system-design-interview/chapter25/#garbage-collection","title":"Garbage collection","text":"<p>Garbage collection is the process of reclaiming storage space, which is no longer used. There are a few ways data becomes garbage:  * lazy object deletion - object is marked as deleted without actually getting deleted  * orphan data - eg an upload failed mid-flight and old parts need to be deleted  * corrupted data - data which failed checksum verification</p> <p>The garbage collector is also responsible for reclaiming unused space in replicas.  With replication, data is deleted from both primaries and replicas. With erasure coding (8+4), data is deleted from all 12 nodes.</p> <p>To facilitate the deletion, we'll use a process called compaction:  * Garbage collector copies objects which are not deleted from \"data/b\" to \"data/d\"  * <code>object_mapping</code> table is updated once copying is complete using a database transaction  * To avoid making too many small files, compaction is done on files which grow beyond a certain threshold </p>"},{"location":"booknotes/system-design-interview/chapter25/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Things we covered:  * Designing an S3-like object storage  * Comparing differences between object, block and file storages  * Covered uploading, downloading, listing, versioning of objects in a bucket  * Deep dived in the design - data store and metadata store, replication and erasure coding, multipart uploads, sharding</p>"},{"location":"booknotes/system-design-interview/chapter26/","title":"Real-time Gaming Leaderboard","text":"<p>We are going to design a leaderboard for an online mobile game: </p>"},{"location":"booknotes/system-design-interview/chapter26/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: How is the score calculated for the leaderboard?</li> <li>I: User gets a point whenever they win a match.</li> <li>C: Are all players included in the leaderboard?</li> <li>I: Yes</li> <li>C: Is there a time segment, associated with the leaderboard?</li> <li>I: Each month, a new tournament starts which starts a new leaderboard.</li> <li>C: Can we assume we only care about top 10 users?</li> <li>I: We want to display top 10 users, along with position of specific user. If time permits, we can discuss showing users around particular user in the leaderboard.</li> <li>C: How many users are in a tournament?</li> <li>I: 5mil DAU and 25mil MAU</li> <li>C: How many matches are played on average during a tournament?</li> <li>I: Each player plays 10 matches per day on average</li> <li>C: How do we determine the rank if two players have the same score?</li> <li>I: Their rank is the same in that case. If time permits, we can discuss breaking ties.</li> <li>C: Does the leaderboard need to be real-time?</li> <li>I: Yes, we want to present real-time results or as close as possible to real-time. It is not okay to present batched result history.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter26/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Display top 10 players on leaderboard</li> <li>Show a user's specific rank</li> <li>Display users which are four places above and below given user (bonus)</li> </ul>"},{"location":"booknotes/system-design-interview/chapter26/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Real-time updates on scores</li> <li>Score update is reflected on the leaderboard in real-time</li> <li>General scalability, availability, reliability</li> </ul>"},{"location":"booknotes/system-design-interview/chapter26/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>With 50mil DAU, if the game has an even distribution of players during a 24h period, we'd have an average of 50 users per second. However, since distribution is typically uneven, we can estimate that the peak online users would be 250 users per second.</p> <p>QPS for users scoring a point - given 10 games per day on average, 50 users/s * 10 = 500 QPS. Peak QPS = 2500.</p> <p>QPS for fetching the top 10 leaderboard - assuming users open that once a day on average, QPS is 50.</p>"},{"location":"booknotes/system-design-interview/chapter26/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"booknotes/system-design-interview/chapter26/#api-design","title":"API Design","text":"<p>The first API we need is one to update a user's score: <pre><code>POST /v1/scores\n</code></pre></p> <p>This API takes two params - <code>user_id</code> and <code>points</code> scored for winning a game.</p> <p>This API should only be accessible to game servers, not end clients.</p> <p>Next one is for getting the top 10 players of the leaderboard: <pre><code>GET /v1/scores\n</code></pre></p> <p>Example response: <pre><code>{\n  \"data\": [\n    {\n      \"user_id\": \"user_id1\",\n      \"user_name\": \"alice\",\n      \"rank\": 1,\n      \"score\": 12543\n    },\n    {\n      \"user_id\": \"user_id2\",\n      \"user_name\": \"bob\",\n      \"rank\": 2,\n      \"score\": 11500\n    }\n  ],\n  ...\n  \"total\": 10\n}\n</code></pre></p> <p>You can also get the score of a particular user: <pre><code>GET /v1/scores/{:user_id}\n</code></pre></p> <p>Example response: <pre><code>{\n    \"user_info\": {\n        \"user_id\": \"user5\",\n        \"score\": 1000,\n        \"rank\": 6,\n    }\n}\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter26/#high-level-architecture","title":"High-level architecture","text":"<p>  * When a player wins a game, client sends a request to the game service  * Game service validates if win is valid and calls the leaderboard service to update the player's score  * Leaderboard service updates the user's score in the leaderboard store  * Player makes a call to leaderboard service to fetch leaderboard data, eg top 10 players and given player's rank</p> <p>An alternative design which was considered is the client updating their score directly within the leaderboard service: </p> <p>This option is not secure as it's susceptible to man-in-the-middle attacks. Players can put a proxy and change their score as they please.</p> <p>One additional caveat is that for games, where the game logic is managed by the server, cliets don't need to call the server explicitly to record their win.  Servers do it automatically for them based on the game logic.</p> <p>One additional consideration is whether we should put a message queue between the game server and the leaderboard service. This would be useful if other services are interested in game results, but that is not an explicit requirement in the interview so far, hence it's not included in the design: </p>"},{"location":"booknotes/system-design-interview/chapter26/#data-models","title":"Data models","text":"<p>Let's discuss the options we have for storing leaderboard data - relational DBs, Redis, NoSQL.</p> <p>The NoSQL solution is discussed in the deep dive section.</p>"},{"location":"booknotes/system-design-interview/chapter26/#relational-database-solution","title":"Relational database solution","text":"<p>If the scale doesn't matter and we don't have that many users, a relational DB serves our quite well.</p> <p>We can start from a simple leaderboard table, one for each month (personal note - this doesn't make sense. You can just add a <code>month</code> column and avoid the headache of maintaining new tables each month):  </p> <p>There is additional data to include in there, but that is irrelevant to the queries we'd run, so it's omitted.</p> <p>What happens when a user wins a point? </p> <p>If a user doesn't exist in the table yet, we need to insert them first: <pre><code>INSERT INTO leaderboard (user_id, score) VALUES ('mary1934', 1);\n</code></pre></p> <p>On subsequent calls, we'd just update their score: <pre><code>UPDATE leaderboard set score=score + 1 where user_id='mary1934';\n</code></pre></p> <p>How do we find the top players of a leaderboard? </p> <p>We can run the following query: <pre><code>SELECT (@rownum := @rownum + 1) AS rank, user_id, score\nFROM leaderboard\nORDER BY score DESC;\n</code></pre></p> <p>This is not performant though as it makes a table scan to order all records in the database table.</p> <p>We can optimize it by adding an index on <code>score</code> and using the <code>LIMIT</code> operation to avoid scanning everything: <pre><code>SELECT (@rownum := @rownum + 1) AS rank, user_id, score\nFROM leaderboard\nORDER BY score DESC\nLIMIT 10;\n</code></pre></p> <p>This approach, however, doesn't scale well if the user is not at the top of the leaderboard and you'd want to locate their rank.</p>"},{"location":"booknotes/system-design-interview/chapter26/#redis-solution","title":"Redis solution","text":"<p>We want to find a solution, which works well even for millions of players without having to fallback on complex database queries.</p> <p>Redis is an in-memory data store, which is fast as it works in-memory and has a suitable data structure to serve our needs - sorted set.</p> <p>A sorted set is a data structure similar to sets in programming languages, which allows you to keep a data structure sorted by a given criteria. Internally, it is implemented using a hash-map to maintain mapping between key (user_id) and value (score) and a skip list which maps scores to users in sorted order: </p> <p>How does a skip list work?  * It is a linked list which allows for fast search  * It consists of a sorted linked list and multi-level indexes </p> <p>This structure enables us to quickly search for specific values when the data set is large enough. In the example below (64 nodes), it requires traversing 62 nodes in a base linked list to find the given value and 11 nodes in the skip-list case: </p> <p>Sorted sets are more performant than relational databases as the data is kept sorted at all times at the price of O(logN) add and find operation.</p> <p>In contract, here's an example nested query we need to run to find the rank of a given user in a relational DB: <pre><code>SELECT *,(SELECT COUNT(*) FROM leaderboard lb2\nWHERE lb2.score &gt;= lb1.score) RANK\nFROM leaderboard lb1\nWHERE lb1.user_id = {:user_id};\n</code></pre></p> <p>What operations do we need to operate our leaderboard in Redis?  * <code>ZADD</code> - insert the user into the set if they don't exist. Otherwise, update the score. O(logN) time complexity.  * <code>ZINCRBY</code> - increment the score of a user by given amount. If user doesn't exist, score starts at zero. O(logN) time complexity.  * <code>ZRANGE/ZREVRANGE</code> - fetch a range of users, sorted by their score. We can specify order (ASC/DESC), offset and result size. O(logN+M) time complexity where M is result size.  * <code>ZRANK/ZREVRANK</code> - Fetch the position (rank) of given user in ASC/DESC order. O(logN) time complexity.</p> <p>What happens when a user scores a point? <pre><code>ZINCRBY leaderboard_feb_2021 1 'mary1934'\n</code></pre></p> <p>There's a new leaderboard created every month while old ones are moved to historical storage.</p> <p>What happens when a user fetches top 10 players? <pre><code>ZREVRANGE leaderboard_feb_2021 0 9 WITHSCORES\n</code></pre></p> <p>Example result: <pre><code>[(user2,score2),(user1,score1),(user5,score5)...]\n</code></pre></p> <p>What about user fetching their leaderboard position? </p> <p>This can be easily achieved by the following query, given that we know a user's leaderboard position: <pre><code>ZREVRANGE leaderboard_feb_2021 357 365\n</code></pre></p> <p>A user's position can be fetched using <code>ZREVRANK &lt;user-id&gt;</code>.</p> <p>Let's explore what our storage requirements are:  * Assuming worst-case scenario of all 25mil MAU participating in the game for a given month  * ID is 24-character string and score is 16-bit integer, we need 26 bytes * 25mil = ~650MB of storage  * Even if we double the storage cost due to the overhead of the skip list, this would still easily fit in a modern redis cluster</p> <p>Another non-functional requirement to consider is supporting 2500 updates per second. This is well within a single Redis server's capabilities.</p> <p>Additional caveats:  * We can spin up a Redis replica to avoid losing data when a redis server crashes  * We can still leverage Redis persistence to not lose data in the event of a crash  * We'll need two supporting tables in MySQL to fetch user details such as username, display name, etc as well as store when eg a user won a game  * The second table in MySQL can be used to reconstruct leaderboard when there is an infrastructure failure  * As a small performance optimization, we could cache the user details of top 10 players as they'd be frequently accessed</p>"},{"location":"booknotes/system-design-interview/chapter26/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":""},{"location":"booknotes/system-design-interview/chapter26/#to-use-a-cloud-provider-or-not","title":"To use a cloud provider or not","text":"<p>We can either choose to deploy and manage our own services or use a cloud provider to manage them for us.</p> <p>If we choose to manage the services our selves, we'll use redis for leaderboard data, mysql for user profile and potentially a cache for user profile if we want to scale the database: </p> <p>Alternatively, we could use cloud offerings to manage a lot of the services for us. For example, we can use AWS API Gateway to route API calls to AWS Lambda functions: </p> <p>AWS Lambda enables us to run code without managing or provisioning servers ourselves. It runs only when needed and scales automatically.</p> <p>Exmaple user scoring a point: </p> <p>Example user retrieving leaderboard: </p> <p>Lambdas are an implementation of a serverless architecture. We don't need to manage scaling and environment setup.</p> <p>Author recommends going with this approach if we build the game from the ground up.</p>"},{"location":"booknotes/system-design-interview/chapter26/#scaling-redis","title":"Scaling Redis","text":"<p>With 5mil DAU, we can get away with a single Redis instance from both a storage and QPS perspective.</p> <p>However, if we imagine userbase grows 10x to 500mil DAU, then we'd need 65gb for storage and QPS goes to 250k.</p> <p>Such scale would require sharding.</p> <p>One way to achieve it is by range-partitioning the data: </p> <p>In this example, we'll shard based on user's score. We'll maintain the mapping between user_id and shard in application code. We can do that either via MySQL or another cache for the mapping itself.</p> <p>To fetch the top 10 players, we'd query the shard with the highest scores (<code>[900-1000]</code>).</p> <p>To fetch a user's rank, we'll need to calculate the rank within the user's shard and add up all users with higher scores in other shards. The latter is a O(1) operation as total records per shard can quickly be accessed via the info keyspace command.</p> <p>Alternatively, we can use hash partitioning via Redis Cluster. It is a proxy which distributes data across redis nodes based on partitioning similar to consistent hashing, but not exactly the same: </p> <p>Calculating the top 10 players is challenging with this setup. We'll need to get the top 10 players of each shard and merge the results in the application: </p> <p>There are some limitations with the hash partitioning:  * If we need to fetch top K users, where K is high, latency can increase as we'll need to fetch a lot of data from all the shards  * Latency increases as the number of partitions grows  * There is no straightforward approach to determine a user's rank</p> <p>Due to all this, the author leans towards using fixed partitions for this problem.</p> <p>Other caveats:  * A best practice is to allocate twice as much memory as required for write-heavy redis nodes to accommodate snapshots if required  * We can use a tool called Redis-benchmark to track the performance of a redis setup and make data-driven decisions</p>"},{"location":"booknotes/system-design-interview/chapter26/#alternative-solution-nosql","title":"Alternative solution: NoSQL","text":"<p>An alternative solution to consider is using an appropriate NoSQL database optimized for:  * heavy writes  * effectively sorting items within the same partition by score</p> <p>DynamoDB, Cassandra or MongoDB are all good fits.</p> <p>In this chapter, the author has decided to use DynamoDB. It is a fully-managed NoSQL database, which offers reliable performance and great scalability. It also enables usage of global secondary indexes when we need to query fields not part of the primary key. </p> <p>Let's start from a table for storing a leaderboard for a chess game: </p> <p>This works well, but doesn't scale well if we need to query anything by score. Hence, we can put the score as a sort key: </p> <p>Another problem with this design is that we're partitioning by month. This leads to a hotspot partition as the latest month will be unevenly accessed compared to the others.</p> <p>We could use a technique called write sharding, where we append a partition number for each key, calculated via <code>user_id % num_partitions</code>: </p> <p>An important trade-off to consider is how many partitions we should use:  * The more partitions there are, the higher the write scalability  * However, read scalability suffers as we need to query more partitions to collect aggregate results</p> <p>Using this approach requires that we use the \"scatter-gather\" technique we saw earlier, which grows in time complexity as we add more partitions: </p> <p>To make a good evaluation on the number of partitions, we'd need to do some benchmarking.</p> <p>This NoSQL approach still has one major downside - it is hard to calculate the specific rank of a user.</p> <p>If we have sufficient scale to require us to shard, we could then perhaps tell users what \"percentile\" of scores they're in.</p> <p>A cron job can periodically run to analyze score distributions, based on which a user's percentile is determined, eg: <pre><code>10th percentile = score &lt; 100\n20th percentile = score &lt; 500\n...\n90th percentile = score &lt; 6500\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter26/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Other things to discuss if time permits:  * Faster retrieval - We can cache the user object via a Redis hash with mapping <code>user_id -&gt; user object</code>. This enables faster retrieval vs. querying the database.  * Breaking ties - When two players have the same score, we can break the tie by sorting them based on last played game.  * System failure recovery - In the event of a large-scale Redis outage, we can recreate the leaderboard by going through the MySQL WAL entries and recreate it via an ad-hoc script</p>"},{"location":"booknotes/system-design-interview/chapter27/","title":"Payment System","text":"<p>We'll design a payment system in this chapter, which underpins all of modern e-commerce.</p> <p>A payment system is used to settle financial transactions, transferring monetary value.</p>"},{"location":"booknotes/system-design-interview/chapter27/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: What kind of payment system are we building?</li> <li>I: A payment backend for an e-commerce system, similar to Amazon.com. It handles everything related to money movement.</li> <li>C: What payment options are supported - Credit cards, PayPal, bank cards, etc?</li> <li>I: The system should support all these options in real life. For the purposes of the interview, we can use credit card payments.</li> <li>C: Do we handle credit card processing ourselves?</li> <li>I: No, we use a third-party provider like Stripe, Braintree, Square, etc.</li> <li>C: Do we store credit card data in our system?</li> <li>I: Due to compliance reasons, we do not store credit card data directly in our systems. We rely on third-party payment processors.</li> <li>C: Is the application global? Do we need to support different currencies and international payments?</li> <li>I: The application is global, but we assume only one currency is used for the purposes of the interview.</li> <li>C: How many payment transactions per day do we support?</li> <li>I: 1mil transactions per day.</li> <li>C: Do we need to support the payout flow to eg payout to payers each month?</li> <li>I: Yes, we need to support that</li> <li>C: Is there anything else I should pay attention to?</li> <li>I: We need to support reconciliations to fix any inconsistencies in communicating with internal and external systems.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter27/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Pay-in flow - payment system receives money from customers on behalf of merchants</li> <li>Pay-out flow - payment system sends money to sellers around the world</li> </ul>"},{"location":"booknotes/system-design-interview/chapter27/#non-functional-requirements","title":"Non-functional requirements","text":"<ul> <li>Reliability and fault-tolerance. Failed payments need to be carefully handled</li> <li>A reconciliation between internal and external systems needs to be setup.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter27/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>The system needs to process 1mil transactions per day, which is 10 transactions per second.</p> <p>This is not a high throughput for any database system, so it's not the focus of this interview.</p>"},{"location":"booknotes/system-design-interview/chapter27/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":"<p>At a high-level, we have three actors, participating in money movement: </p>"},{"location":"booknotes/system-design-interview/chapter27/#pay-in-flow","title":"Pay-in flow","text":"<p>Here's the high-level overview of the pay-in flow:   * Payment service - accepts payment events and coordinates the payment process. It typically also does a risk check using a third-party provider for AML violations or criminal activity.  * Payment executor - executes a single payment order via the Payment Service Provider (PSP). Payment events may contain several payment orders.  * Payment service provider (PSP) - moves money from one account to another, eg from buyer's credit card account to e-commerce site's bank account.  * Card schemes - organizations that process credit card operations, eg Visa MasterCard, etc.  * Ledger - keeps financial record of all payment transactions.  * Wallet - keeps the account balance for all merchants.</p> <p>Here's an example pay-in flow:  * user clicks \"place order\" and a payment event is sent to the payment service  * payment service stores the event in its database  * payment service calls the payment executor for all payment orders, part of that payment event  * payment executor stores the payment order in its database  * payment executor calls external PSP to process the credit card payment  * After the payment executor processes the payment, the payment service updates the wallet to record how much money the seller has  * wallet service stores updated balance information in its database  * payment service calls the ledger to record all money movements</p>"},{"location":"booknotes/system-design-interview/chapter27/#apis-for-payment-service","title":"APIs for payment service","text":"<pre><code>POST /v1/payments\n{\n  \"buyer_info\": {...},\n  \"checkout_id\": \"some_id\",\n  \"credit_card_info\": {...},\n  \"payment_orders\": [{...}, {...}, {...}]\n}\n</code></pre> <p>Example <code>payment_order</code>: <pre><code>{\n  \"seller_account\": \"SELLER_IBAN\",\n  \"amount\": \"3.15\",\n  \"currency\": \"USD\",\n  \"payment_order_id\": \"globally_unique_payment_id\"\n}\n</code></pre></p> <p>Caveats:  * The <code>payment_order_id</code> is forwarded to the PSP to deduplicate payments, ie it is the idempotency key.  * The amount field is <code>string</code> as <code>double</code> is not appropriate for representing monetary values.</p> <pre><code>GET /v1/payments/{:id}\n</code></pre> <p>This endpoint returns the execution status of a single payment, based on the <code>payment_order_id</code>.</p>"},{"location":"booknotes/system-design-interview/chapter27/#payment-service-data-model","title":"Payment service data model","text":"<p>We need to maintain two tables - <code>payment_events</code> and <code>payment_orders</code>.</p> <p>For payments, performance is typically not an important factor. Strong consistency, however, is.</p> <p>Other considerations for choosing the database:  * Strong market of DBAs to hire to administer the databaseS  * Proven track-record where the database has been used by other big financial institutions  * Richness of supporting tools  * Traditional SQL over NoSQL/NewSQL for its ACID guarantees</p> <p>Here's what the <code>payment_events</code> table contains:  * <code>checkout_id</code> - string, primary key  * <code>buyer_info</code> - string (personal note - prob a foreign key to another table is more appropriate)  * <code>seller_info</code> - string (personal note - same remark as above)  * <code>credit_card_info</code> - depends on card provider  * <code>is_payment_done</code> - boolean</p> <p>Here's what the <code>payment_orders</code> table contains:  * <code>payment_order_id</code> - string, primary key  * <code>buyer_account</code> - string  * <code>amount</code> - string  * <code>currency</code> - string  * <code>checkout_id</code> - string, foreign key  * <code>payment_order_status</code> - enum (<code>NOT_STARTED</code>, <code>EXECUTING</code>, <code>SUCCESS</code>, <code>FAILED</code>)  * <code>ledger_updated</code> - boolean  * <code>wallet_updated</code> - boolean</p> <p>Caveats:  * there are many payment orders, linked to a given payment event  * we don't need the <code>seller_info</code> for the pay-in flow. That's required on pay-out only  * <code>ledger_updated</code> and <code>wallet_updated</code> are updated when the respective service is called to record the result of a payment  * payment transitions are managed by a background job, which checks updates of in-flight payments and triggers an alert if a payment is not processed in a reasonable timeframe</p>"},{"location":"booknotes/system-design-interview/chapter27/#double-entry-ledger-system","title":"Double-entry ledger system","text":"<p>The double-entry accounting mechanism is key to any payment system. It is a mechanism of tracking money movements by always applying money operations to two accounts, where one's account balance increases (credit) and the other decreases (debit): | Account | Debit | Credit | |---------|-------|--------| | buyer   | $1    |        | | seller  |       | $1     |</p> <p>Sum of all transaction entries is always zero. This mechanism provides end-to-end traceability of all money movements within the system.</p>"},{"location":"booknotes/system-design-interview/chapter27/#hosted-payment-page","title":"Hosted payment page","text":"<p>To avoid storing credit card information and having to comply with various heavy regulations, most companies prefer utilizing a widget, provided by PSPs, which store and handle credit card payments for you: </p>"},{"location":"booknotes/system-design-interview/chapter27/#pay-out-flow","title":"Pay-out flow","text":"<p>The components of the pay-out flow are very similar to the pay-in flow.</p> <p>Main differences:  * money is moved from e-commerce site's bank account to merchant's bank account  * we can utilize a third-party account payable provider such as Tipalti  * There's a lot of bookkeeping and regulatory requirements to handle with regards to pay-outs as well</p>"},{"location":"booknotes/system-design-interview/chapter27/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>This section focuses on making the system faster, more robust and secure.</p>"},{"location":"booknotes/system-design-interview/chapter27/#psp-integration","title":"PSP Integration","text":"<p>If our system can directly connect to banks or card schemes, payment can be made without a PSP. These kinds of connections are very rare and uncommon, typically done at large companies which can justify the investment.</p> <p>If we go down the traditional route, a PSP can be integrated in one of two ways:  * Through API, if our payment system can collect payment information  * Through a hosted payment page to avoid dealing with payment information regulations</p> <p>Here's how the hosted payment page workflow works:   * User clicks \"checkout\" button in the browser  * Client calls the payment service with the payment order information  * After receiving payment order information, the payment service sends a payment registration request to the PSP.  * The PSP receives payment info such as currency, amount, expiration, etc, as well as a UUID for idempotency purposes. Typically the UUID of the payment order.  * The PSP returns a token back which uniquely identifies the payment registration. The token is stored in the payment service database.  * Once token is stored, the user is served with a PSP-hosted payment page. It is initialized using the token as well as a redirect URL for success/failure.   * User fills in payment details on the PSP page, PSP processes payment and returns the payment status  * User is now redirected back to the redirectURL. Example redirect url - <code>https://your-company.com/?tokenID=JIOUIQ123NSF&amp;payResult=X324FSa</code>  * Asynchronously, the PSP calls our payment service via a webhook to inform our backend of the payment result  * Payment service records the payment result based on the webhook received</p>"},{"location":"booknotes/system-design-interview/chapter27/#reconciliation","title":"Reconciliation","text":"<p>The previous section explains the happy path of a payment. Unhappy paths are detected and reconciled using a background reconciliation process.</p> <p>Every night, the PSP sends a settlement file which our system uses to compare the external system's state against our internal system's state. </p> <p>This process can also be used to detect internal inconsistencies between eg the ledger and the wallet services.</p> <p>Mismatches are handled manually by the finance team. Mismatches are handled as:  * classifiable, hence, it is a known mismatch which can be adjusted using a standard procedure  * classifiable, but can't be automated. Manually adjusted by the finance team  * unclassifiable. Manually investigated and adjusted by the finance team</p>"},{"location":"booknotes/system-design-interview/chapter27/#handling-payment-processing-delays","title":"Handling payment processing delays","text":"<p>There are cases, where a payment can take hours to complete, although it typically takes seconds.</p> <p>This can happen due to:  * a payment being flagged as high-risk and someone has to manually review it  * credit card requires extra protection, eg 3D Secure Authentication, which requires extra details from card holder to complete</p> <p>These situations are handled by:  * waiting for the PSP to send us a webhook when a payment is complete or polling its API if the PSP doesn't provide webhooks  * showing a \"pending\" status to the user and giving them a page, where they can check-in for payment updates. We could also send them an email once their payment is complete</p>"},{"location":"booknotes/system-design-interview/chapter27/#communication-among-internal-services","title":"Communication among internal services","text":"<p>There are two types of communication patterns services use to communicate with one another - synchronous and asynchronous.</p> <p>Synchronous communication (ie HTTP) works well for small-scale systems, but suffers as scale increases:  * low performance - request-response cycle is long as more services get involved in the call chain  * poor failure isolation - if PSPs or any other service fails, user will not receive a response  * tight coupling - sender needs to know the receiver  * hard to scale - not easy to support sudden increase in traffic due to not having a buffer</p> <p>Asynchronous communication can be divided into two categories.</p> <p>Single receiver - multiple receivers subscribe to the same topic and messages are processed only once: </p> <p>Multiple receivers - multiple receivers subscribe to the same topic, but messages are forwarded to all of them: </p> <p>Latter model works well for our payment system as a payment can trigger multiple side effects, handled by different services.</p> <p>In a nutshell, synchronous communication is simpler but doesn't allow services to be autonomous.  Async communication trades simplicity and consistency for scalability and resilience.</p>"},{"location":"booknotes/system-design-interview/chapter27/#handling-failed-payments","title":"Handling failed payments","text":"<p>Every payment system needs to address failed payments. Here are some of the mechanism we'll use to achieve that:  * Tracking payment state - whenever a payment fails, we can determine whether to retry/refund based on the payment state.  * Retry queue - payments which we'll retry are published to a retry queue  * Dead-letter queue - payments which have terminally failed are pushed to a dead-letter queue, where the failed payment can be debugged and inspected. </p>"},{"location":"booknotes/system-design-interview/chapter27/#exactly-once-delivery","title":"Exactly-once delivery","text":"<p>We need to ensure a payment gets processed exactly-once to avoid double-charging a customer.</p> <p>An operation is executed exactly-once if it is executed at-least-once and at-most-once at the same time.</p> <p>To achieve the at-least-once guarantee, we'll use a retry mechanism: </p> <p>Here are some common strategies on deciding the retry intervals:  * immediate retry - client immediately sends another request after failure  * fixed intervals - wait a fixed amount of time before retrying a payment  * incremental intervals - incrementally increase retry interval between each retry  * exponential back-off - double retry interval between subsequent retries  * cancel - client cancels the request. This happens when the error is terminal or retry threshold is reached</p> <p>As a rule of thumb, default to an exponential back-off retry strategy. A good practice is for the server to specify a retry interval using a <code>Retry-After</code> header.</p> <p>An issue with retries is that the server can potentially process a payment twice:  * client clicks the \"pay button\" twice, hence, they are charged twice  * payment is successfully processed by PSP, but not by downstream services (ledger, wallet). Retry causes the payment to be processed by the PSP again</p> <p>To address the double payment problem, we need to use an idempotency mechanism - a property that an operation applied multiple times is processed only once.</p> <p>From an API perspective, clients can make multiple calls which produce the same result.  Idempotency is managed by a special header in the request (eg <code>idempotency-key</code>), which is typically a UUID. </p> <p>Idempotency can be achieved using the database's mechanism of adding unique key constraints:  * server attempts to insert a new row in the database  * the insertion fails due to a unique key constraint violation  * server detects that error and instead returns the existing object back to the client</p> <p>Idempotency is also applied at the PSP side, using the nonce, which was previously discussed. PSPs will take care to not process payments with the same nonce twice.</p>"},{"location":"booknotes/system-design-interview/chapter27/#consistency","title":"Consistency","text":"<p>There are several stateful services called throughout a payment's lifecycle - PSP, ledger, wallet, payment service.</p> <p>Communication between any two services can fail.  We can ensure eventual data consistency between all services by implementing exactly-once processing and reconciliation.</p> <p>If we use replication, we'll have to deal with replication lag, which can lead to users observing inconsistent data between primary and replica databases.</p> <p>To mitigate that, we can serve all reads and writes from the primary database and only utilize replicas for redundancy and fail-over. Alternatively, we can ensure replicas are always in-sync by utilizing a consensus algorithm such as Paxos or Raft. We could also use a consensus-based distributed database such as YugabyteDB or CockroachDB.</p>"},{"location":"booknotes/system-design-interview/chapter27/#payment-security","title":"Payment security","text":"<p>Here are some mechanisms we can use to ensure payment security:  * Request/response eavesdropping - we can use HTTPS to secure all communication  * Data tampering - enforce encryption and integrity monitoring  * Man-in-the-middle attacks - use SSL \\w certificate pinning  * Data loss - replicate data across multiple regions and take data snapshots  * DDoS attack - implement rate limiting and firewall  * Card theft - use tokens instead of storing real card information in our system  * PCI compliance - a security standard for organizations which handle branded credit cards  * Fraud - address verification, card verification value (CVV), user behavior analysis, etc</p>"},{"location":"booknotes/system-design-interview/chapter27/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Other talking points:  * Monitoring and alerting  * Debugging tools - we need tools which make it easy to understand why a payment has failed  * Currency exchange - important when designing a payment system for international use  * Geography - different regions might have different payment methods  * Cash payment - very common in places like India and Brazil  * Google/Apple Pay integration</p>"},{"location":"booknotes/system-design-interview/chapter28/","title":"Digital Wallet","text":"<p>Payment platforms usually have a wallet service, where they allow clients to store funds within the application, which they can withdraw later.</p> <p>You can also use it to pay for goods &amp; services or transfer money to other users, who use the digital wallet service. That can be faster and cheaper than doing it via normal payment rails. </p>"},{"location":"booknotes/system-design-interview/chapter28/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design Scope","text":"<ul> <li>C: Should we only focus on transfers between digital wallets? Should we support any other operations?</li> <li>I: Let's focus on transfers between digital wallets for now.</li> <li>C: How many transactions per second does the system need to support?</li> <li>I: Let's assume 1mil TPS</li> <li>C: A digital wallet has strict correctness requirements. Can we assume transactional guarantees are sufficient?</li> <li>I: Sounds good</li> <li>C: Do we need to prove correctness?</li> <li>I: We can do that via reconciliation, but that only detects discrepancies vs. showing us the root cause for them. Instead, we want to be able to replay data from the beginning to reconstruct the history.</li> <li>C: Can we assume availability requirement is 99.99%?</li> <li>I: Yes</li> <li>C: Do we need to take foreign exchange into consideration?</li> <li>I: No, it's out of scope</li> </ul> <p>Here's what we have to support in summary:  * Support balance transfers between two accounts  * Support 1mil TPS  * Reliability is 99.99%  * Support transactions  * Support reproducibility</p>"},{"location":"booknotes/system-design-interview/chapter28/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<p>A traditional relational database, provisioned in the cloud can support ~1000 TPS.</p> <p>In order to reach 1mil TPS, we'd need 1000 database nodes. But if each transfer has two legs, then we actually need to support 2mil TPS.</p> <p>One of our design goals would be to increase the TPS a single node can handle so that we can have less database nodes. | Per-node TPS | Node Number | |--------------|-------------| | 100          | 20,000      | | 1,000        | 2,000       | | 10,000       | 200         |</p>"},{"location":"booknotes/system-design-interview/chapter28/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"booknotes/system-design-interview/chapter28/#api-design","title":"API Design","text":"<p>We only need to support one endpoint for this interview: <pre><code>POST /v1/wallet/balance_transfer - transfers balance from one wallet to another\n</code></pre></p> <p>Request parameters - from_account, to_account, amount (string to not lose precision), currency, transaction_id (idempotency key).</p> <p>Sample response: <pre><code>{\n    \"status\": \"success\"\n    \"transaction_id\": \"01589980-2664-11ec-9621-0242ac130002\"\n}\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter28/#in-memory-sharding-solution","title":"In-memory sharding solution","text":"<p>Our wallet application maintains account balances for every user account.</p> <p>One good data structure to represent this is a <code>map&lt;user_id, balance&gt;</code>, which can be implemented using an in-memory Redis store.</p> <p>Since one redis node cannot withstand 1mil TPS, we need to partition our redis cluster into multiple nodes.</p> <p>Example partitioning algorithm: <pre><code>String accountID = \"A\";\nInt partitionNumber = 7;\nInt myPartition = accountID.hashCode() % partitionNumber;\n</code></pre></p> <p>Zookeeper can be used to store the number of partitions and addresses of redis nodes as it's a highly-available configuration storage. </p> <p>Finally, a wallet service is a stateless service responsible for carrying out transfer operations. It can easily scale horizontally: </p> <p>Although this solution addresses scalability concerns, it doesn't allow us to execute balance transfers atomically.</p>"},{"location":"booknotes/system-design-interview/chapter28/#distributed-transactions","title":"Distributed transactions","text":"<p>One approach for handling transactions is to use the two-phase commit protocol on top of standard, sharded relational databases: </p> <p>Here's how the two-phase commit (2PC) protocol works:   * Coordinator (wallet service) performs read and write operations on multiple databases as normal  * When application is ready to commit the transaction, coordinator asks all databases to prepare it  * If all databases replied with a \"yes\", then the coordinator asks the databases to commit the transaction.  * Otherwise, all databases are asked to abort the transaction</p> <p>Downsides to the 2PC approach:  * Not performant due to lock contention  * The coordinator is a single point of failure</p>"},{"location":"booknotes/system-design-interview/chapter28/#distributed-transaction-using-try-confirmcancel-tcc","title":"Distributed transaction using Try-Confirm/Cancel (TC/C)","text":"<p>TC/C is a variation of the 2PC protocol, which works with compensating transactions:  * Coordinator asks all databases to reserve resources for the transaction  * Coordinator collects replies from DBs - if yes, DBs are asked to try-confirm. If no, DBs are asked to try-cancel.</p> <p>One important difference between TC/C and 2PC is that 2PC performs a single transaction, whereas in TC/C, there are two independent transactions.</p> <p>Here's how TC/C works in phases: | Phase | Operation | A                   | C                   | |-------|-----------|---------------------|---------------------| | 1     | Try       | Balance change: -\\(1 | Do nothing          | | 2     | Confirm   | Do nothing          | Balance change: +\\)1 | |       | Cancel    | Balance change: +$1 | Do Nothing          |</p> <p>Phase 1 - try:   * coordinator starts local transaction in A's DB to reduce A's balance by 1$  * C's DB is given a NOP instruction, which does nothing</p> <p>Phase 2a - confirm:   * if both DBs replied with \"yes\", confirm phase starts.  * A's DB receives NOP, whereas C's DB is instructed to increase C's balance by 1$ (local transaction)</p> <p>Phase 2b - cancel:   * If any of the operations in phase 1 fails, the cancel phase starts.  * A's DB is instructed to increase A's balance by 1$, C's DB receives NOP</p> <p>Here's a comparison between 2PC and TC/C: |      | First Phase                                            | Second Phase: success              | Second Phase: fail                        | |------|--------------------------------------------------------|------------------------------------|-------------------------------------------| | 2PC  | transactions are not done yet                          | Commit/Cancel all transactions     | Cancel all transactions                   | | TC/C | All transactions are completed - committed or canceled | Execute new transactions if needed | Reverse the already committed transaction |</p> <p>TC/C is also referred to as a distributed transaction by compensation. High-level operation is handled in the business logic.</p> <p>Other properties of TC/C:  * database agnostic, as long as database supports transactions  * Details and complexity of distributed transactions need to be handled in the business logic</p>"},{"location":"booknotes/system-design-interview/chapter28/#tcc-failure-modes","title":"TC/C Failure modes","text":"<p>If the coordinator dies mid-flight, it needs to recover its intermediary state.  That can be done by maintaining phase status tables, atomically updated within the database shards: </p> <p>What does that table contain:  * ID and content of distributed transaction  * status of try phase - not sent, has been sent, response received  * second phase name - confirm or cancel  * status of second phase  * out-of-order flag (explained later)</p> <p>One caveat when using TC/C is that there is a brief moment where the account states are inconsistent with each other while a distributed transaction is in-flight: </p> <p>This is fine as long as we always recover from this state and that users cannot use the intermediary state to eg spend it.  This is guaranteed by always executing deductions prior to additions. | Try phase choices  | Account A | Account C | |--------------------|-----------|-----------| | Choice 1           | -\\(1       | NOP       | | Choice 2 (invalid) | NOP       | +\\)1       | | Choice 3 (invalid) | -\\(1       | +\\)1       |</p> <p>Note that choice 3 from table above is invalid because we cannot guarantee atomic execution of transactions across different databases without relying on 2PC.</p> <p>One edge-case to address is out of order execution: </p> <p>It is possible that a database receives a cancel operation, before receiving a try. This edge case can be handled by adding an out of order flag in our phase status table. When we receive a try operation, we first check if the out of order flag is set and if so, a failure is returned.</p>"},{"location":"booknotes/system-design-interview/chapter28/#distributed-transaction-using-saga","title":"Distributed transaction using Saga","text":"<p>Another popular approach is using Sagas - a standard for implementing distributed transactions with microservice architectures.</p> <p>Here's how it works:  * all operations are ordered in a sequence. All operations are independent in their own databases.  * operations are executed from first to last  * when an operation fails, the entire process starts to roll back until the beginning with compensating operations </p> <p>How do we coordinate the workflow? There are two approaches we can take:  * Choreography - all services involved in a saga subscribe to the related events and do their part in the saga  * Orchestration - a single coordinator instructs all services to do their jobs in the correct order</p> <p>The challenge of using choreography is that business logic is split across multiple service, which communicate asynchronously. The orchestration approach handles complexity well, so it is typically the preferred approach in a digital wallet system.</p> <p>Here's a comparison between TC/C and Saga: |                                           | TC/C            | Saga                     | |-------------------------------------------|-----------------|--------------------------| | Compensating action                       | In Cancel phase | In rollback phase        | | Central coordination                      | Yes             | Yes (orchestration mode) | | Operation execution order                 | any             | linear                   | | Parallel execution possibility            | Yes             | No (linear execution)    | | Could see the partial inconsistent status | Yes             | Yes                      | | Application or database logic             | Application     | Application              |</p> <p>The main difference is that TC/C is parallelizable, so our decision is based on the latency requirement - if we need to achieve low latency, we should go for the TC/C approach.</p> <p>Regardless of the approach we take, we still need to support auditing and replaying history to recover from failed states.</p>"},{"location":"booknotes/system-design-interview/chapter28/#event-sourcing","title":"Event sourcing","text":"<p>In real-life, a digital wallet application might be audited and we have to answer certain questions:  * Do we know the account balance at any given time?  * How do we know the historical and current balances are correct?  * How do we prove the system logic is correct after a code change?</p> <p>Event sourcing is a technique which helps us answer these questions.</p> <p>It consists of four concepts:  * command - intended action from the real world, eg transfer 1$ from account A to B. Need to have a global order, due to which they're put into a FIFO queue.    * commands, unlike events, can fail and have some randomness due to eg IO or invalid state.    * commands can produce zero or more events    * event generation can contain randomness such as external IO. This will be revisited later  * event - historical facts about events which occured in the system, eg \"transferred 1$ from A to B\".    * unlike commands, events are facts that have happened within our system    * similar to commands, they need to be ordered, hence, they're enqueued in a FIFO queue  * state - what has changed as a result of an event. Eg a key-value store between account and their balances.  * state machine - drives the event sourcing process. It mainly validates commands and applies events to update the system state.    * the state machine should be deterministic, hence, it shouldn't read external IO or rely on randomness.  </p> <p>Here's a dynamic view of event sourcing: </p> <p>For our wallet service, the commands are balance transfer requests. We can put them in a FIFO queue, such as Kafka: </p> <p>Here's the full picture:   * state machine reads commands from the command queue  * balance state is read from the database  * command is validated. If valid, two events for each of the accounts is generated  * next event is read and applied by updating the balance (state) in the database</p> <p>The main advantage of using event sourcing is its reproducibility. In this design, all state update operations are saved as immutable history of all balance changes.</p> <p>Historical balances can always be reconstructed by replaying events from the beginning.  Because the event list is immutable and the state machine is deterministic, we are guaranteed to succeed in replaying any of the intermediary states. </p> <p>All audit-related questions asked in the beginning of the section can be addressed by relying on event sourcing:  * Do we know the account balance at any given time? - events can be replayed from the start until the point which we are interested in  * How do we know the historical and current balances are correct? - correctness can be verified by recalculating all events from the start  * How do we prove the system logic is correct after a code change? - we can run different versions of the code against the events and verify their results are identical</p> <p>Answering client queries about their balance can be addressed using the CQRS architecture - there can be multiple read-only state machines which are responsible for querying the historical state, based on the immutable events list: </p>"},{"location":"booknotes/system-design-interview/chapter28/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>In this section we'll explore some performance optimizations as we're still required to scale to 1mil TPS.</p>"},{"location":"booknotes/system-design-interview/chapter28/#high-performance-event-sourcing","title":"High-performance event sourcing","text":"<p>The first optimization we'll explore is to save commands and events into local disk store instead of an external store such as Kafka.</p> <p>This avoids the network latency and also, since we're only doing appends, that operation is generally fast for HDDs.</p> <p>The next optimization is to cache recent commands and events in-memory in order to save the time of loading them back from disk.</p> <p>At a low-level, we can achieve the aforementioned optimizations by leveraging a command called mmap, which stores data in local disk as well as cache it in-memory: </p> <p>The next optimization we can do is also store state in the local file system using SQLite - a file-based local relational database. RocksDB is also another good option.</p> <p>For our purposes, we'll choose RocksDB because it uses a log-structured merge-tree (LSM), which is optimized for write operations. Read performance is optimized via caching. </p> <p>To optimize the reproducibility, we can periodically save snapshots to disk so that we don't have to reproduce a given state from the very beginning every time. We could store snapshots as large binary files in distributed file storage, eg HDFS: </p>"},{"location":"booknotes/system-design-interview/chapter28/#reliable-high-performance-event-sourcing","title":"Reliable high-performance event sourcing","text":"<p>All the optimizations done so far are great, but they make our service stateful. We need to introduce some form of replication for reliability purposes.</p> <p>Before we do that, we should analyze what kind of data needs high reliability in our system:  * state and snapshot can always be regenerated by reproducing them from the events list. Hence, we only need to guarantee the event list reliability.  * one might think we can always regenerate the events list from the command list, but that is not true, since commands are non-deterministic.  * conclusion is that we need to ensure high reliability for the events list only</p> <p>In order to achieve high reliability for events, we need to replicate the list across multiple nodes. We need to guarantee:  * that there is no data loss  * the relative order of data within a log file remains the same across replicas</p> <p>To achieve this, we can employ a consensus algorithm, such as Raft.</p> <p>With Raft, there is a leader who is active and there are followers who are passive. If a leader dies, one of the followers picks up.  As long as more than half of the nodes are up, the system continues running. </p> <p>With this approach, all nodes update the state, based on the events list. Raft ensures leader and followers have the same events list.</p>"},{"location":"booknotes/system-design-interview/chapter28/#distributed-event-sourcing","title":"Distributed event sourcing","text":"<p>So far, we've managed to design a system which has high single-node performance and is reliable.</p> <p>Some limitations we have to tackle:  * The capacity of a single raft group is limited. At some point, we need to shard the data and implement distributed transactions  * In the CQRS architecture, the request/response flow is slow. A client would need to periodically poll the system to learn when their wallet has been updated</p> <p>Polling is not real-time, hence, it can take a while for a user to learn about an update in their balance. Also, it can overload the query services if the polling frequency is too high: </p> <p>To mitigate the system load, we can introduce a reverse proxy, which sends commands on behalf of the user and polls for response on their behalf: </p> <p>This alleviates the system load as we could fetch data for multiple users using a single request, but it still doesn't solve the real-time receipt requirement.</p> <p>One final change we could do is make the read-only state machines push responses back to the reverse proxy once it's available. This can give the user the sense that updates happen real-time: </p> <p>Finally, to scale the system even further, we can shard the system into multiple raft groups, where we implement distributed transactions on top of them using an orchestrator either via TC/C or Sagas: </p> <p>Here's an example lifecycle of a balance transfer request in our final system:  * User A sends a distributed transaction to the Saga coordinator with two operations - <code>A-1</code> and <code>C+1</code>.  * Saga coordinator creates a record in the phase status table to trace the status of the transaction  * Coordinator determines which partitions it needs to send commands to.  * Partition 1's raft leader receives the <code>A-1</code> command, validates it, converts it to an event and replicates it across other nodes in the raft group  * Event result is synchronized to the read state machine, which pushes a response back to the coordinator  * Coordinator creates a record indicating that the operation was successful and proceeds with the next operation - <code>C+1</code>  * Next operation is executed similarly to the first one - partition is determined, command is sent, executed, read state machine pushes back a response  * Coordinator creates a record indicating operation 2 was also successful and finally informs the client of the result</p>"},{"location":"booknotes/system-design-interview/chapter28/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Here's the evolution of our design:  * We started from a solution using an in-memory Redis. The problem with this approach is that it is not durable storage.  * We moved on to using relational databases, on top of which we execute distributed transactions using 2PC, TC/C or distributed saga.  * Next, we introduced event sourcing in order to make all the operations auditable  * We started by storing the data into external storage using external database and queue, but that's not performant  * We proceeded to store data in local file storage, leveraging the performance of append-only operations. We also used caching to optimize the read path  * The previous approach, although performant, wasn't durable. Hence, we introduced Raft consensus with replication to avoid single points of failure  * We also adopted CQRS with a reverse proxy to manage a transaction's lifecycle on behalf of our users  * Finally, we partitioned our data across multiple raft groups, which are orchestrated using a distributed transaction mechanism - TC/C or distributed saga</p>"},{"location":"booknotes/system-design-interview/chapter29/","title":"Stock Exchange","text":"<p>We'll design an electronic stock exchange in this chapter.</p> <p>Its basic function is to efficiently match buyers and sellers.</p> <p>Major stock exchanges are NYSE, NASDAQ, among others. </p>"},{"location":"booknotes/system-design-interview/chapter29/#step-1-understand-the-problem-and-establish-design-scope","title":"Step 1 - Understand the Problem and Establish Design scope","text":"<ul> <li>C: Which securities are we going to trade? Stocks, options or futures?</li> <li>I: Only stocks for simplicity</li> <li>C: Which order types are supported - place, cancel, replace? What about limit, market, conditional orders?</li> <li>I: We need to support placing and canceling an order. We need to only consider limit orders for the order type.</li> <li>C: Does the system need to support after hours trading?</li> <li>I: No, just normal trading hours</li> <li>C: Could you describe the exchange's basic functions?</li> <li>I: Clients can place or cancel limit orders and receive matched trades in real-time. They should be able to see the order book in real time.</li> <li>C: What's the scale of the exchange?</li> <li>I: Tens of thousands of users trading at the same time and ~100 symbols. Billions of orders per day. We need to also support risk checks for compliance.</li> <li>C: What kind of risk checks?</li> <li>I: Let's do simple risk checks - eg limiting a user to trade only 1mil apple stocks in a day</li> <li>C: How about user wallet engagement?</li> <li>I: We need to ensure clients have sufficient funds before placing orders. Funds meant for pending orders need to be withheld until order is finalized.</li> </ul>"},{"location":"booknotes/system-design-interview/chapter29/#non-functional-requirements","title":"Non-functional requirements","text":"<p>The scale mentioned by the interviewer hints that we are to design a small to medium scale exchange. We need to also ensure flexibility to support more symbols and users in the future.</p> <p>Other non-functional requirements:  * Availability - At least 99.99%. Downtime can harm reputation  * Fault tolerance - fault tolerance and a fast recovery mechanism are needed to limit the impact of a production incident  * Latency - Round-trip latency should be in the ms level with focus on 99th percentile. Persistently high 99p latency causes bad experience for a handful or users.  * Security - We should have an account management system. For legal compliance, we need to support KYC to verify user identity. We should also protect against DDoS for public resources.</p>"},{"location":"booknotes/system-design-interview/chapter29/#back-of-the-envelope-estimation","title":"Back-of-the-envelope estimation","text":"<ul> <li>100 symbols, 1bil orders per day</li> <li>Normal trading hours are from 09:30 to 16:00 (6.5h)</li> <li>QPS = 1bil / 6.5 / 3600 = 43000</li> <li>Peak QPS = 5*QPS = 215000</li> <li>Trading volume is significantly higher when the market opens</li> </ul>"},{"location":"booknotes/system-design-interview/chapter29/#step-2-propose-high-level-design-and-get-buy-in","title":"Step 2 - Propose High-Level Design and Get Buy-In","text":""},{"location":"booknotes/system-design-interview/chapter29/#business-knowledge-101","title":"Business Knowledge 101","text":"<p>Let's discuss some basic concepts, related to an exchange.</p> <p>A broker mediates interactions between an exchange and end users - Robinhood, Fidelity, etc.</p> <p>Institutional clients trade in large quantities using specialized trading software. They need specialized treatment. Eg order splitting when trading in large volumes to avoid impacting the market.</p> <p>Types of orders:  * Limit - buy or sell at a fixed price. It might not find a match immediately or it might be partially matched.  * Market - doesn't specify a price. Executed at the current market price immediately.</p> <p>Prices:  * Bid - highest price a buyer is willing to buy a stock  * Ask - lowest price a seller is willing to sell a stock</p> <p>The US market has three tiers of price quotes - L1, L2, L3.</p> <p>L1 market data contains best bid/ask prices and quantities: </p> <p>L2 includes more price levels: </p> <p>L3 shows levels and queued quantity at each level: </p> <p>A candlestick shows the market open and close price, as well as the highest and lowest prices in the given interval: </p> <p>FIX is a protocol for exchanging securities transaction information, used by most vendors. Example securities transaction: <pre><code>8=FIX.4.2 | 9=176 | 35=8 | 49=PHLX | 56=PERS | 52=20071123-05:30:00.000 | 11=ATOMNOCCC9990900 | 20=3 | 150=E | 39=E | 55=MSFT | 167=CS | 54=1 | 38=15 | 40=2 | 44=15 | 58=PHLX EQUITY TESTING | 59=0 | 47=C | 32=0 | 31=0 | 151=15 | 14=0 | 6=0 | 10=128 |\n</code></pre></p>"},{"location":"booknotes/system-design-interview/chapter29/#high-level-design","title":"High-level design","text":"<p>Trade flow:  * Client places order via trading interface  * Broker sends the order to the exchange  * Order enters exchange through client gateway, which validates, rate limits, authenticates, etc. Order is forwarded to order manager.  * Order manager performs risk checks based on rules set by the risk manager  * After passing risk checks, order manager verifies there are sufficient funds in the wallet for the order  * Order is sent to matching engine. When match is found, matching engine emits two executions (called fills) for buy and sell. Both orders are sequenced so that they're deterministic.  * Executions are returned to the client.</p> <p>Market data flow (M1-M3):  * matching engine generates a stream of executions, sent to the market data publisher  * Market data publisher constructs the candlestick charts and sends them to the data service  * Market data is stored in specialized storage for real-time analytics. Brokers connect to the data service for timely market data.</p> <p>Reporter flow (R1-R2):  * reporter collects all necessary reporting fields from orders and executions and writes them to DB  * reporting fields - client_id, price, quantity, order_type, filled_quantity, remaining_quantity</p> <p>Trading flow is on the critical path, whereas the rest of the flows are not, hence, latency requirements differ between them.</p>"},{"location":"booknotes/system-design-interview/chapter29/#trading-flow","title":"Trading flow","text":"<p>The trading flow is on the critical path, hence, it should be highly optimized for low latency.</p> <p>The matching engine is at its heart, also called the cross engine. Primary responsibilities:  * Maintain the order book for each symbol - a list of buy/sell orders for a symbol.  * Match buy and sell orders - a match results in two executions (fills), with one each for the buy and sell sides. This function must be fast and accurate  * Distribute the execution stream as market data  * Matches must be produced in a deterministic order. Foundational for high availability</p> <p>Next is the sequencer - it is the key component making the matching engine deterministic by stamping each inbound order and outbound fill with a sequence ID. </p> <p>We stamp inbound orders and outbound fills for several reasons:  * timeliness and fairness  * fast recovery/replay  * exactly-once guarantee</p> <p>Conceptually, we could use Kafka as our sequencer since it's effectively an inbound and outbound message queue. However, we're going to implement it ourselves in order to achieve lower latency.</p> <p>The order manager manages the orders state. It also interacts with the matching engine - sending orders and receiving fills.</p> <p>The order manager's responsibilities:  * Sends orders for risk checks - eg verifying user's trade volume is less than 1mil  * Checks the order against the user wallet and verifies there are sufficient funds to execute it  * It sends the order to the sequencer and on to the matching engine. To reduce bandwidth, only necessary order information is passed to the matching engine  * Executions (fills) are received back from the sequencer, where they are then send to the brokers via the client gateway</p> <p>The main challenge with implementing the order manager is the state transition management. Event sourcing is one viable solution (discussed in deep dive).</p> <p>Finally, the client gateway receives orders from users and sends them to the order manager. Its responsibilities: </p> <p>Since the client gateway is on the critical path, it should stay lightweight.</p> <p>There can be multiple client gateways for different clients. Eg a colo engine is a trading engine server, rented by the broker in the exchange's data center: </p>"},{"location":"booknotes/system-design-interview/chapter29/#market-data-flow","title":"Market data flow","text":"<p>The market data publisher receives executions from the matching engine and builds the order book/candlestick charts from the execution stream.</p> <p>That data is sent to the data service, which is responsible for showing the aggregated data to subscribers: </p>"},{"location":"booknotes/system-design-interview/chapter29/#reporting-flow","title":"Reporting flow","text":"<p>The reporter is not on the critical path, but it is an important component nevertheless. </p> <p>It is responsible for trading history, tax reporting, compliance reporting, settlements, etc. Latency is not a critical requirement for the reporting flow. Accuracy and compliance are more important.</p>"},{"location":"booknotes/system-design-interview/chapter29/#api-design","title":"API Design","text":"<p>Clients interact with the stock exchange via the brokers to place orders, view executions, market data, download historical data for analysis, etc.</p> <p>We use a RESTful API for communication between the client gateway and the brokers.</p> <p>For institutional clients, a proprietary protocol is used to satisfy their low-latency requirements.</p> <p>Create order: <pre><code>POST /v1/order\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * side - buy or sell. String  * price - the price of the limit order. Long  * orderType - limit or market (we only support limit orders in our design). String  * quantity - the quantity of the order. Long</p> <p>Response:  * id - the ID of the order. Long  * creationTime - the system creation time of the order. Long  * filledQuantity - the quantity that has been successfully executed. Long  * remainingQuantity - the quantity still to be executed. Long  * status - new/canceled/filled. String  * rest of the attributes are the same as the input parameters</p> <p>Get execution: <pre><code>GET /execution?symbol={:symbol}&amp;orderId={:orderId}&amp;startTime={:startTime}&amp;endTime={:endTime}\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * orderId - the ID of the order. Optional. String  * startTime - query start time in epoch [11]. Long  * endTime - query end time in epoch. Long</p> <p>Response:  * executions - array with each execution in scope (see attributes below). Array  * id - the ID of the execution. Long  * orderId - the ID of the order. Long  * symbol - the stock symbol. String  * side - buy or sell. String  * price - the price of the execution. Long  * orderType - limit or market. String  * quantity - the filled quantity. Long</p> <p>Get order book: <pre><code>GET /marketdata/orderBook/L2?symbol={:symbol}&amp;depth={:depth}\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * depth - order book depth per side. Int</p> <p>Response:  * bids - array with price and size. Array  * asks - array with price and size. Array</p> <p>get candlesticks: <pre><code>GET /marketdata/candles?symbol={:symbol}&amp;resolution={:resolution}&amp;startTime={:startTime}&amp;endTime={:endTime}\n</code></pre></p> <p>Parameters:  * symbol - the stock symbol. String  * resolution - window length of the candlestick chart in seconds. Long  * startTime - start time of the window in epoch. Long  * endTime - end time of the window in epoch. Long</p> <p>Response:  * candles - array with each candlestick data (attributes listed below). Array  * open - open price of each candlestick. Double  * close - close price of each candlestick. Double  * high - high price of each candlestick. Double  * low - low price of each candlestick. Double</p>"},{"location":"booknotes/system-design-interview/chapter29/#data-models","title":"Data models","text":"<p>There are three main types of data in our exchange:  * Product, order, execution  * order book  * candlestick chart</p>"},{"location":"booknotes/system-design-interview/chapter29/#product-order-execution","title":"Product, order, execution","text":"<p>Products describe the attributes of a traded symbol - product type, trading symbol, UI display symbol, etc.</p> <p>This data doesn't change frequently, it is primarily used for rendering in a UI.</p> <p>An order represents an instruction for a buy/sell order. Executions are outbound matched result.</p> <p>Here's the data model: </p> <p>We encounter orders and executions in all of our three flows:  * in the critical path, they are processed in-memory for high performance. They are stored and recovered from the sequencer.  * The reporter writes orders and executions to the database for reporting use-cases  * Executions are forwarded to market data to reconstruct the order book and candlestick chart</p>"},{"location":"booknotes/system-design-interview/chapter29/#order-book","title":"Order book","text":"<p>The order book is a list of buy/sell orders for an instrument, organized by price level.</p> <p>An efficient data structure for this model, needs to satisfy:  * constant lookup time - getting volume at price level or between price levels  * fast add/execute/cancel operations  * query best bid/ask price  * iterate through price levels</p> <p>Example order book execution: </p> <p>After fulfilling this large order, the price increases as the bid/ask spread widens.</p> <p>Example order book implementation in pseudo code: <pre><code>class PriceLevel{\n    private Price limitPrice;\n    private long totalVolume;\n    private List&lt;Order&gt; orders;\n}\n\nclass Book&lt;Side&gt; {\n    private Side side;\n    private Map&lt;Price, PriceLevel&gt; limitMap;\n}\n\nclass OrderBook {\n    private Book&lt;Buy&gt; buyBook;\n    private Book&lt;Sell&gt; sellBook;\n    private PriceLevel bestBid;\n    private PriceLevel bestOffer;\n    private Map&lt;OrderID, Order&gt; orderMap;\n}\n</code></pre></p> <p>For a more efficient implementation, we can use a doubly-linked list instead of a standard list:  * Placing a new order is O(1), because we're adding an order to the tail of the list.  * Matching an order is O(1), because we are deleting an order from the head  * Canceling an order means deleting an order from the order book. We utilize <code>orderMap</code> for O(1) lookup and O(1) delete (due to the <code>Order</code> having a reference to the previous element in the list). </p> <p>This data structure is also used in the market data services to reconstruct the order book.</p>"},{"location":"booknotes/system-design-interview/chapter29/#candlestick-chart","title":"Candlestick chart","text":"<p>The candlestick data is calcualated within the market data services based on processing orders in a time interval: <pre><code>class Candlestick {\n    private long openPrice;\n    private long closePrice;\n    private long highPrice;\n    private long lowPrice;\n    private long volume;\n    private long timestamp;\n    private int interval;\n}\n\nclass CandlestickChart {\n    private LinkedList&lt;Candlestick&gt; sticks;\n}\n</code></pre></p> <p>Some optimizations to avoid consuming too much memory:  * Use pre-allocated ring buffers to hold sticks to reduce the allocation number  * Limit the number of sticks in memory and persist the rest to disk</p> <p>We'll use an in-memory columnar database (eg KDB) for real-time analytics. After market close, data is persisted in historical database.</p>"},{"location":"booknotes/system-design-interview/chapter29/#step-3-design-deep-dive","title":"Step 3 - Design Deep Dive","text":"<p>One interesting thing to be aware of about modern exchanges is that unlike most other software, they typically run everything on one gigantic server.</p> <p>Let's explore the details.</p>"},{"location":"booknotes/system-design-interview/chapter29/#performance","title":"Performance","text":"<p>For an exchange, it is very important to have good overall latency for all percentiles.</p> <p>How can we reduce latency?  * Reduce the number of tasks on the critical path  * Shorten the time spent on each task by reducing network/disk usage and/or reducing task execution time</p> <p>To achieve the first goal, we're stripped the critical path from all extraneous responsibility, even logging is removed to achieve optimal latency.</p> <p>If we follow the original design, there are several bottlenecks - network latency between services and disk usage of the sequencer.</p> <p>With such a design we can achieve tens of milliseconds end to end latency. We want to achieve tens of microseconds instead.</p> <p>Hence, we'll put everything on one server and processes are going to communicate via mmap as an event store: </p> <p>Another optimization is using an application loop (while loop executing mission-critical tasks), pinned to the same CPU to avoid context switching: </p> <p>Another side effect of using an application loop is that there is no lock contention - multiple threads fighting for the same resource.</p> <p>Let's now explore how mmap works - it is a UNIX syscall, which maps a file on disk to an application's memory.</p> <p>One trick we can use is creating the file in <code>/dev/shm</code>, which stands for \"shared memory\". Hence, we have no disk access at all.</p>"},{"location":"booknotes/system-design-interview/chapter29/#event-sourcing","title":"Event sourcing","text":"<p>Event sourcing is discussed in-depth in the digital wallet chapter. Reference it for all the details.</p> <p>In a nutshell, instead of storing current states, we store immutable state transitions:   * On the left - traditional schema  * On the right - event source schema</p> <p>Here's how our design looks like thus far:   * external domain interacts with our client gateway using the FIX protocol  * Order manager receives the new order event, validates it and adds it to its internal state. Order is then sent to matching core  * If order is matched, the <code>OrderFilledEvent</code> is generated and sent over mmap  * Other components subscribe to the event store and do their part of the processing</p> <p>One additional optimizations - all components hold a copy of the order manager, which is packaged as a library to avoid extra calls for managing orders</p> <p>The sequencer in this design, changes to not be an event store, but be a single writer, sequencing events before forwarding them to the event store: </p>"},{"location":"booknotes/system-design-interview/chapter29/#high-availability","title":"High availability","text":"<p>We aim for 99.99% availability - only 8.64s of downtime per day.</p> <p>To achieve that, we have to identify single-point-of-failures in the exchange architecture:  * setup backup instances of critical services (eg matching engine) which are on stand-by  * aggressively automate failure detection and failover to the backup instance</p> <p>Stateless services such as the client gateway can easily be horizontally scaled by adding more servers.</p> <p>For stateful components, we can process inbound events, but not publish outbound events if we're not the leader: </p> <p>To detect the primary replica being down, we can send heartbeats to detect that its non-functional.</p> <p>This mechanism only works within the boundary of a single server.  If we want to extend it, we can setup an entire server as hot/warm replica and failover in case of failure.</p> <p>To replicate the event store across the replicas, we can use reliable UDP for faster communication.</p>"},{"location":"booknotes/system-design-interview/chapter29/#fault-tolerance","title":"Fault tolerance","text":"<p>What if even the warm instances go down? It is a low probability event but we should be ready for it.</p> <p>Large tech companies tackle this problem by replicating core data to data centers in multiple cities to mitigate eg natural disasters.</p> <p>Questions to consider:  * If the primary instance is down, how and when do we failover to the backup instance?  * How do we choose the leader among the backup instances?  * What is the recovery time needed (RTO - recovery time objective)?  * What functionalities need to be recovered? Can our system operate under degraded conditions?</p> <p>How to address these:  * System can be down due to a bug (affecting primary and replicas), we can use chaos engineering to surface edge-cases and disastrous outcomes like these  * Initially though, we could perform failovers manually until we gather sufficient knowledge about the system's failure modes  * leader-election can be used (eg Raft) to determine which replica becomes the leader in the event of the primary going down</p> <p>Example of how replication works across different servers: </p> <p>Example leader-election terms: </p> <p>For details on how Raft works, check this out</p> <p>Finally, we need to also consider loss tolerance - how much data can we lose before things get critical? This will determine how often we backup our data.</p> <p>For a stock exchange, data loss is unacceptable, so we have to backup data often and rely on raft's replication to reduce probability of data loss.</p>"},{"location":"booknotes/system-design-interview/chapter29/#matching-algorithms","title":"Matching algorithms","text":"<p>Slight detour on how matching works via pseudo code: <pre><code>Context handleOrder(OrderBook orderBook, OrderEvent orderEvent) {\n    if (orderEvent.getSequenceId() != nextSequence) {\n        return Error(OUT_OF_ORDER, nextSequence);\n    }\n\n    if (!validateOrder(symbol, price, quantity)) {\n        return ERROR(INVALID_ORDER, orderEvent);\n    }\n\n    Order order = createOrderFromEvent(orderEvent);\n    switch (msgType):\n        case NEW:\n            return handleNew(orderBook, order);\n        case CANCEL:\n            return handleCancel(orderBook, order);\n        default:\n            return ERROR(INVALID_MSG_TYPE, msgType);\n\n}\n\nContext handleNew(OrderBook orderBook, Order order) {\n    if (BUY.equals(order.side)) {\n        return match(orderBook.sellBook, order);\n    } else {\n        return match(orderBook.buyBook, order);\n    }\n}\n\nContext handleCancel(OrderBook orderBook, Order order) {\n    if (!orderBook.orderMap.contains(order.orderId)) {\n        return ERROR(CANNOT_CANCEL_ALREADY_MATCHED, order);\n    }\n\n    removeOrder(order);\n    setOrderStatus(order, CANCELED);\n    return SUCCESS(CANCEL_SUCCESS, order);\n}\n\nContext match(OrderBook book, Order order) {\n    Quantity leavesQuantity = order.quantity - order.matchedQuantity;\n    Iterator&lt;Order&gt; limitIter = book.limitMap.get(order.price).orders;\n    while (limitIter.hasNext() &amp;&amp; leavesQuantity &gt; 0) {\n        Quantity matched = min(limitIter.next.quantity, order.quantity);\n        order.matchedQuantity += matched;\n        leavesQuantity = order.quantity - order.matchedQuantity;\n        remove(limitIter.next);\n        generateMatchedFill();\n    }\n    return SUCCESS(MATCH_SUCCESS, order);\n}\n</code></pre></p> <p>This matching algorithm uses the FIFO algorithm for determining which orders at a price level to match.</p>"},{"location":"booknotes/system-design-interview/chapter29/#determinism","title":"Determinism","text":"<p>Functional determinism is guaranteed via the sequencer technique we used.</p> <p>The actual time when the event happens doesn't matter: </p> <p>Latency determinism is something we have to track. We can calculate it based on monitoring 99 or 99.99 percentile latency.</p> <p>Things which can cause latency spikes are garbage collector events in eg Java.</p>"},{"location":"booknotes/system-design-interview/chapter29/#market-data-publisher-optimizations","title":"Market data publisher optimizations","text":"<p>The market data publisher receives matched results from the matching engine and rebuilds the order book and candlestick charts based on them.</p> <p>We only keep part of the candlesticks as we don't have infinite memory. Clients can choose how much granular info they want. More granular info might require a higher price: </p> <p>A ring buffer (aka circular buffer) is a fixed-size queue with the head connected to the tail. The space is preallocated to avoid allocations. The data structure is also lock-free.</p> <p>Another technique to optimize the ring buffer is padding, which ensures the sequence number is never in a cache line with anything else.</p>"},{"location":"booknotes/system-design-interview/chapter29/#distribution-fairness-of-market-data-and-multicast","title":"Distribution fairness of market data and multicast","text":"<p>We need to ensure subscribers receive the data at the same time since if one receives data before another, that gives them crucial market insight, which they can use to manipulate the market.</p> <p>To achieve this, we can use multicast using reliable UDP when publishing data to subscribers.</p> <p>Data can be transported via the internet in three ways:  * Unicast - one source, one destination  * Broadcast - one source to entire subnetwork  * Multicast - one source to a set of hosts on different subnetworks</p> <p>In theory, by using multicast, all subscribers should receive the data at the same time.</p> <p>UDP, however, is unreliable and the data might not reach everyone. It can be enhanced with retransmissions, however.</p>"},{"location":"booknotes/system-design-interview/chapter29/#colocation","title":"Colocation","text":"<p>Exchanges offer brokers the ability to colocate their servers in the same data center as the exchange.</p> <p>This reduces the latency drastically and can be considered a VIP service.</p>"},{"location":"booknotes/system-design-interview/chapter29/#network-security","title":"Network Security","text":"<p>DDoS is a challenge for exchanges as there are some internet-facing services. Here's our options:  * Isolate public services and data from private services, so DDoS attacks don't impact the most important clients  * Use a caching layer to store data which is infrequently updated  * Harden URLs against DDoS, eg prefer <code>https://my.website.com/data/recent</code> vs. <code>https://my.website.com/data?from=123&amp;to=456</code>, because the former is more cacheable  * Effective allowlist/blocklist mechanism is needed.  * Rate limiting can be used to mitigate DDoS</p>"},{"location":"booknotes/system-design-interview/chapter29/#step-4-wrap-up","title":"Step 4 - Wrap Up","text":"<p>Other interesting notes:  * not all exchanges rely on putting everything on one big server, but some still do  * modern exchanges rely more on cloud infrastructure and also on automatic market makers (AMM) to avoid maintaining an order book</p>"}]}